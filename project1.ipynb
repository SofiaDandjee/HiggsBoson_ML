{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_s6B33VEndZ"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaBZH_VLEndi"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUp3aGWvqm3j"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgWxh_GdEndl"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'standardize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-516c0c5a4242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_TRAIN_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TRAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'standardize' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jv3lZU9hlEw",
    "outputId": "fd436073-e0fd-40c4-efb2-bdae44420cbf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e931d8d4f077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReKwkQ-cEnds"
   },
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 544,
     "status": "error",
     "timestamp": 1571295874514,
     "user": {
      "displayName": "Sofia Dandjee",
      "photoUrl": "",
      "userId": "04011238111956572445"
     },
     "user_tz": -120
    },
    "id": "b2HFZUfIxCgX",
    "outputId": "eea2b54a-30c0-4fc8-ef35-8cf93c297fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(0/29): loss=29.527681339071556, weights = [0.69421466 0.30432107 0.74989677 0.29682512 0.29877896 0.88807588\n",
      " 0.20019051 0.17182301 0.92296499 0.69110861 0.66225489 0.70696627\n",
      " 0.68436555 0.57217622 0.81998011 0.04627918 0.95486884 0.34911707\n",
      " 0.74237959 0.45027441 0.6700492  0.1433274  0.25186146 0.88814764\n",
      " 0.22712664 0.80984157 0.2146933  0.18467767 0.39519062 0.64820984\n",
      " 0.05621989]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(1/29): loss=23.55404916011668, weights = [6.84125878e-01 2.89196254e-01 7.51034105e-01 2.84114342e-01\n",
      " 2.51572177e-01 8.31379688e-01 1.46374882e-01 1.15108575e-01\n",
      " 9.39805392e-01 6.61969887e-01 6.05800940e-01 6.92382843e-01\n",
      " 6.52641652e-01 5.15473022e-01 8.00294081e-01 4.12426408e-02\n",
      " 9.46049534e-01 3.19950728e-01 7.33796939e-01 4.47855898e-01\n",
      " 6.38654198e-01 1.41154021e-01 1.99195821e-01 8.27884741e-01\n",
      " 1.78286603e-01 7.63009221e-01 1.67871982e-01 1.27677205e-01\n",
      " 3.38484492e-01 5.91507569e-01 6.43979025e-04]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(2/29): loss=18.933857110384736, weights = [ 0.67413798  0.27587225  0.75059585  0.27143812  0.21024347  0.78168166\n",
      "  0.09934234  0.06539206  0.95349078  0.63572746  0.5560637   0.67849907\n",
      "  0.62494939  0.46576815  0.78300171  0.03634798  0.93734547  0.29317859\n",
      "  0.72543023  0.44543426  0.6107901   0.13905998  0.15283033  0.77489613\n",
      "  0.13536946  0.72183215  0.12670586  0.07771257  0.2887767   0.54180356\n",
      " -0.04808925]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(3/29): loss=15.356213269780064, weights = [ 0.66424996  0.26412806  0.74880112  0.25882278  0.17407225  0.73812188\n",
      "  0.05825856  0.02181359  0.96442538  0.61203482  0.51222637  0.66525595\n",
      "  0.60078678  0.42220166  0.767808    0.03158726  0.92875143  0.26853863\n",
      "  0.71726404  0.44301078  0.58602888  0.13703758  0.11199736  0.72828987\n",
      "  0.09764879  0.68561668  0.09050126  0.03391934  0.24520732  0.49823788\n",
      " -0.09081758]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(4/29): loss=12.581791286963043, weights = [ 0.65446082  0.25376981  0.74584161  0.24629078  0.14242626  0.69994594\n",
      "  0.02239159 -0.01638118  0.9729635   0.59058783  0.47357218  0.65260107\n",
      "  0.57971363  0.38401915  0.7544542   0.02695335  0.92026275  0.2458002\n",
      "  0.70928466  0.44058656  0.56399488  0.13508003  0.07602331  0.68728344\n",
      "  0.06448732  0.65375429  0.05864963 -0.00446085  0.20702193  0.46005613\n",
      " -0.12827607]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(5/29): loss=10.426350395419128, weights = [ 0.64476957  0.24462744  0.74188494  0.23386123  0.11475077  0.66649202\n",
      " -0.00890067 -0.04985403  0.97941571  0.57111951  0.4394721   0.64048785\n",
      "  0.56134389  0.3505588   0.74271333  0.02243985  0.91187528  0.22476023\n",
      "  0.70147994  0.43816262  0.54435841  0.13318137  0.04431708  0.65119033\n",
      "  0.03532595  0.62571108  0.03061711 -0.03809328  0.17355872  0.42659649\n",
      " -0.16110964]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(6/29): loss=8.748050658813291, weights = [ 0.63517524  0.23655176  0.73707764  0.22155032  0.09055906  0.63717953\n",
      " -0.0361815  -0.07918552  0.98405417  0.55339548  0.40937409  0.62887476\n",
      "  0.54533909  0.32123999  0.73238627  0.018041    0.90358526  0.20523983\n",
      "  0.69383902  0.43573985  0.52683013  0.13133634  0.01635997  0.61940827\n",
      "  0.00967419  0.60101873  0.00593533 -0.06756161  0.14423707  0.39727835\n",
      " -0.1898842 ]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(7/29): loss=7.437689419154719, weights = [ 0.62567684  0.22941191  0.73154777  0.20937164  0.06942408  0.61149913\n",
      " -0.05994505 -0.10488497  0.9871174   0.53721001  0.38279366  0.61772482\n",
      "  0.53140242  0.29555338  0.72329839  0.01375157  0.89538932  0.18708137\n",
      "  0.68635225  0.43331904  0.51115604  0.12954032 -0.00830322  0.59140885\n",
      " -0.01289825  0.57926634 -0.01580656 -0.09337785  0.11854765  0.37159238\n",
      " -0.21509624]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(8/29): loss=6.411186206648754, weights = [ 0.61627344  0.2230931   0.72540716  0.19733657  0.05097111  0.589004\n",
      " -0.08062481 -0.12739916  0.98881433  0.52238244  0.35930557  0.60700496\n",
      "  0.51927368  0.27305214  0.7152965   0.00956686  0.88728443  0.17014585\n",
      "  0.67901098  0.43090089  0.49711322  0.12778922 -0.03007197  0.56672851\n",
      " -0.03276887  0.56009347 -0.03496904 -0.11599119  0.09604362  0.34909175\n",
      " -0.23718144]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(9/29): loss=5.60379830010546, weights = [ 0.60696406  0.21749464  0.71875342  0.18545453  0.03487137  0.56930216\n",
      " -0.09860104 -0.14712006  0.98932805  0.50875416  0.33853661  0.59668561\n",
      "  0.50872473  0.25334429  0.70824624  0.00548258  0.87926785  0.15431068\n",
      "  0.6718075   0.42848604  0.48450598  0.12607946 -0.04929687  0.54496056\n",
      " -0.05026884  0.54318391 -0.05186829 -0.13579567  0.07633301  0.32938449\n",
      " -0.25652206]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(10/29): loss=4.965668376897441, weights = [ 0.59774778  0.21252819  0.71167172  0.17373323  0.02083638  0.55204975\n",
      " -0.11420731 -0.1643915   0.98881885  0.49618587  0.32015916  0.58674027\n",
      "  0.49955559  0.23608597  0.70202976  0.00149484  0.87133708  0.13946763\n",
      "  0.66473487  0.42607506  0.47316252  0.12440785 -0.06628563  0.52574822\n",
      " -0.06568871  0.52826027 -0.06678173 -0.15313697  0.05907194  0.31212672\n",
      " -0.27345351]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(11/29): loss=4.458397478669544, weights = [ 0.58862366  0.20811629  0.70423631  0.16217891  0.00861299  0.53694513\n",
      " -0.12773625 -0.17951511  0.98742715  0.48455527  0.30388567  0.57714516\n",
      "  0.49159099  0.22097552  0.69654372 -0.00239993  0.86348987  0.12552113\n",
      "  0.65778688  0.42366844  0.46293202  0.12277161 -0.0813083   0.50877846\n",
      " -0.07928335  0.51507919 -0.0799527  -0.16831833  0.04395877  0.2970168\n",
      " -0.28827013]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(12/29): loss=4.052407167920986, weights = [ 0.57959079  0.20419098  0.69651186  0.15079652 -0.00202091  0.52372368\n",
      " -0.13944452 -0.19275549  0.98527586  0.47375489  0.28946368  0.56787886\n",
      "  0.48467727  0.20774832  0.69169746 -0.00620499  0.85572416  0.1123867\n",
      "  0.65095791  0.42126665  0.45368207  0.12116825 -0.09460192  0.49377668\n",
      " -0.09127635  0.5034272  -0.09159468 -0.18160577  0.03072889  0.28379011\n",
      " -0.30123017]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(13/29): loss=3.724909129285725, weights = [ 0.57064824  0.20069264  0.68855467  0.13958989 -0.01126023  0.51215328\n",
      " -0.14955729 -0.20434476  0.98247254  0.46369036  0.27667157  0.55892207\n",
      "  0.47867983  0.19617225  0.68741149 -0.00992333  0.84803804  0.09998961\n",
      "  0.64424292  0.41887008  0.4452964   0.11959559 -0.10637456  0.48050195\n",
      " -0.10186388  0.49311702 -0.10189494 -0.19323266  0.01915016  0.27221454\n",
      " -0.31256027]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(14/29): loss=3.4583423248063374, weights = [ 0.56179512  0.19756901  0.68041368  0.12856187 -0.01927598  0.50203031\n",
      " -0.15827205 -0.21448651  0.97911133  0.45427876  0.26531476  0.55025736\n",
      "  0.47348068  0.18604368  0.68361609 -0.01355772  0.8404298   0.08826372\n",
      "  0.63763731  0.41647912  0.4376729   0.11805168 -0.11680888  0.46874292\n",
      " -0.11121801  0.48398438 -0.11101776 -0.20340369  0.00901896  0.26208645\n",
      " -0.32245929]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(15/29): loss=3.239170028791219, weights = [ 5.53030524e-01  1.94774228e-01  6.72131397e-01  1.17714489e-01\n",
      " -2.62182483e-02  4.93176158e-01 -1.65762027e-01 -2.23359358e-01\n",
      "  9.75274579e-01  4.45447219e-01  2.55222386e-01  5.41868920e-01\n",
      "  4.68976472e-01  1.77184004e-01  6.80250130e-01 -1.71107045e-02\n",
      "  8.32897827e-01  7.71503981e-02  6.31136937e-01  4.14094075e-01\n",
      "  4.30721908e-01  1.16534816e-01 -1.26065202e-01  4.58314174e-01\n",
      " -1.19489702e-01  4.75885173e-01 -1.19107240e-01 -2.12298436e-01\n",
      "  1.56675769e-04  2.53227246e-01 -3.31101751e-01]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(16/29): loss=3.056953864834817, weights = [ 0.54435358  0.19226807  0.66374471  0.10704903 -0.03221873  0.48543416\n",
      " -0.17217917 -0.23111997  0.97103427  0.43713169  0.24624442  0.53374241\n",
      "  0.46507667  0.16943654  0.67726    -0.02058466  0.82544065  0.06659766\n",
      "  0.62473802  0.41171526  0.42436467  0.11504347 -0.13428431  0.44905303\n",
      " -0.12681139  0.46869298 -0.12628981 -0.22007441 -0.00759336  0.24548024\n",
      " -0.3386408 ]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(17/29): loss=2.903641053907499, weights = [ 0.5357634   0.19001526  0.65528555  0.09656614 -0.037393    0.47866687\n",
      " -0.17765676 -0.23790575  0.96645332  0.42927588  0.2382491   0.52586478\n",
      "  0.461702    0.16266386  0.67459869 -0.02398178  0.8180569   0.05655933\n",
      "  0.6184371   0.40934294  0.41853199  0.11357628 -0.14158981  0.44081678\n",
      " -0.13329924  0.4622969  -0.13267635 -0.22686978 -0.01436859  0.238708\n",
      " -0.34521084]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(18/29): loss=2.7730157732327467, weights = [ 0.52725913  0.18798488  0.64678155  0.08626596 -0.04184248  0.47275376\n",
      " -0.18231168 -0.24383726  0.96158672  0.42183028  0.23112069  0.51822411\n",
      "  0.45878301  0.15674542  0.672225   -0.02730414  0.8107453   0.04699436\n",
      "  0.61223102  0.40697737  0.41316311  0.11213205 -0.14809021  0.43348023\n",
      " -0.13905518  0.45659967 -0.13836413 -0.23280574 -0.02028955  0.23279\n",
      " -0.35092983]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(19/29): loss=2.6602768332373956, weights = [ 0.5188399   0.18614977  0.63825654  0.07614812 -0.04565617  0.4675891\n",
      " -0.18624643 -0.24902022  0.95648247  0.41475137  0.22475754  0.5108095\n",
      "  0.45625894  0.15157548  0.67010279 -0.03055367  0.80350466  0.03786621\n",
      "  0.60611689  0.40461876  0.40820461  0.1107097  -0.15388084  0.42693354\n",
      " -0.14416859  0.45151596 -0.14343849 -0.2379886  -0.02546198  0.22762049\n",
      " -0.35590129]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(20/29): loss=2.5617125833354035, weights = [ 0.51050486  0.18448616  0.62973106  0.06621188 -0.04891218  0.46308015\n",
      " -0.18955094 -0.25354735  0.95118244  0.40800083  0.21907032  0.50361093\n",
      "  0.45407658  0.14706131  0.66820039 -0.03373217  0.79633385  0.0291423\n",
      "  0.60009202  0.40226732  0.40360956  0.10930828 -0.15904539  0.42108034\n",
      " -0.1487179   0.44697093 -0.14797426 -0.24251159 -0.02997862  0.22310673\n",
      " -0.36021608]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(21/29): loss=2.474450656401336, weights = [ 0.50225317  0.18297316  0.62122275  0.05645615 -0.05167908  0.45914557\n",
      " -0.19230404 -0.25749999  0.94572318  0.40154497  0.21398056  0.4966192\n",
      "  0.45218939  0.14312158  0.66649004 -0.03684136  0.78923182  0.02079355\n",
      "  0.59415395  0.39992323  0.3993367   0.10792693 -0.16365739  0.41583611\n",
      " -0.15277192  0.44289893 -0.1520371  -0.24645651 -0.0339208   0.2191674\n",
      " -0.36395394]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(22/29): loss=2.3962653181080595, weights = [ 0.494084    0.18159249  0.61274671  0.04687954 -0.05401703  0.45571404\n",
      " -0.19457488 -0.26094947  0.9401365   0.39535412  0.20941928  0.4898258\n",
      "  0.45055666  0.13968493  0.6649474  -0.03988286  0.78219758  0.01279396\n",
      "  0.58830038  0.39758667  0.39534977  0.10656487 -0.16778143  0.41112669\n",
      " -0.15639097  0.43924236 -0.15568459 -0.24989507 -0.03735985  0.21573116\n",
      " -0.36718489]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(23/29): loss=2.3254291561408102, weights = [ 0.48599652  0.1803281   0.60431584  0.03748043 -0.05597884  0.45272298\n",
      " -0.19642409 -0.26395836  0.93445014  0.38940213  0.20532586  0.48322287\n",
      "  0.44914279  0.13668882  0.66355114 -0.0428582   0.77523017  0.0051202\n",
      "  0.58252917  0.39525779  0.39161687  0.10522141 -0.17147426  0.40688701\n",
      " -0.15962801  0.43595073 -0.15896724 -0.25289019 -0.04035835  0.21273544\n",
      " -0.36997037]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(24/29): loss=2.260598899129452, weights = [ 0.47798991  0.1791659   0.59594111  0.02825699 -0.05761088  0.45011755\n",
      " -0.19790483 -0.26658151  0.92868818  0.383666    0.201647    0.47680311\n",
      "  0.44791666  0.13407837  0.66228259 -0.04576884  0.76832871 -0.00224861\n",
      "  0.57683835  0.39293673  0.38810995  0.10389594 -0.17478574  0.40305999\n",
      " -0.16252947  0.43297972 -0.16192935 -0.25549703 -0.04297115  0.21012538\n",
      " -0.37236431]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(25/29): loss=2.2007275064554754, weights = [ 0.47006337  0.17809357  0.58763179  0.0192072  -0.05895382  0.44784962\n",
      " -0.19906372 -0.26886703  0.92287158  0.37812542  0.19833581  0.47055971\n",
      "  0.44685109  0.13180548  0.66112536 -0.04861618  0.76149233 -0.00933131\n",
      "  0.57122604  0.39062362  0.38480433  0.10258789 -0.17775971  0.39959558\n",
      " -0.16513608  0.43029049 -0.16460978 -0.25776398 -0.04524637  0.20785288\n",
      " -0.37441408]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(26/29): loss=2.1449964799071113, weights = [ 0.4622161   0.17710028  0.57939568  0.01032892 -0.06004339  0.445877\n",
      " -0.19994162 -0.27085712  0.91701851  0.37276251  0.19535106  0.46448634\n",
      "  0.44592234  0.12982796  0.66006512 -0.05140154  0.75472022 -0.01614482\n",
      "  0.56569049  0.38831858  0.38167829  0.10129674 -0.18043469  0.39644983\n",
      " -0.16748356  0.42784895 -0.1670426  -0.25973347 -0.0472262   0.20587572\n",
      " -0.37616127]\n",
      "Computing gradients ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(27/29): loss=2.0927637411800455, weights = [ 0.4544473   0.17617654  0.5712393   0.00161987 -0.06091094  0.44416272\n",
      " -0.20057435 -0.27258877  0.91114471  0.36756145  0.19265649  0.45857704\n",
      "  0.44510968  0.1281088   0.65908929 -0.05412621  0.74801157 -0.02270447\n",
      "  0.56023005  0.38602174  0.37871271  0.10002205 -0.18284456  0.39358422\n",
      " -0.16960326  0.42562522 -0.1692577  -0.26144271 -0.04894764  0.20415694\n",
      " -0.37764236]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(28/29): loss=2.04352349143845, weights = [ 0.44675619  0.17531404  0.56316805 -0.00692231 -0.061584    0.44267432\n",
      " -0.20099331 -0.27409441  0.90526382  0.36250832  0.19022014  0.45282624\n",
      "  0.44439506  0.12661558  0.65818688 -0.05679141  0.74136565 -0.02902411\n",
      "  0.55484315  0.38373318  0.37589074  0.09876338 -0.18501912  0.39096494\n",
      " -0.17152266  0.42359308 -0.17128129 -0.2629243  -0.05044311  0.20266408\n",
      " -0.37888943]\n",
      "Computing gradients ...\n",
      "(250000,)\n",
      "Gradient Descent(29/29): loss=1.9968752944421633, weights = [ 0.43914198  0.17450549  0.55518635 -0.01530008 -0.06208674  0.44138337\n",
      " -0.20122604 -0.27540248  0.89938757  0.35759077  0.18801392  0.4472287\n",
      "  0.44376271  0.12531985  0.65734826 -0.05939832  0.73478171 -0.03511631\n",
      "  0.5495283   0.38145301  0.37319755  0.09752034 -0.18698458  0.38856232\n",
      " -0.17326589  0.42172954 -0.17313637 -0.26420686 -0.05174107  0.20136872\n",
      " -0.37993063]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9968752944421633"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 30\n",
    "gamma = 0.01\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUQybl2IEndu"
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ewy_ZY2kEndx"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "Ya9xiZufEnd2",
    "outputId": "a5f1377a-08a2-4255-b58d-9a733eff30fa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1a5e51a4d3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArUSTzDsEnd8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
