{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from cross_validation import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TRAINING DATA\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TRAINING DATA\n",
    "tx_clean = remove_undefined_values (tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING\n",
    "\n",
    "tx = build_poly_all_features(tx,2)\n",
    "tx, mean, std = standardize(tx_clean,0)\n",
    "y, tx = build_model_data(tx,y)\n",
    "y = classify(y)\n",
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "y = y.reshape(num_samples,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 61)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_features\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_gradient import *\n",
    "from cost import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 61), (61, 1), (250000, 1))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial weight\n",
    "initial_w = np.zeros((num_features,1))\n",
    "\n",
    "#Maximum iterations through the whole set\n",
    "max_iter = 100\n",
    "\n",
    "#Step-size\n",
    "gamma = 0.001\n",
    "gammas = np.logspace(-5,0, 7)\n",
    "lambdas = np.logspace (-5,0,7)\n",
    "\n",
    "#Regularization factor\n",
    "lambda_ = 0.0001\n",
    "loss_min = np.inf\n",
    "tx.shape,initial_w.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sofia/Desktop/ML_Project1/cost.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    }
   ],
   "source": [
    "w, loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4998471049434518\n",
      "0.5002611301788014\n",
      "0.5009774093499437\n",
      "0.5010241790815276\n",
      "0.4998470177482243\n",
      "0.5002611287243588\n",
      "0.500977409394318\n",
      "0.5010241790771051\n",
      "0.49984701774823526\n",
      "0.5002611287243486\n",
      "0.5009774093943071\n",
      "0.5010241790770738\n",
      "0.4998470177482623\n",
      "0.5002611287243389\n",
      "0.5009774093942965\n",
      "0.5010241790770433\n",
      "0.4998470177482888\n",
      "0.5002611287243297\n",
      "0.5009774093942861\n",
      "0.5010241790770132\n",
      "0.4998470177483149\n",
      "0.5002611287243205\n",
      "0.500977409394276\n",
      "0.5010241790769835\n",
      "0.49984701774834067\n",
      "0.5002611287243116\n",
      "0.5009774093942659\n",
      "0.501024179076954\n",
      "0.4998463481573568\n",
      "0.5002605845399177\n",
      "0.5009770066179493\n",
      "0.5010253050861802\n",
      "0.4998463488086286\n",
      "0.5002605845586028\n",
      "0.5009770066176896\n",
      "0.5010253050862179\n",
      "0.4998463488086302\n",
      "0.5002605845586023\n",
      "0.5009770066176892\n",
      "0.5010253050862162\n",
      "0.49984634880863144\n",
      "0.5002605845586017\n",
      "0.5009770066176887\n",
      "0.5010253050862149\n",
      "0.4998463488086327\n",
      "0.5002605845586012\n",
      "0.5009770066176884\n",
      "0.5010253050862139\n",
      "0.49984634880863366\n",
      "0.5002605845586012\n",
      "0.5009770066176881\n",
      "0.5010253050862129\n",
      "0.49984634880863443\n",
      "0.5002605845586007\n",
      "0.5009770066176875\n",
      "0.501025305086212\n",
      "0.49984997715299617\n",
      "0.5002657277448281\n",
      "0.5009834813822133\n",
      "0.5010470351174469\n",
      "0.4998499936601868\n",
      "0.5002657280697602\n",
      "0.5009834813846572\n",
      "0.5010470351176373\n",
      "0.49984999366016847\n",
      "0.5002657280697652\n",
      "0.5009834813846609\n",
      "0.5010470351176447\n",
      "0.4998499936601643\n",
      "0.500265728069766\n",
      "0.5009834813846618\n",
      "0.5010470351176461\n",
      "0.49984999366016336\n",
      "0.5002657280697663\n",
      "0.5009834813846619\n",
      "0.5010470351176466\n",
      "0.4998499936601631\n",
      "0.5002657280697664\n",
      "0.500983481384662\n",
      "0.5010470351176468\n",
      "0.4998499936601631\n",
      "0.5002657280697664\n",
      "0.500983481384662\n",
      "0.5010470351176467\n",
      "0.5005011600998975\n",
      "0.5009356711334192\n",
      "0.5016708950591207\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591227\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591228\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591227\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591228\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591228\n",
      "0.5018686024432344\n",
      "0.5005011708262289\n",
      "0.5009356711394969\n",
      "0.5016708950591228\n",
      "0.5018686024432344\n",
      "0.5111677128378679\n",
      "0.5117360506006942\n",
      "0.512643771788989\n",
      "0.5133149551542194\n",
      "0.5111677128378679\n",
      "0.5117360506006942\n",
      "0.5126437717889892\n",
      "0.5133149551542194\n",
      "0.511167712837868\n",
      "0.5117360506006942\n",
      "0.5126437717889891\n",
      "0.5133149551542194\n",
      "0.5111677128378678\n",
      "0.5117360506006942\n",
      "0.5126437717889891\n",
      "0.5133149551542194\n",
      "0.5111677128378679\n",
      "0.5117360506006942\n",
      "0.5126437717889891\n",
      "0.5133149551542194\n",
      "0.5111677128378679\n",
      "0.5117360506006942\n",
      "0.5126437717889892\n",
      "0.5133149551542194\n",
      "0.5111677128378679\n",
      "0.5117360506006942\n",
      "0.5126437717889892\n",
      "0.5133149551542194\n",
      "0.557994868554349\n",
      "0.5595585760501293\n",
      "0.5600307011888324\n",
      "0.5607581992015082\n",
      "0.5579948685543489\n",
      "0.5595585760501293\n",
      "0.5600307011888324\n",
      "0.5607581992015082\n",
      "0.557994868554349\n",
      "0.5595585760501293\n",
      "0.5600307011888325\n",
      "0.5607581992015082\n",
      "0.557994868554349\n",
      "0.5595585760501293\n",
      "0.5600307011888324\n",
      "0.5607581992015082\n",
      "0.5579948685543489\n",
      "0.5595585760501293\n",
      "0.5600307011888325\n",
      "0.5607581992015082\n",
      "0.557994868554349\n",
      "0.5595585760501293\n",
      "0.5600307011888325\n",
      "0.5607581992015082\n",
      "0.557994868554349\n",
      "0.5595585760501293\n",
      "0.5600307011888325\n",
      "0.5607581992015082\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n",
      "0.644981370848976\n",
      "0.6472998518367395\n",
      "0.6461488301925151\n",
      "0.6447936723179468\n"
     ]
    }
   ],
   "source": [
    "losses_tr = np.zeros((len(gammas),len(lambdas)))\n",
    "losses_val = np.zeros((len(gammas),len(lambdas)))\n",
    "\n",
    "k_fold = 4\n",
    "seed = 1\n",
    "\n",
    "for ind_lambda, lambda_ in enumerate(lambdas):\n",
    "    for ind_gamma ,gamma_ in enumerate (gammas):\n",
    "        for i,k in enumerate(range(k_fold)):\n",
    "            tx_train, y_train, tx_val, y_val = cross_validation(y, tx, k, k_fold, seed)\n",
    "            w, loss_training = reg_logistic_regression(y_train, tx_train, lambda_, initial_w, max_iter, gamma)\n",
    "            loss_val = reg_logistic_loss (y_val,tx_val,w,lambda_)/len(y_val)\n",
    "            loss_training /= len(y_train)\n",
    "            losses_tr[ind_lambda][ind_gamma]= loss_training\n",
    "            losses_val [ind_lambda][ind_gamma] = loss_val\n",
    "            \n",
    "            \n",
    "            if loss_training < loss_min:\n",
    "                weights_star = w\n",
    "                gamma_star = gamma\n",
    "                lambda_star = lambda_\n",
    "                loss_min = loss_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.67212896e-06, 2.67212896e-06, 2.67212896e-06, 2.67212896e-06,\n",
       "        2.67212896e-06, 2.67212896e-06, 2.67212896e-06],\n",
       "       [2.67213496e-06, 2.67213496e-06, 2.67213496e-06, 2.67213496e-06,\n",
       "        2.67213496e-06, 2.67213496e-06, 2.67213496e-06],\n",
       "       [2.67225085e-06, 2.67225085e-06, 2.67225085e-06, 2.67225085e-06,\n",
       "        2.67225085e-06, 2.67225085e-06, 2.67225085e-06],\n",
       "       [2.67663255e-06, 2.67663255e-06, 2.67663255e-06, 2.67663255e-06,\n",
       "        2.67663255e-06, 2.67663255e-06, 2.67663255e-06],\n",
       "       [2.73767976e-06, 2.73767976e-06, 2.73767976e-06, 2.73767976e-06,\n",
       "        2.73767976e-06, 2.73767976e-06, 2.73767976e-06],\n",
       "       [2.99071040e-06, 2.99071040e-06, 2.99071040e-06, 2.99071040e-06,\n",
       "        2.99071040e-06, 2.99071040e-06, 2.99071040e-06],\n",
       "       [3.43889959e-06, 3.43889959e-06, 3.43889959e-06, 3.43889959e-06,\n",
       "        3.43889959e-06, 3.43889959e-06, 3.43889959e-06]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64490661, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.64490661, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.64490661, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.64490661, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.64490661,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.64490661, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.64490661]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TEST SET\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN AND STANDARDIZE TEST SET\n",
    "tx_test_clean = remove_undefined_values (tX_test)\n",
    "tx_test, _, _ = standardize(tx_test_clean,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD TEST MODEL\n",
    "ytest, tx_test = build_model_data(tx_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT LABELS\n",
    "OUTPUT_PATH = 'data/submission_reg_logistic.csv'\n",
    "y_pred = predict_labels(w, tx_test,'logistic')\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
