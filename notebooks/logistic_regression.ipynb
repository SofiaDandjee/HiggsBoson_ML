{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "\n",
    "from cross_validation import *\n",
    "\n",
    "from plots import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TRAINING DATA\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TRAINING DATA\n",
    "tx_clean = remove_undefined_values (tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING\n",
    "tx, mean, std = standardize(tx_clean,0)\n",
    "y, tx = build_model_data(tx,y)\n",
    "y = classify(y)\n",
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "y = y.reshape(num_samples,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples, num_features\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_gradient import *\n",
    "from cost import *\n",
    "from implementations import *\n",
    "\n",
    "\n",
    "w_initial = np.zeros((num_features,1))\n",
    "max_iter = 100\n",
    "gamma = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.4955144160130809\n",
      "Gradient Descent(1/99): loss=0.4954681055076642\n",
      "Gradient Descent(2/99): loss=0.4954161146474288\n",
      "Gradient Descent(3/99): loss=0.495526957962801\n",
      "Gradient Descent(4/99): loss=0.4954784877999115\n",
      "Gradient Descent(5/99): loss=0.495450021715995\n",
      "Gradient Descent(6/99): loss=0.4953912980745556\n",
      "Gradient Descent(7/99): loss=0.49546459328938786\n",
      "Gradient Descent(8/99): loss=0.49551714582724116\n",
      "Gradient Descent(9/99): loss=0.4954205751986122\n",
      "Gradient Descent(10/99): loss=0.4954916735864279\n",
      "Gradient Descent(11/99): loss=0.49548433495183536\n",
      "Gradient Descent(12/99): loss=0.4955057185492739\n",
      "Gradient Descent(13/99): loss=0.4955191731603077\n",
      "Gradient Descent(14/99): loss=0.49558399800358427\n",
      "Gradient Descent(15/99): loss=0.49542545272328664\n",
      "Gradient Descent(16/99): loss=0.4955508005888775\n",
      "Gradient Descent(17/99): loss=0.4955409281551711\n",
      "Gradient Descent(18/99): loss=0.49545964957177185\n",
      "Gradient Descent(19/99): loss=0.49550971901615026\n",
      "Gradient Descent(20/99): loss=0.49550640381296773\n",
      "Gradient Descent(21/99): loss=0.49545880294103867\n",
      "Gradient Descent(22/99): loss=0.49559247432619374\n",
      "Gradient Descent(23/99): loss=0.4955013151404934\n",
      "Gradient Descent(24/99): loss=0.49552273095317284\n",
      "Gradient Descent(25/99): loss=0.4954941130056394\n",
      "Gradient Descent(26/99): loss=0.49550522580189393\n",
      "Gradient Descent(27/99): loss=0.4953855579634686\n",
      "Gradient Descent(28/99): loss=0.4955188837717613\n",
      "Gradient Descent(29/99): loss=0.4954814297019996\n",
      "Gradient Descent(30/99): loss=0.4954736657178081\n",
      "Gradient Descent(31/99): loss=0.49540326720627553\n",
      "Gradient Descent(32/99): loss=0.49551032058484384\n",
      "Gradient Descent(33/99): loss=0.49540592757639396\n",
      "Gradient Descent(34/99): loss=0.4955014429044095\n",
      "Gradient Descent(35/99): loss=0.49548583325164763\n",
      "Gradient Descent(36/99): loss=0.4956067108019549\n",
      "Gradient Descent(37/99): loss=0.4955769042257255\n",
      "Gradient Descent(38/99): loss=0.4954851847008573\n",
      "Gradient Descent(39/99): loss=0.49558004170285475\n",
      "Gradient Descent(40/99): loss=0.4954926062101099\n",
      "Gradient Descent(41/99): loss=0.49549276635657435\n",
      "Gradient Descent(42/99): loss=0.4954806178337994\n",
      "Gradient Descent(43/99): loss=0.4955324442082359\n",
      "Gradient Descent(44/99): loss=0.4955386696583402\n",
      "Gradient Descent(45/99): loss=0.49542725442502644\n",
      "Gradient Descent(46/99): loss=0.4954885211276388\n",
      "Gradient Descent(47/99): loss=0.4954320254039367\n",
      "Gradient Descent(48/99): loss=0.49546998759079386\n",
      "Gradient Descent(49/99): loss=0.49546531902403373\n",
      "Gradient Descent(50/99): loss=0.49548830923419274\n",
      "Gradient Descent(51/99): loss=0.4955850439376831\n",
      "Gradient Descent(52/99): loss=0.49549614859605146\n",
      "Gradient Descent(53/99): loss=0.4954187998415622\n",
      "Gradient Descent(54/99): loss=0.49554047762803705\n",
      "Gradient Descent(55/99): loss=0.4954323591098275\n",
      "Gradient Descent(56/99): loss=0.49556918013969536\n",
      "Gradient Descent(57/99): loss=0.49548383078769415\n",
      "Gradient Descent(58/99): loss=0.4954846734988982\n",
      "Gradient Descent(59/99): loss=0.4955011776414427\n",
      "Gradient Descent(60/99): loss=0.4955340567697383\n",
      "Gradient Descent(61/99): loss=0.4955304509452586\n",
      "Gradient Descent(62/99): loss=0.4953806816552181\n",
      "Gradient Descent(63/99): loss=0.49547915925024705\n",
      "Gradient Descent(64/99): loss=0.49542643117811747\n",
      "Gradient Descent(65/99): loss=0.495539477975866\n",
      "Gradient Descent(66/99): loss=0.49534770656351634\n",
      "Gradient Descent(67/99): loss=0.49543223101379785\n",
      "Gradient Descent(68/99): loss=0.4955340036070521\n",
      "Gradient Descent(69/99): loss=0.495428626053504\n",
      "Gradient Descent(70/99): loss=0.49550923070505426\n",
      "Gradient Descent(71/99): loss=0.4954731933190772\n",
      "Gradient Descent(72/99): loss=0.4954382896991615\n",
      "Gradient Descent(73/99): loss=0.49545010914206394\n",
      "Gradient Descent(74/99): loss=0.49552253491270115\n",
      "Gradient Descent(75/99): loss=0.4954107370592055\n",
      "Gradient Descent(76/99): loss=0.4954809388368362\n",
      "Gradient Descent(77/99): loss=0.49538577410183526\n",
      "Gradient Descent(78/99): loss=0.49540037132093684\n",
      "Gradient Descent(79/99): loss=0.4954831661670052\n",
      "Gradient Descent(80/99): loss=0.49554373637759247\n",
      "Gradient Descent(81/99): loss=0.49550675498414143\n",
      "Gradient Descent(82/99): loss=0.49549165372586734\n",
      "Gradient Descent(83/99): loss=0.4954847776231597\n",
      "Gradient Descent(84/99): loss=0.49541038360806566\n",
      "Gradient Descent(85/99): loss=0.49545511692577343\n",
      "Gradient Descent(86/99): loss=0.49546545562199623\n",
      "Gradient Descent(87/99): loss=0.49546503632815797\n",
      "Gradient Descent(88/99): loss=0.49550999637215304\n",
      "Gradient Descent(89/99): loss=0.4954145450985996\n",
      "Gradient Descent(90/99): loss=0.4954283650731227\n",
      "Gradient Descent(91/99): loss=0.4954847888444119\n",
      "Gradient Descent(92/99): loss=0.49542876116318335\n",
      "Gradient Descent(93/99): loss=0.49546263348495595\n",
      "Gradient Descent(94/99): loss=0.49539894178720584\n",
      "Gradient Descent(95/99): loss=0.4955021967045237\n",
      "Gradient Descent(96/99): loss=0.49548602743784315\n",
      "Gradient Descent(97/99): loss=0.4954681718886621\n",
      "Gradient Descent(98/99): loss=0.49553756420145195\n",
      "Gradient Descent(99/99): loss=0.4953772034674551\n"
     ]
    }
   ],
   "source": [
    "w, loss = logistic_regression(y,tx,w_initial, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.50295504)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test_clean = remove_undefined_values (tX_test)\n",
    "tx_test, _, _ = standardize(tx_test_clean,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest, tx_test = build_model_data(tx_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission_ridge.csv'\n",
    "y_pred = predict_labels(w, tx_test,'logistic')\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
