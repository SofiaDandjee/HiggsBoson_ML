{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=36.40063283650966, weights = [ 0.43481271 -0.05523457  0.83871667  0.4571222  -1.08759019 -0.52469925\n",
      " -1.00013346 -0.65207936  1.00181663  0.03536943 -0.89364183 -0.22817401\n",
      " -0.38065068 -0.9273913   0.2683672   0.63003969  0.80831813  0.13842754\n",
      "  0.66567682  0.33220036  0.12459022  0.09846941 -0.4041649  -0.86750226\n",
      " -0.74714677 -0.29052797 -0.85869644 -0.55936159 -1.26774678 -0.76916434\n",
      " -1.25433406]\n",
      "Gradient Descent(1/99): loss=67.07550512921571, weights = [ 0.28491737  0.49924125  0.08096573  0.13405021  0.66220399  1.54358702\n",
      "  1.02678598  1.41569574 -0.10524134  0.76583661  1.02628866 -0.16091312\n",
      "  0.90237417  1.14081572  0.89524143  0.44196596  0.67068489  0.60999134\n",
      "  0.49233663  0.29723405  1.08528624  0.09139616  1.38931814  1.27109637\n",
      "  1.01236502  1.38874336  0.8208275   1.51949075  0.80037272  1.29902735\n",
      "  0.76262041]\n",
      "Gradient Descent(2/99): loss=137.08356843355799, weights = [ 0.1650011  -0.26305971  0.72342227  0.09277217 -1.83367573 -1.46459969\n",
      " -1.87403028 -1.59259925  1.23166896 -0.51196148 -1.83640041 -0.35598134\n",
      " -0.8200334  -1.86749478 -0.19733754  0.27930452  0.53476483 -0.34291934\n",
      "  0.300722    0.26912689 -0.44909983  0.04518474 -1.31745894 -1.85509287\n",
      " -1.52426901 -1.03652681 -1.60426979 -1.50404863 -2.20798248 -1.70925042\n",
      " -2.15240114]\n",
      "Gradient Descent(3/99): loss=287.102165414804, weights = [ 0.06906808  0.86548566 -0.51627153 -0.17056739  1.843173    2.90273734\n",
      "  2.38072741  2.77417364 -0.89401146  1.18702048  2.30061542 -0.07977374\n",
      "  1.75852434  2.49982474  1.25950671  0.21618151  0.45962789  0.92562549\n",
      "  0.27237388  0.23270482  1.68764934  0.06815937  2.55682447  2.66668241\n",
      "  2.164578    2.48336245  1.91578265  2.88647623  2.15927512  2.65802987\n",
      "  2.11259869]\n",
      "Gradient Descent(4/99): loss=604.9767369946267, weights = [-7.67833775e-03 -7.67358954e-01  1.07026277e+00  4.50018855e-03\n",
      " -3.45538932e+00 -3.45086913e+00 -3.76884285e+00 -3.57925341e+00\n",
      "  2.07181636e+00 -1.40182962e+00 -3.71099000e+00 -4.50668319e-01\n",
      " -1.95310510e+00 -3.85392887e+00 -9.52436222e-01  1.02515864e-01\n",
      "  3.47085047e-01 -9.57634066e-01  1.01214349e-01  2.12553003e-01\n",
      " -1.48613902e+00  5.87553128e-04 -3.11207329e+00 -3.92771722e+00\n",
      " -3.20341025e+00 -2.64648899e+00 -3.21398511e+00 -3.49958000e+00\n",
      " -4.19448505e+00 -3.69566234e+00 -4.05428612e+00]\n",
      "Gradient Descent(5/99): loss=1276.967557526422, weights = [-0.06907547  1.60518719 -1.39516126 -0.38517675  4.28593537  5.77778122\n",
      "  5.20111268  5.64854907 -2.3236324   2.26867184  5.0404633   0.13138336\n",
      "  3.45501028  5.37477835  2.19442783  0.12716757  0.32999411  1.77778267\n",
      "  0.20289514  0.17430483  3.07392018  0.07261285  5.10170647  5.63627144\n",
      "  4.5893172   4.79302355  4.22565637  5.77761698  5.03415579  5.53295553\n",
      "  4.94133896]\n",
      "Gradient Descent(6/99): loss=2696.81891946229, weights = [-1.18193176e-01 -1.84939272e+00  2.06478645e+00  9.21217427e-02\n",
      " -6.93122763e+00 -7.64146668e+00 -7.80641366e+00 -7.77001611e+00\n",
      "  4.00637565e+00 -3.13866245e+00 -7.65979622e+00 -6.70990787e-01\n",
      " -4.40535211e+00 -8.04469877e+00 -2.42867129e+00 -4.58058809e-03\n",
      "  2.07363445e-01 -2.18275258e+00 -4.69122116e-02  1.68525909e-01\n",
      " -3.59693226e+00 -5.19654557e-02 -6.85575897e+00 -8.28301221e+00\n",
      " -6.74670151e+00 -6.03609578e+00 -6.60345301e+00 -7.71080278e+00\n",
      " -8.38528659e+00 -7.88639625e+00 -8.10244923e+00]\n",
      "Gradient Descent(7/99): loss=5696.437458571398, weights = [-0.15748734  3.16162304 -3.06306108 -0.6608117   9.40178085 11.85723619\n",
      " 11.12776431 11.7271727  -5.23474292  4.66318006 10.82108492  0.53457238\n",
      "  7.01160412 11.45419813  4.25483025  0.12167984  0.26500006  3.59547236\n",
      "  0.24527443  0.12185612  6.06272979  0.11411867 10.50825512 11.93183736\n",
      "  9.7210185   9.68906943  9.12185451 11.88977424 11.11350793 11.61230993\n",
      " 10.88496245]\n",
      "Gradient Descent(8/99): loss=12033.299194821837, weights = [ -0.18892267  -4.13447191   4.30848716   0.39399891 -14.31248246\n",
      " -16.48847544 -16.36512253 -16.61683821   8.16835811  -6.72231993\n",
      " -16.01818818  -1.18412655  -9.59456941 -16.89192898  -5.48589293\n",
      "  -0.10599083   0.08241782  -4.77986988  -0.22880849   0.1407551\n",
      "  -8.00809032  -0.13846419 -14.74331673 -17.46355019 -14.22122047\n",
      " -13.17904264 -13.74633887 -16.60274758 -17.23251536 -16.73355205\n",
      " -16.68570181]\n",
      "Gradient Descent(9/99): loss=25420.14457792203, weights = [ -0.21407094   6.45530613  -6.47425752  -1.16552571  20.17645387\n",
      "  24.70571183  23.62045773  24.57439519 -11.33222954   9.78922096\n",
      "  23.01250902   1.34160742  14.52718324  24.30273327   8.65183843\n",
      "   0.1952089    0.26668868   7.41517425   0.42569599   0.07014312\n",
      "  12.41629167   0.22038373  21.94577189  25.24911233  20.57210159\n",
      "  20.0485399   19.48146309  24.80599613  23.96195589  24.46070974\n",
      "  23.41147898]\n",
      "Gradient Descent(10/99): loss=53700.25080722243, weights = [ -0.23418955  -8.95248889   9.13947578   1.08366595 -29.93323249\n",
      " -35.17286479 -34.47174954 -35.30035485  16.99729702 -14.23898983\n",
      " -33.69692039  -2.30699231 -20.54831446 -35.57665878 -11.91200251\n",
      "  -0.26239176  -0.06782944 -10.29002623  -0.54988046   0.13497707\n",
      " -17.2932055   -0.30722684 -31.39261106 -36.84231515 -30.00251451\n",
      " -28.2543711  -28.82168461 -35.38333737 -35.91719514 -35.41811794\n",
      " -34.84538928]\n",
      "Gradient Descent(11/99): loss=113442.7590564765, weights = [-2.50284440e-01  1.34252110e+01 -1.36048204e+01 -2.19682866e+00\n",
      "  4.29148201e+01  5.18533264e+01  4.99866656e+01  5.17198212e+01\n",
      " -2.41876013e+01  2.06613264e+01  4.87461200e+01  3.01405221e+00\n",
      "  3.04163329e+01  5.14505959e+01  1.79645529e+01  3.89640320e-01\n",
      "  3.63446484e-01  1.54609841e+01  8.50898159e-01  7.80370759e-03\n",
      "  2.58672867e+01  4.55278520e-01  4.61198813e+01  5.33954585e+01\n",
      "  4.35024729e+01  4.19452231e+01  4.13783209e+01  5.20955362e+01\n",
      "  5.11096752e+01  5.16082966e+01  4.98489754e+01]\n",
      "Gradient Descent(12/99): loss=239650.4294913109, weights = [ -0.26316035 -19.11689343  19.40904079   2.56404632 -62.95242816\n",
      " -74.6395309  -72.74639806 -74.76473491  35.66669764 -30.08345063\n",
      " -71.06382766  -4.70554909 -43.67415926 -75.04392584 -25.46933873\n",
      "  -0.56653085  -0.3087134  -21.95189972  -1.19731557   0.164063\n",
      " -36.88305984  -0.65606663 -66.55326201 -77.76906675 -63.33538839\n",
      " -60.09159646 -60.65904118 -75.0543354  -75.38432003 -74.88502566\n",
      " -73.23122683]\n",
      "Gradient Descent(13/99): loss=506267.535942669, weights = [-2.73461082e-01  2.81643643e+01 -2.86135418e+01 -4.35988719e+00\n",
      "  9.09331145e+01  1.09207777e+02  1.05662988e+02  1.09070076e+02\n",
      " -5.13324061e+01  4.36568179e+01  1.03090233e+02  6.52582399e+00\n",
      "  6.39976872e+01  1.08805685e+02  3.76517443e+01  8.17862985e-01\n",
      "  6.30788802e-01  3.24389101e+01  1.77084633e+00 -8.77594946e-02\n",
      "  5.43050141e+01  9.57094113e-01  9.71995302e+01  1.12865236e+02\n",
      "  9.19475607e+01  8.82103930e+01  8.76437824e+01  1.09748783e+02\n",
      "  1.08464495e+02  1.08962819e+02  1.05677795e+02]\n",
      "Gradient Descent(14/99): loss=1069503.3451055435, weights = [  -0.28170167  -40.57367663   41.15115732    5.70156577 -132.72119885\n",
      " -158.00973469 -153.62480616 -158.12969316   75.11482063  -63.53295221\n",
      " -150.01989775   -9.78957965  -92.51285584 -158.41528741  -54.09867387\n",
      "   -1.19754354   -0.76572844  -46.60448358   -2.54967151    0.25696287\n",
      "  -78.24757352   -1.38898283 -140.81993547 -164.21934448 -133.74917918\n",
      " -127.34244145 -127.91022645 -158.85714444 -158.75535025 -158.25561051\n",
      " -154.34173265]\n",
      "Gradient Descent(15/99): loss=2259354.130695669, weights = [  -0.28829413   59.31736287  -60.27856605   -8.92300939  192.36005699\n",
      "  230.37460728  223.2592642   230.22845016 -108.67095566   92.25326059\n",
      "  217.87748427   13.93102848  134.95420894  229.97397396   79.25029355\n",
      "    1.72990872    1.23835645   68.29098305    3.72523961   -0.26237046\n",
      "  114.39789669    2.02002416  205.11784423  238.50427401  194.29086475\n",
      "  185.95154726  185.3855009   231.54595124  229.632244    230.12992922\n",
      "  223.59980455]\n",
      "Gradient Descent(16/99): loss=4772945.887462381, weights = [-2.93568106e-01 -8.58855403e+01  8.71181888e+01  1.23334069e+01\n",
      " -2.80121116e+02 -3.34127674e+02 -3.24503399e+02 -3.34236156e+02\n",
      "  1.58453452e+02 -1.34181745e+02 -3.16831617e+02 -2.05404073e+01\n",
      " -1.95671440e+02 -3.34535566e+02 -1.14571563e+02 -2.52592126e+00\n",
      " -1.69580605e+00 -9.86959465e+01 -5.39863438e+00  4.76771294e-01\n",
      " -1.65615637e+02 -2.93534206e+00 -2.97699033e+02 -3.46841193e+02\n",
      " -2.82499296e+02 -2.69408072e+02 -2.69976618e+02 -3.35889790e+02\n",
      " -3.34874901e+02 -3.34374228e+02 -3.25706375e+02]\n",
      "Gradient Descent(17/99): loss=10082976.009126015, weights = [-2.97787285e-01  1.25144856e+02 -1.27139845e+02 -1.85608431e+01\n",
      "  4.06616876e+02  4.86346503e+02  4.71664383e+02  4.86182867e+02\n",
      " -2.29798381e+02  1.94925727e+02  4.60354881e+02  2.95663200e+01\n",
      "  2.84865546e+02  4.85949055e+02  1.67134631e+02  3.65942445e+00\n",
      "  2.55114792e+00  1.44018977e+02  7.85974354e+00 -6.10956420e-01\n",
      "  2.41359740e+02  4.26678388e+00  4.33109486e+02  5.03926117e+02\n",
      "  4.10494080e+02  3.92435017e+02  3.91870127e+02  4.88848693e+02\n",
      "  4.85606211e+02  4.86102542e+02  4.72697900e+02]\n",
      "Gradient Descent(18/99): loss=21300557.306520753, weights = [-3.01162628e-01 -1.81592528e+02  1.84253051e+02  2.63439279e+01\n",
      " -5.91516760e+02 -7.06177978e+02 -6.85508635e+02 -7.06261837e+02\n",
      "  3.34509151e+02 -2.83420500e+02 -6.69238575e+02 -4.32583959e+01\n",
      " -4.13583728e+02 -7.06590710e+02 -2.42316664e+02 -5.33060244e+00\n",
      " -3.63633916e+00 -2.08749564e+02 -1.14128419e+01  9.58544293e-01\n",
      " -3.50170349e+02 -6.20125455e+00 -6.29098998e+02 -7.32629307e+02\n",
      " -5.96738202e+02 -5.69523620e+02 -5.70093806e+02 -7.09873140e+02\n",
      " -7.06928481e+02 -7.06425839e+02 -6.87733032e+02]\n",
      "Gradient Descent(19/99): loss=44997999.17761645, weights = [-3.03862902e-01  2.64222207e+02 -2.68360973e+02 -3.89213564e+01\n",
      "  8.59231585e+02  1.02709746e+03  9.96407770e+02  1.02689727e+03\n",
      " -4.85683616e+02  4.11830778e+02  9.72583112e+02  6.25911842e+01\n",
      "  6.01569442e+02  1.02670684e+03  3.52797575e+02  7.73630968e+00\n",
      "  5.34461132e+00  3.03988626e+02  1.65972452e+01 -1.33242467e+00\n",
      "  5.09581556e+02  9.01359207e+00  9.14759308e+02  1.06464190e+03\n",
      "  8.67228494e+02  8.28638220e+02  8.28075747e+02  1.03241044e+03\n",
      "  1.02636167e+03  1.02685514e+03  9.98911033e+02]\n",
      "Gradient Descent(20/99): loss=95059481.80106682, weights = [-3.06023122e-01 -3.83761738e+02  3.89475442e+02  5.59405757e+01\n",
      " -1.24935636e+03 -1.49214091e+03 -1.44816051e+03 -1.49217239e+03\n",
      "  7.06430588e+02 -5.98686317e+02 -1.41371869e+03 -9.12546835e+01\n",
      " -8.73917950e+02 -1.49256377e+03 -5.12177145e+02 -1.12553697e+01\n",
      " -7.71901450e+00 -4.41247535e+02 -2.41155129e+01  1.98907006e+00\n",
      " -7.40036627e+02 -1.31003589e+01 -1.32917899e+03 -1.54761241e+03\n",
      " -1.26057732e+03 -1.20352491e+03 -1.20409858e+03 -1.49992030e+03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -1.49289821e+03 -1.49239141e+03 -1.45253650e+03]\n",
      "Gradient Descent(21/99): loss=200815708.82445326, weights = [-3.07751297e-01  5.58040532e+02 -5.66674399e+02 -8.19348585e+01\n",
      "  1.81538490e+03  2.16945159e+03  2.10492410e+03  2.16917453e+03\n",
      " -1.02624949e+03  8.70052244e+02  2.05466882e+03  1.32354129e+02\n",
      "  1.27062546e+03  2.16907549e+03  7.45019658e+02  1.63486964e+01\n",
      "  1.12598184e+01  6.41922912e+02  3.50574204e+01 -2.84564592e+00\n",
      "  1.07621699e+03  1.90414065e+01  1.93226879e+03  2.24917341e+03\n",
      "  1.83209100e+03  1.75012971e+03  1.74957233e+03  2.18070187e+03\n",
      "  2.16872543e+03  2.16921285e+03  2.11053877e+03]\n",
      "Gradient Descent(22/99): loss=424228579.71827537, weights = [-3.09133838e-01 -8.10836662e+02  8.23031164e+02  1.18462653e+02\n",
      " -2.63906719e+03 -3.15250562e+03 -3.05930084e+03 -3.15242610e+03\n",
      "  1.49212282e+03 -1.26469121e+03 -2.98646244e+03 -1.92650319e+02\n",
      " -1.84637662e+03 -3.15294978e+03 -1.08226082e+03 -2.37719513e+01\n",
      " -1.63321629e+01 -9.32410933e+02 -5.09486673e+01  4.17537479e+00\n",
      " -1.56363073e+03 -2.76749605e+01 -2.80810555e+03 -3.26928199e+03\n",
      " -2.66295666e+03 -2.54286945e+03 -2.54345050e+03 -3.16891360e+03\n",
      " -3.15327717e+03 -3.15276158e+03 -3.06821531e+03]\n",
      "Gradient Descent(23/99): loss=896194271.8166964, weights = [-3.10239870e-01  1.17875187e+03 -1.19685347e+03 -1.72803830e+02\n",
      "  3.83527784e+03  4.58270839e+03  4.44667730e+03  4.58226929e+03\n",
      " -2.16821148e+03  1.83805912e+03  4.34059504e+03  2.79728673e+02\n",
      "  2.68403326e+03  4.58236306e+03  1.57360295e+03  3.45421226e+01\n",
      "  2.37655360e+01  1.35581357e+03  7.40563299e+01 -6.03445670e+00\n",
      "  2.27325780e+03  4.02253036e+01  4.08179606e+03  4.75153201e+03\n",
      "  3.87038751e+03  3.69680527e+03  3.69625862e+03  4.60650085e+03\n",
      "  4.58200269e+03  4.58247734e+03  4.45887191e+03]\n",
      "Gradient Descent(24/99): loss=1893234476.467337, weights = [-3.11124696e-01 -1.71303241e+03  1.73894369e+03  2.50540351e+02\n",
      " -5.57487496e+03 -6.66006778e+03 -6.46289503e+03 -6.65975344e+03\n",
      "  3.15191655e+03 -2.67164343e+03 -6.30893424e+03 -4.06852350e+02\n",
      " -3.90071338e+03 -6.66055685e+03 -2.28657449e+03 -5.02141006e+01\n",
      " -3.45195880e+01 -1.97001091e+03 -1.07633347e+02  8.80072800e+00\n",
      " -3.30348774e+03 -5.84643704e+01 -5.93236662e+03 -6.90635141e+03\n",
      " -5.62552224e+03 -5.37227090e+03 -5.37286754e+03 -6.69470435e+03\n",
      " -6.66086930e+03 -6.66033515e+03 -6.48138994e+03]\n",
      "Gradient Descent(25/99): loss=3999508695.8269033, weights = [-3.11832557e-01  2.49003203e+03 -2.52811072e+03 -3.64768862e+02\n",
      "  8.10234988e+03  9.68078103e+03  9.39367723e+03  9.67999993e+03\n",
      " -4.58063917e+03  3.88300008e+03  9.16966773e+03  5.91060765e+02\n",
      "  5.66990287e+03  9.68050078e+03  3.32401065e+03  7.29756657e+01\n",
      "  5.01909519e+01  2.86392309e+03  1.56443490e+02 -1.27651920e+01\n",
      "  4.80204579e+03  8.49766622e+01  8.62274106e+03  1.00378355e+04\n",
      "  8.17634204e+03  7.80920942e+03  7.80868545e+03  9.73106873e+03\n",
      "  9.68011866e+03  9.68056632e+03  9.41977958e+03]\n",
      "Gradient Descent(26/99): loss=8449069572.55377, weights = [-3.12398846e-01 -3.61893508e+03  3.67384518e+03  5.29556171e+02\n",
      " -1.17768536e+04 -1.40698844e+04 -1.36530954e+04 -1.40690736e+04\n",
      "  6.65827302e+03 -5.64386787e+03 -1.33277526e+04 -8.59360124e+02\n",
      " -8.24054796e+03 -1.40704682e+04 -4.83071642e+03 -1.06074409e+02\n",
      " -7.29353678e+01 -4.16197208e+03 -2.27380468e+02  1.85767596e+01\n",
      " -6.97897576e+03 -1.23508025e+02 -1.25324413e+04 -1.45897547e+04\n",
      " -1.18840232e+04 -1.13494578e+04 -1.13500874e+04 -1.41430298e+04\n",
      " -1.40707491e+04 -1.40701757e+04 -1.36918217e+04]\n",
      "Gradient Descent(27/99): loss=17848886469.005535, weights = [-3.12851877e-01  5.26015601e+03 -5.34041694e+03 -7.70301965e+02\n",
      "  1.71166485e+04  2.04505991e+04  1.98443316e+04  2.04490958e+04\n",
      " -9.67695990e+03  8.20299232e+03  1.93712065e+04  1.24875825e+03\n",
      "  1.19776391e+04  2.04504564e+04  7.02179685e+03  1.54167016e+02\n",
      "  1.06020075e+02  6.04984191e+03  3.30489268e+02 -2.69799395e+01\n",
      "  1.01441847e+04  1.79514915e+02  1.82156200e+04  2.12052970e+04\n",
      "  1.72727851e+04  1.64967733e+04  1.64962972e+04  2.05568583e+04\n",
      "  2.04500283e+04  2.04504190e+04  1.98998221e+04]\n",
      "Gradient Descent(28/99): loss=37706252203.443954, weights = [-3.13214301e-01 -7.64519690e+03  7.76138601e+03  1.11898284e+03\n",
      " -2.48787056e+04 -2.97233184e+04 -2.88426026e+04 -2.97214587e+04\n",
      "  1.40655429e+04 -1.19227726e+04 -2.81552015e+04 -1.81529463e+03\n",
      " -1.74085591e+04 -2.97241024e+04 -1.02052818e+04 -2.24081330e+02\n",
      " -1.54085782e+02 -8.79255173e+03 -4.80348871e+02  3.92323734e+01\n",
      " -1.47435370e+04 -2.60914677e+02 -2.64752659e+04 -3.08211472e+04\n",
      " -2.51052763e+04 -2.39764262e+04 -2.39771254e+04 -2.98778151e+04\n",
      " -2.97243166e+04 -2.97236604e+04 -2.89240605e+04]\n",
      "Gradient Descent(29/99): loss=79655470816.42311, weights = [-3.13504241e-01  1.11121255e+04 -1.12814809e+04 -1.62700320e+03\n",
      "  3.61595913e+04  4.32021315e+04  4.19216044e+04  4.31991030e+04\n",
      " -2.04430767e+04  1.73290910e+04  4.09222250e+04  2.63816238e+03\n",
      "  2.53029069e+04  4.32022795e+04  1.48334722e+04  3.25685437e+02\n",
      "  2.23963908e+02  1.27801784e+04  6.98166189e+02 -5.70060242e+01\n",
      "  2.14296021e+04  3.79229344e+02  3.84808448e+04  4.47968629e+04\n",
      "  3.64892639e+04  3.48494849e+04  3.48491100e+04  4.34266317e+04\n",
      "  4.32017545e+04  4.32020247e+04  4.20391859e+04]\n",
      "Gradient Descent(30/99): loss=168274322167.26428, weights = [-3.13736193e-01 -1.61507755e+04  1.63964335e+04  2.36416096e+03\n",
      " -5.25567253e+04 -6.27916160e+04 -6.09308595e+04 -6.27875400e+04\n",
      "  2.97135971e+04 -2.51871288e+04 -5.94785918e+04 -3.83473124e+03\n",
      " -3.67762240e+04 -6.27928227e+04 -2.15591931e+04 -4.73374575e+02\n",
      " -3.25515437e+02 -1.85747780e+04 -1.01475094e+03  8.28704109e+01\n",
      " -3.11463759e+04 -5.51190074e+02 -5.59298474e+04 -6.51103965e+04\n",
      " -5.30355301e+04 -5.06512367e+04 -5.06520830e+04 -6.31179698e+04\n",
      " -6.27928959e+04 -6.27920647e+04 -6.11025811e+04]\n",
      "Gradient Descent(31/99): loss=355484026529.0286, weights = [-3.13921754e-01  2.34745755e+04 -2.38321312e+04 -3.43680903e+03\n",
      "  7.63883143e+04  9.12653573e+04  8.85604218e+04  9.12591069e+04\n",
      " -4.31867901e+04  3.66082184e+04  8.64493248e+04  5.57331682e+03\n",
      "  5.34528977e+04  9.12661196e+04  3.13358487e+04  6.88022287e+02\n",
      "  4.73125751e+02  2.69981929e+04  1.47489399e+03 -1.20434821e+02\n",
      "  4.52703548e+04  8.01131305e+02  8.12916865e+04  9.46346830e+04\n",
      "  7.70845900e+04  7.36200807e+04  7.36199195e+04  9.17396444e+04\n",
      "  9.12653896e+04  9.12654055e+04  8.88091800e+04]\n",
      "Gradient Descent(32/99): loss=750969556672.7883, weights = [-3.14070203e-01 -3.41190324e+04  3.46382094e+04  4.99463182e+03\n",
      " -1.11027290e+05 -1.32649281e+05 -1.28718179e+05 -1.32640523e+05\n",
      "  6.27705297e+04 -5.32084390e+04 -1.25650101e+05 -8.10084432e+03\n",
      " -7.76909321e+04 -1.32651380e+05 -4.55446333e+04 -1.00001364e+03\n",
      " -6.87663239e+02 -3.92399961e+04 -2.14368927e+03  1.75058726e+02\n",
      " -6.57978090e+04 -1.16440477e+03 -1.18153432e+05 -1.37547354e+05\n",
      " -1.12038943e+05 -1.07002491e+05 -1.07003648e+05 -1.33338688e+05\n",
      " -1.32651156e+05 -1.32649955e+05 -1.29080577e+05]\n",
      "Gradient Descent(33/99): loss=1586443364434.6174, weights = [-3.14188963e-01  4.95905891e+04 -5.03457117e+04 -7.26007375e+03\n",
      "  1.61372568e+05  1.92800215e+05  1.87086162e+05  1.92787159e+05\n",
      " -9.12334978e+04  7.73358913e+04  1.82626541e+05  1.17739105e+04\n",
      "  1.12920514e+05  1.92802275e+05  6.61975646e+04  1.45346826e+03\n",
      "  9.99488475e+02  5.70341286e+04  3.11575270e+03 -2.54428346e+02\n",
      "  9.56345900e+04  1.69241043e+03  1.71730749e+05  1.99918420e+05\n",
      "  1.62843306e+05  1.55524006e+05  1.55524296e+05  1.93802184e+05\n",
      "  1.92801112e+05  1.92800591e+05  1.87612043e+05]\n",
      "Gradient Descent(34/99): loss=3351404229633.1797, weights = [-3.14283970e-01 -7.20774520e+04  7.31744560e+04  1.05515710e+04\n",
      " -2.34547935e+05 -2.80225472e+05 -2.71920725e+05 -2.80206823e+05\n",
      "  1.32604186e+05 -1.12404211e+05 -2.65439196e+05 -1.71131216e+04\n",
      " -1.64124349e+05 -2.80229458e+05 -9.62145180e+04 -2.11255315e+03\n",
      " -1.45270882e+03 -8.28958262e+04 -4.52860151e+03  3.69810296e+02\n",
      " -1.38999888e+05 -2.45983736e+03 -2.49602418e+05 -2.90572369e+05\n",
      " -2.36685240e+05 -2.26046030e+05 -2.26047843e+05 -2.81681839e+05\n",
      " -2.80228605e+05 -2.80226623e+05 -2.72685925e+05]\n",
      "Gradient Descent(35/99): loss=7079931475780.512, weights = [-3.14359977e-01  1.04761375e+05 -1.06356358e+05 -1.53368243e+04\n",
      "  3.40904082e+05  4.07295333e+05  3.95224423e+05  4.07267899e+05\n",
      " -1.92733461e+05  1.63374193e+05  3.85803490e+05  2.48728327e+04\n",
      "  2.38547453e+05  4.07300134e+05  1.39843878e+05  3.07049314e+03\n",
      "  2.11144503e+03  1.20485848e+05  6.58211109e+03 -5.37492186e+02\n",
      "  2.02030394e+05  3.57526147e+03  3.62785709e+05  4.22333146e+05\n",
      "  3.44010896e+05  3.28548247e+05  3.28549491e+05  4.09412042e+05\n",
      "  4.07298057e+05  4.07296400e+05  3.96335740e+05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(36/99): loss=14956545455943.232, weights = [-3.14420780e-01 -1.52265628e+05  1.54583316e+05  2.22907534e+04\n",
      " -4.95488619e+05 -5.91984140e+05 -5.74439966e+05 -5.91944594e+05\n",
      "  2.80129659e+05 -2.37456872e+05 -5.60747420e+05 -3.61517982e+04\n",
      " -3.46717260e+05 -5.91992111e+05 -2.03256009e+05 -4.46282303e+03\n",
      " -3.06888721e+03 -1.75119942e+05 -9.56679243e+03  7.81229133e+02\n",
      " -2.93641242e+05 -5.19647359e+03 -5.27291922e+05 -6.13841822e+05\n",
      " -5.00003891e+05 -4.77528706e+05 -4.77531905e+05 -5.95060730e+05\n",
      " -5.91989928e+05 -5.91986296e+05 -5.76056088e+05]\n",
      "Gradient Descent(37/99): loss=31596104106507.367, weights = [-3.14469427e-01  2.21311147e+05 -2.24680351e+05 -3.23991776e+04\n",
      "  7.20169223e+05  8.60422033e+05  8.34922077e+05  8.60364227e+05\n",
      " -4.07154861e+05  3.45132415e+05  8.15020249e+05  5.25446590e+04\n",
      "  5.03937741e+05  8.60432626e+05  2.95423696e+05  6.48650138e+03\n",
      "  4.46048393e+03  2.54529308e+05  1.39048866e+04 -1.13547138e+03\n",
      "  4.26794393e+05  7.55283481e+03  7.66394453e+05  8.92190234e+05\n",
      "  7.26732295e+05  6.94066604e+05  6.94069863e+05  8.64893662e+05\n",
      "  8.60428617e+05  8.60424562e+05  8.37270152e+05]\n",
      "Gradient Descent(38/99): loss=66747618803416.7, weights = [-3.14508337e-01 -3.21665301e+05  3.26561724e+05  4.70900907e+04\n",
      " -1.04673283e+06 -1.25058270e+06 -1.21352000e+06 -1.25049901e+06\n",
      "  5.91781182e+05 -5.01633979e+05 -1.18459398e+06 -7.63715124e+04\n",
      " -7.32449688e+05 -1.25059909e+06 -4.29384032e+05 -9.42783153e+03\n",
      " -6.48310678e+03 -3.69945865e+05 -2.02101059e+04  1.65036313e+03\n",
      " -6.20325254e+05 -1.09776912e+04 -1.11391845e+06 -1.29675722e+06\n",
      " -1.05627162e+06 -1.00879261e+06 -1.00879874e+06 -1.25708206e+06\n",
      " -1.25059410e+06 -1.25058698e+06 -1.21693371e+06]\n",
      "Gradient Descent(39/99): loss=141006138000684.25, weights = [-3.14539476e-01  4.67525675e+05 -4.74642961e+05 -6.84438583e+04\n",
      "  1.52137702e+06  1.81766436e+06  1.76379517e+06  1.81754239e+06\n",
      " -8.60125833e+05  7.29101541e+05  1.72175218e+06  1.11002137e+05\n",
      "  1.06458186e+06  1.81768719e+06  6.24090243e+05  1.37029104e+04\n",
      "  9.42289229e+03  5.37699717e+05  2.93744467e+04 -2.39871794e+03\n",
      "  9.01614382e+05  1.59555653e+04  1.61902878e+06  1.88477596e+06\n",
      "  1.53524153e+06  1.46623395e+06  1.46624147e+06  1.82711082e+06\n",
      "  1.81767910e+06  1.81766998e+06  1.76875594e+06]\n",
      "Gradient Descent(40/99): loss=297879254875390.2, weights = [-3.14564371e-01 -6.79526657e+05  6.89870733e+05  9.94793563e+04\n",
      " -2.21125104e+06 -2.64188985e+06 -2.56359376e+06 -2.64171290e+06\n",
      "  1.25015340e+06 -1.05971522e+06 -2.50248661e+06 -1.61336738e+05\n",
      " -1.54731980e+06 -2.64192402e+06 -9.07085555e+05 -1.99165456e+04\n",
      " -1.36957369e+04 -7.81520865e+05 -4.26943914e+04  3.48643253e+03\n",
      " -1.31045400e+06 -2.31906686e+04 -2.35318275e+06 -2.73943435e+06\n",
      " -2.23140214e+06 -2.13110190e+06 -2.13111421e+06 -2.65561989e+06\n",
      " -2.64191310e+06 -2.64189862e+06 -2.57080491e+06]\n",
      "Gradient Descent(41/99): loss=629277928913187.4, weights = [-3.14584311e-01  9.87660484e+05 -1.00269569e+06 -1.44589215e+05\n",
      "  3.21395001e+06  3.83986451e+06  3.72606453e+06  3.83960699e+06\n",
      " -1.81703918e+06  1.54024663e+06  3.63724774e+06  2.34495132e+05\n",
      "  2.24895761e+06  3.83991318e+06  1.31840716e+06  2.89477680e+04\n",
      "  1.99061137e+04  1.13590481e+06  6.20543082e+04 -5.06735985e+03\n",
      "  1.90468437e+06  3.37065593e+04  3.42024170e+06  3.98163997e+06\n",
      "  3.24323902e+06  3.09745823e+06  3.09747473e+06  3.85982043e+06\n",
      "  3.83989647e+06  3.83987665e+06  3.73654469e+06]\n",
      "Gradient Descent(42/99): loss=1329366531358210.8, weights = [-3.14600229e-01 -1.43551838e+06  1.45737078e+06  2.10153087e+05\n",
      " -4.67132705e+06 -5.58106358e+06 -5.41566086e+06 -5.58068962e+06\n",
      "  2.64098239e+06 -2.23867683e+06 -5.28657019e+06 -3.40828058e+05\n",
      " -3.26875481e+06 -5.58113532e+06 -1.91624285e+06 -4.20742360e+04\n",
      " -2.89326114e+04 -1.65098411e+06 -9.01930483e+04  7.36517879e+03\n",
      " -2.76836952e+06 -4.89909117e+04 -4.97116186e+06 -5.78712855e+06\n",
      " -4.71389694e+06 -4.50201040e+06 -4.50203579e+06 -5.61006864e+06\n",
      " -5.58111187e+06 -5.58108183e+06 -5.43089420e+06]\n",
      "Gradient Descent(43/99): loss=2808322512991199.0, weights = [-3.14613013e-01  2.08645925e+06 -2.11822126e+06 -3.05448328e+05\n",
      "  6.78955593e+06  8.11181664e+06  7.87141126e+06  8.11127278e+06\n",
      " -3.83854434e+06  3.25381243e+06  7.68378357e+06  4.95377402e+05\n",
      "  4.75098323e+06  8.11191991e+06  2.78517029e+06  6.11529393e+04\n",
      "  4.20522008e+04  2.39962915e+06  1.31091394e+05 -1.07049373e+04\n",
      "  4.02369667e+06  7.12060110e+04  7.22535240e+06  8.41132154e+06\n",
      "  6.85142959e+06  6.54346333e+06  6.54349883e+06  8.15397408e+06\n",
      "  8.11188499e+06  8.11184256e+06  7.89355128e+06]\n",
      "Gradient Descent(44/99): loss=5932656758640751.0, weights = [-3.14623168e-01 -3.03257121e+06  3.07873524e+06  4.43954316e+05\n",
      " -9.86830372e+06 -1.17901471e+07 -1.14407292e+07 -1.17893569e+07\n",
      "  5.57914599e+06 -4.72926491e+06 -1.11680216e+07 -7.20008291e+05\n",
      " -6.90533252e+06 -1.17902982e+07 -4.04811476e+06 -8.88829523e+04\n",
      " -6.11209180e+04 -3.48774857e+06 -1.90535237e+05  1.55591347e+04\n",
      " -5.84825522e+06 -1.03494618e+05 -1.05017131e+07 -1.22254644e+07\n",
      " -9.95823389e+06 -9.51061833e+06 -9.51067132e+06 -1.18514210e+07\n",
      " -1.17902482e+07 -1.17901853e+07 -1.14729097e+07]\n",
      "Gradient Descent(45/99): loss=1.253289679266855e+16, weights = [-3.14631396e-01  4.40770115e+06 -4.47479900e+06 -6.45267525e+05\n",
      "  1.43431195e+07  1.71364303e+07  1.66285676e+07  1.71352815e+07\n",
      " -8.10902830e+06  6.87376622e+06  1.62321992e+07  1.04649821e+06\n",
      "  1.00365796e+07  1.71366489e+07  5.88374682e+06  1.29187229e+05\n",
      "  8.88364050e+04  5.06928076e+06  2.76934092e+05 -2.26144709e+04\n",
      "  8.50016709e+06  1.50424610e+05  1.52637510e+07  1.77691429e+07\n",
      "  1.44738291e+07  1.38232417e+07  1.38233173e+07  1.72254890e+07\n",
      "  1.71365755e+07  1.71364853e+07  1.66753394e+07]\n",
      "Gradient Descent(46/99): loss=2.647608119025362e+16, weights = [-3.14637826e-01 -6.40638822e+06  6.50391124e+06  9.37865694e+05\n",
      " -2.08470566e+07 -2.49070027e+07 -2.41688481e+07 -2.49053334e+07\n",
      "  1.17860955e+07 -9.99069915e+06 -2.35927456e+07 -1.52103672e+06\n",
      " -1.45877006e+07 -2.49073215e+07 -8.55175144e+06 -1.87767623e+05\n",
      " -1.29119580e+05 -7.36796286e+06 -4.02510807e+05  3.28690869e+04\n",
      " -1.23545965e+07 -2.18635163e+05 -2.21851511e+07 -2.58266219e+07\n",
      " -2.10370366e+07 -2.00914371e+07 -2.00915485e+07 -2.50364455e+07\n",
      " -2.49072156e+07 -2.49070833e+07 -2.42368297e+07]\n",
      "Gradient Descent(47/99): loss=5.5931432835461304e+16, weights = [-3.14643191e-01  9.31138705e+06 -9.45313280e+06 -1.36314471e+06\n",
      "  3.03002254e+07  3.62011691e+07  3.51282956e+07  3.61987424e+07\n",
      " -1.71305404e+07  1.45210158e+07  3.42909570e+07  2.21075572e+06\n",
      "  2.12025438e+07  3.62016314e+07  1.24295730e+07  2.72911493e+05\n",
      "  1.87669294e+05  1.07089915e+07  5.85030710e+05 -4.77736805e+04\n",
      "  1.79568311e+07  3.17776029e+05  3.22450840e+07  3.75377918e+07\n",
      "  3.05763529e+07  2.92019690e+07  2.92021295e+07  3.63893081e+07\n",
      "  3.62014767e+07  3.62012856e+07  3.52271027e+07]\n",
      "Gradient Descent(48/99): loss=1.181566545497434e+17, weights = [-3.14647163e-01 -1.35336673e+07  1.37396876e+07  1.98126675e+06\n",
      " -4.40399669e+07 -5.26167127e+07 -5.10573411e+07 -5.26131860e+07\n",
      "  2.48984432e+07 -2.11056203e+07 -4.98403089e+07 -3.21323080e+06\n",
      " -3.08169096e+07 -5.26173856e+07 -1.80658049e+07 -3.96664148e+05\n",
      " -2.72768581e+05 -1.55650198e+07 -8.50314893e+05  6.94368262e+04\n",
      " -2.60994173e+07 -4.61872657e+05 -4.68667277e+07 -5.45594325e+07\n",
      " -4.44413050e+07 -4.24437011e+07 -4.24439357e+07 -5.28901641e+07\n",
      " -5.26171616e+07 -5.26168827e+07 -5.12009537e+07]\n",
      "Gradient Descent(49/99): loss=2.4960910719840698e+17, weights = [-3.14650805e-01  1.96705552e+07 -1.99699967e+07 -2.87967955e+06\n",
      "  6.40100405e+07  7.64759420e+07  7.42094679e+07  7.64708158e+07\n",
      " -3.61887271e+07  3.06760361e+07  7.24405679e+07  4.67028100e+06\n",
      "  4.47909433e+07  7.64769191e+07  2.62578066e+07  5.76532862e+05\n",
      "  3.96456426e+05  2.26230320e+07  1.23589309e+06 -1.00923187e+05\n",
      "  3.79342880e+07  6.71310398e+05  6.81186062e+07  7.92995939e+07\n",
      "  6.45933670e+07  6.16899438e+07  6.16902833e+07  7.68733908e+07\n",
      "  7.64765927e+07  7.64761885e+07  7.44182011e+07]\n",
      "Gradient Descent(50/99): loss=5.273059450930536e+17, weights = [-3.14653044e-01 -2.85902358e+07  2.90254594e+07  4.18547951e+06\n",
      " -9.30356131e+07 -1.11154219e+08 -1.07860005e+08 -1.11146769e+08\n",
      "  5.25986300e+07 -4.45861899e+07 -1.05288992e+08 -6.78803624e+06\n",
      " -6.51015497e+07 -1.11155641e+08 -3.81644985e+07 -8.37963667e+05\n",
      " -5.76230954e+05 -3.28815227e+07 -1.79631306e+06  1.46687158e+05\n",
      " -5.51357204e+07 -9.75718397e+05 -9.90072215e+07 -1.15258267e+08\n",
      " -9.38834508e+07 -8.96634593e+07 -8.96639542e+07 -1.11731893e+08\n",
      " -1.11155167e+08 -1.11154578e+08 -1.08163390e+08]\n",
      "Gradient Descent(51/99): loss=1.1139479759024428e+18, weights = [-3.14655819e-01  4.15545764e+07 -4.21871542e+07 -6.08340014e+06\n",
      "  1.35222930e+08  1.61557482e+08  1.56769495e+08  1.61546653e+08\n",
      " -7.64496585e+07  6.48039505e+07  1.53032646e+08  9.86609423e+06\n",
      "  9.46220709e+07  1.61559546e+08  5.54703217e+07  1.21794116e+06\n",
      "  8.37524853e+05  4.77917626e+07  2.61085737e+06 -2.13202946e+05\n",
      "  8.01372023e+07  1.41816125e+06  1.43902386e+08  1.67522523e+08\n",
      "  1.36455223e+08  1.30321663e+08  1.30322381e+08  1.62397103e+08\n",
      "  1.61558857e+08  1.61558003e+08  1.57210449e+08]\n",
      "Gradient Descent(52/99): loss=2.3532450270367703e+18, weights = [-3.14656614e-01 -6.03976414e+07  6.13170632e+07  8.84193818e+06\n",
      " -1.96540232e+08 -2.34816275e+08 -2.27857159e+08 -2.34800536e+08\n",
      "  1.11116019e+08 -9.41895241e+07 -2.22425824e+08 -1.43399088e+07\n",
      " -1.37528773e+08 -2.34819277e+08 -8.06235287e+07 -1.77022076e+06\n",
      " -1.21730337e+06 -6.94631005e+07 -3.79475959e+06  3.09880559e+05\n",
      " -1.16475691e+08 -2.06123133e+06 -2.09155416e+08 -2.43486185e+08\n",
      " -1.98331312e+08 -1.89416467e+08 -1.89417512e+08 -2.36036626e+08\n",
      " -2.34818276e+08 -2.34817033e+08 -2.28498067e+08]\n",
      "Gradient Descent(53/99): loss=4.971293343198483e+18, weights = [-3.14659328e-01  8.77851590e+07 -8.91214963e+07 -1.28513460e+07\n",
      "  2.85662074e+08  3.41294521e+08  3.31179769e+08  3.41271644e+08\n",
      " -1.61501958e+08  1.36900087e+08  3.23285576e+08  2.08423893e+07\n",
      "  1.99891667e+08  3.41298882e+08  1.17182545e+08  2.57293343e+06\n",
      "  1.76929375e+06  1.00961382e+08  5.51550633e+06 -4.50396954e+05\n",
      "  1.69291993e+08  2.99590374e+06  3.03997656e+08  3.53895830e+08\n",
      "  2.88265325e+08  2.75308014e+08  2.75309531e+08  3.43068243e+08\n",
      "  3.41297427e+08  3.41295621e+08  3.32111298e+08]\n",
      "Gradient Descent(54/99): loss=1.0501990749025145e+19, weights = [-3.14658459e-01 -1.27591640e+08  1.29533944e+08  1.86788322e+07\n",
      " -4.15196520e+08 -4.96055690e+08 -4.81354370e+08 -4.96022440e+08\n",
      "  2.34735575e+08 -1.98977900e+08 -4.69880528e+08 -3.02934424e+07\n",
      " -2.90533228e+08 -4.96062031e+08 -1.70319371e+08 -3.73963891e+06\n",
      " -2.57158607e+06 -1.46742666e+08 -8.01653158e+06  6.54631003e+05\n",
      " -2.46058027e+08 -4.35440656e+06 -4.41846435e+08 -5.14371108e+08\n",
      " -4.18980225e+08 -4.00147376e+08 -4.00149583e+08 -4.98633714e+08\n",
      " -4.96059917e+08 -4.96057291e+08 -4.82708306e+08]\n",
      "Gradient Descent(55/99): loss=2.2185737609611567e+19, weights = [-3.14662133e-01  1.85448507e+08 -1.88271555e+08 -2.71488133e+07\n",
      "  6.03468806e+08  7.20993844e+08  6.99626159e+08  7.20945517e+08\n",
      " -3.41177225e+08  2.89205111e+08  6.82949464e+08  4.40301074e+07\n",
      "  4.22276516e+08  7.21003059e+08  2.47551274e+08  5.43539099e+06\n",
      "  3.73768059e+06  2.13283632e+08  1.16516553e+07 -9.51475670e+05\n",
      "  3.57633883e+08  6.32892715e+06  6.42203215e+08  7.47614449e+08\n",
      "  6.08968244e+08  5.81595577e+08  5.81598782e+08  7.24740881e+08\n",
      "  7.20999985e+08  7.20996170e+08  7.01594041e+08]\n",
      "Gradient Descent(56/99): loss=4.686796675461037e+19, weights = [-3.14658757e-01 -2.69540768e+08  2.73643936e+08  3.94595352e+07\n",
      " -8.77113804e+08 -1.04793097e+09 -1.01687404e+09 -1.04786073e+09\n",
      "  4.95885208e+08 -4.20346160e+08 -9.92635240e+08 -6.39957109e+07\n",
      " -6.13759250e+08 -1.04794437e+09 -3.59804247e+08 -7.90008767e+06\n",
      " -5.43254467e+06 -3.09997825e+08 -1.69351383e+07  1.38292558e+06\n",
      " -5.19804192e+08 -9.19880085e+06 -9.33412464e+08 -1.08662278e+09\n",
      " -8.85106982e+08 -8.45322083e+08 -8.45326744e+08 -1.05337712e+09\n",
      " -1.04793990e+09 -1.04793435e+09 -1.01973426e+09]\n",
      "Gradient Descent(57/99): loss=9.900983894984948e+19, weights = [-3.14665300e-01  3.91764953e+08 -3.97728718e+08 -5.73525973e+07\n",
      "  1.27484406e+09  1.52311887e+09  1.47797906e+09  1.52301677e+09\n",
      " -7.20746055e+08  6.10953567e+08  1.44274909e+09  9.30147850e+07\n",
      "  8.92070486e+08  1.52313833e+09  5.22958716e+08  1.14824095e+07\n",
      "  7.89595067e+06  4.50567403e+08  2.46144349e+07 -2.01001792e+06\n",
      "  7.55511185e+08  1.33700286e+07  1.35667155e+09  1.57935561e+09\n",
      "  1.28646178e+09  1.22863628e+09  1.22864305e+09  1.53103458e+09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.52313184e+09  1.52312378e+09  1.48213626e+09]\n",
      "Gradient Descent(58/99): loss=2.0916094483468658e+20, weights = [-3.14657016e-01 -5.69412113e+08  5.78080168e+08  8.33593285e+07\n",
      " -1.85292648e+09 -2.21378234e+09 -2.14817373e+09 -2.21363395e+09\n",
      "  1.04757082e+09 -8.87992556e+08 -2.09696861e+09 -1.35192658e+08\n",
      " -1.29658290e+09 -2.21381063e+09 -7.60096138e+08 -1.66891475e+07\n",
      " -1.14763966e+07 -6.54878735e+08 -3.57759347e+07  2.92146744e+06\n",
      " -1.09810032e+09 -1.94327139e+07 -1.97185891e+09 -2.29551982e+09\n",
      " -1.86981228e+09 -1.78576561e+09 -1.78577546e+09 -2.22528746e+09\n",
      " -2.21380119e+09 -2.21378948e+09 -2.15421603e+09]\n",
      "Gradient Descent(59/99): loss=4.418581153969826e+20, weights = [-3.14669757e-01  8.27613988e+08 -8.40212602e+08 -1.21158902e+08\n",
      "  2.69314234e+09  3.21762952e+09  3.12227047e+09  3.21741385e+09\n",
      " -1.52259540e+09  1.29065582e+09  3.04784622e+09  1.96496233e+08\n",
      "  1.88452287e+09  3.21767065e+09  1.10476434e+09  2.42568986e+07\n",
      "  1.66804079e+07  9.51835743e+08  5.19986550e+07 -4.24621686e+06\n",
      "  1.59603768e+09  2.82445447e+07  2.86600509e+09  3.33643116e+09\n",
      "  2.71768507e+09  2.59552715e+09  2.59554146e+09  3.23435169e+09\n",
      "  3.21765693e+09  3.21763990e+09  3.13105267e+09]\n",
      "Gradient Descent(60/99): loss=9.33437139980809e+20, weights = [-3.14651722e-01 -1.20289839e+09  1.22120989e+09  1.76098821e+08\n",
      " -3.91435696e+09 -4.67667466e+09 -4.53807471e+09 -4.67636119e+09\n",
      "  2.21302151e+09 -1.87590813e+09 -4.42990253e+09 -2.85598123e+08\n",
      " -2.73906622e+09 -4.67673444e+09 -1.60572351e+09 -3.52562724e+07\n",
      " -2.42441961e+07 -1.38344892e+09 -7.55776233e+07  6.17167845e+06\n",
      " -2.31976643e+09 -4.10521303e+07 -4.16560492e+09 -4.84934730e+09\n",
      " -3.95002868e+09 -3.77247783e+09 -3.77249863e+09 -4.70097954e+09\n",
      " -4.67671450e+09 -4.67668975e+09 -4.55083924e+09]\n",
      "Gradient Descent(61/99): loss=1.9719110364482754e+21, weights = [-3.14678727e-01  1.74835679e+09 -1.77497169e+09 -2.55951436e+08\n",
      "  5.68933554e+09  6.79732882e+09  6.59588025e+09  6.79687320e+09\n",
      " -3.21652370e+09  2.72654510e+09  6.43865702e+09  4.15103571e+08\n",
      "  3.98110519e+09  6.79741570e+09  2.33384434e+09  5.12433499e+07\n",
      "  3.52378099e+07  2.01077858e+09  1.09848556e+08 -8.97024719e+06\n",
      "  3.37167247e+09  5.96673595e+07  6.05451275e+09  7.04830046e+09\n",
      "  5.74118272e+09  5.48312084e+09  5.48315107e+09  6.83265483e+09\n",
      "  6.79738672e+09  6.79735074e+09  6.61443289e+09]\n",
      "Gradient Descent(62/99): loss=4.1657150429502484e+21, weights = [-3.14639741e-01 -2.54115516e+09  2.57983869e+09  3.72013492e+08\n",
      " -8.26918425e+09 -9.87960087e+09 -9.58680475e+09 -9.87893865e+09\n",
      "  4.67506740e+09 -3.96290633e+09 -9.35828812e+09 -6.03333708e+08\n",
      " -5.78635098e+09 -9.87972714e+09 -3.39213406e+09 -7.44798226e+07\n",
      " -5.12165156e+07 -2.92257302e+09 -1.59659760e+08  1.30378365e+07\n",
      " -4.90056891e+09 -8.67237282e+07 -8.79995231e+09 -1.02443765e+10\n",
      " -8.34454170e+09 -7.96946078e+09 -7.96950471e+09 -9.93094559e+09\n",
      " -9.87968503e+09 -9.87963274e+09 -9.61377015e+09]\n",
      "Gradient Descent(63/99): loss=8.800184946638262e+21, weights = [-3.14696403e-01  3.69345068e+09 -3.74967539e+09 -5.40704285e+08\n",
      "  1.20188742e+10  1.43595397e+10  1.39339742e+10  1.43585772e+10\n",
      " -6.79499273e+09  5.75989979e+09  1.36018359e+10  8.76917445e+08\n",
      "  8.41019167e+09  1.43597233e+10  4.93030888e+09  1.08252953e+08\n",
      "  7.44408201e+07  4.24782377e+09  2.32058025e+08 -1.89498882e+07\n",
      "  7.12274868e+09  1.26048900e+08  1.27903208e+10  1.48897241e+10\n",
      "  1.21284027e+10  1.15832401e+10  1.15833039e+10  1.44341669e+10\n",
      "  1.43596621e+10  1.43595861e+10  1.39731672e+10]\n",
      "Gradient Descent(64/99): loss=1.8590627130412605e+22, weights = [-3.14614637e-01 -5.36825855e+09  5.44997855e+09  7.85888496e+08\n",
      " -1.74688739e+10 -2.08709223e+10 -2.02523826e+10 -2.08695233e+10\n",
      "  9.87620548e+09 -8.37174609e+09 -1.97696351e+10 -1.27455867e+09\n",
      " -1.22238219e+10 -2.08711890e+10 -7.16597434e+09 -1.57340626e+08\n",
      " -1.08196265e+08 -6.17401402e+09 -3.37285532e+08  2.75427800e+07\n",
      " -1.03525835e+10 -1.83206206e+08 -1.85901357e+10 -2.16415205e+10\n",
      " -1.76280685e+10 -1.68357000e+10 -1.68357928e+10 -2.09793894e+10\n",
      " -2.08711001e+10 -2.08709896e+10 -2.03093477e+10]\n",
      "Gradient Descent(65/99): loss=3.927319927907389e+22, weights = [-3.14734519e-01  7.80251379e+09 -7.92129001e+09 -1.14225233e+09\n",
      "  2.53901946e+10  3.03349135e+10  2.94358949e+10  3.03328802e+10\n",
      " -1.43546047e+10  1.21679430e+10  2.87342439e+10  1.85251166e+09\n",
      "  1.77667559e+10  3.03353012e+10  1.04154100e+10  2.28687273e+08\n",
      "  1.57258232e+08  8.97364184e+09  4.90228813e+08 -4.00321479e+07\n",
      "  1.50469979e+10  2.66281688e+08  2.70198964e+10  3.14549422e+10\n",
      "  2.56215766e+10  2.44699059e+10  2.44700408e+10  3.04925654e+10\n",
      "  3.03351719e+10  3.03350114e+10  2.95186911e+10]\n",
      "Gradient Descent(66/99): loss=8.296568861255178e+22, weights = [-3.14561150e-01 -1.13405904e+10  1.15132261e+10  1.66021056e+09\n",
      " -3.69034654e+10 -4.40903840e+10 -4.27837024e+10 -4.40874287e+10\n",
      "  2.08637494e+10 -1.76855385e+10 -4.17638852e+10 -2.69253943e+09\n",
      " -2.58231522e+10 -4.40909475e+10 -1.51383133e+10 -3.32386300e+08\n",
      " -2.28567517e+08 -1.30427705e+10 -7.12524748e+08  5.81848626e+07\n",
      " -2.18701107e+10 -3.87028032e+08 -3.92721611e+10 -4.57182935e+10\n",
      " -3.72397684e+10 -3.55658686e+10 -3.55660646e+10 -4.43195237e+10\n",
      " -4.40907596e+10 -4.40905262e+10 -4.29040427e+10]\n",
      "Gradient Descent(67/99): loss=1.7526724619612375e+23, weights = [-3.14811675e-01  1.64830200e+10 -1.67339380e+10 -2.41303873e+09\n",
      "  5.36374683e+10  6.40833198e+10  6.21841189e+10  6.40790244e+10\n",
      " -3.03244881e+10  2.57051066e+10  6.07018622e+10  3.91348066e+09\n",
      "  3.75327491e+10  6.40841388e+10  2.20028334e+10  4.83108007e+08\n",
      "  3.32212241e+08  1.89570595e+10  1.03562154e+09 -8.45689880e+07\n",
      "  3.17871874e+10  5.62527219e+08  5.70802571e+10  6.64494104e+10\n",
      "  5.41262690e+10  5.16933336e+10  5.16936186e+10  6.44163636e+10\n",
      "  6.40838657e+10  6.40835265e+10  6.23590280e+10]\n",
      "Gradient Descent(68/99): loss=3.702567664161517e+23, weights = [-3.14444750e-01 -2.39573019e+10  2.43219995e+10  3.50723942e+09\n",
      " -7.79595622e+10 -9.31421208e+10 -9.03817207e+10 -9.31358776e+10\n",
      "  4.40752312e+10 -3.73611753e+10 -8.82273296e+10 -5.68806188e+09\n",
      " -5.45521028e+10 -9.31433113e+10 -3.19800936e+10 -7.02174989e+08\n",
      " -4.82855021e+08 -2.75532031e+10 -1.50522767e+09  1.22917086e+08\n",
      " -4.62011965e+10 -8.17607115e+08 -8.29634954e+10 -9.65811233e+10\n",
      " -7.86700112e+10 -7.51338529e+10 -7.51342671e+10 -9.36261845e+10\n",
      " -9.31429142e+10 -9.31424213e+10 -9.06359430e+10]\n",
      "Gradient Descent(69/99): loss=7.821773665773314e+23, weights = [-3.14979450e-01  3.48208226e+10 -3.53508935e+10 -5.09760915e+09\n",
      "  1.13310593e+11  1.35377735e+11  1.31365622e+11  1.35368661e+11\n",
      " -6.40612958e+10  5.43027283e+10  1.28234315e+11  8.26733303e+09\n",
      "  7.92889408e+10  1.35379465e+11  4.64815766e+10  1.02057865e+09\n",
      "  7.01807285e+08  4.00472974e+10  2.18777832e+09 -1.78654259e+08\n",
      "  6.71512875e+10  1.18835386e+09  1.20583577e+11  1.40376165e+11\n",
      "  1.14343197e+11  1.09203556e+11  1.09204158e+11  1.36081299e+11\n",
      "  1.35378888e+11  1.35378171e+11  1.31735122e+11]\n",
      "Gradient Descent(70/99): loss=1.6523707013046393e+24, weights = [-3.14203750e-01 -5.06104440e+10  5.13808774e+10  7.40913750e+09\n",
      " -1.64691670e+11 -1.96765233e+11 -1.90933813e+11 -1.96752044e+11\n",
      "  9.31101100e+10 -7.89264866e+10 -1.86382604e+11 -1.20161835e+10\n",
      " -1.15242783e+11 -1.96767747e+11 -6.75588069e+10 -1.48336353e+09\n",
      " -1.02004420e+09 -5.82068817e+10 -3.17983390e+09  2.59665646e+08\n",
      " -9.76012689e+10 -1.72721700e+09 -1.75262613e+11 -2.04030218e+11\n",
      " -1.66192512e+11 -1.58722283e+11 -1.58723158e+11 -1.97787830e+11\n",
      " -1.96766909e+11 -1.96765867e+11 -1.91470865e+11]\n",
      "Gradient Descent(71/99): loss=3.4906775000118106e+24, weights = [-3.15330450e-01  7.35599233e+10 -7.46797123e+10 -1.07688363e+10\n",
      "  2.39371672e+11  2.85989102e+11  2.77513405e+11  2.85969932e+11\n",
      " -1.35331209e+11  1.14715972e+11  2.70898435e+11  1.74649632e+10\n",
      "  1.67500018e+11  2.85992757e+11  9.81935795e+10  2.15599981e+09\n",
      "  1.48258674e+09  8.46009918e+10  4.62174048e+09 -3.77411923e+08\n",
      "  1.41858899e+11  2.51042946e+09  2.54736046e+11  2.96548419e+11\n",
      "  2.41553077e+11  2.30695446e+11  2.30696718e+11  2.87475400e+11\n",
      "  2.85991538e+11  2.85990024e+11  2.78293984e+11]\n",
      "Gradient Descent(72/99): loss=7.374150001248588e+24, weights = [-3.13713350e-01 -1.06915923e+11  1.08543484e+11  1.56520020e+10\n",
      " -3.47915577e+11 -4.15671840e+11 -4.03352810e+11 -4.15643978e+11\n",
      "  1.96697609e+11 -1.66734323e+11 -3.93738259e+11 -2.53845106e+10\n",
      " -2.43453475e+11 -4.15677153e+11 -1.42719795e+11 -3.13364531e+09\n",
      " -2.15487078e+09 -1.22963602e+11 -6.71748455e+09  5.48550653e+08\n",
      " -2.06185303e+11 -3.64879231e+09 -3.70246979e+11 -4.31019317e+11\n",
      " -3.51086147e+11 -3.35305087e+11 -3.35306935e+11 -4.17832105e+11\n",
      " -4.15675381e+11 -4.15673181e+11 -4.04487346e+11]\n",
      "Gradient Descent(73/99): loss=1.557808999563283e+25, weights = [-3.16082850e-01  1.55397314e+11 -1.57762899e+11 -2.27494560e+10\n",
      "  5.05679088e+11  6.04159661e+11  5.86254524e+11  6.04119165e+11\n",
      " -2.85890814e+11  2.42340574e+11  5.72280222e+11  3.68952039e+10\n",
      "  3.53848287e+11  6.04167383e+11  2.07436575e+11  4.55460752e+09\n",
      "  3.13200433e+09  1.78721868e+11  9.76355094e+09 -7.97292826e+08\n",
      "  2.99680735e+11  5.30334969e+09  5.38136740e+11  6.26466503e+11\n",
      "  5.10287364e+11  4.87350328e+11  4.87353014e+11  6.07299505e+11\n",
      "  6.04164807e+11  6.04161610e+11  5.87903519e+11]\n",
      "Gradient Descent(74/99): loss=3.290913364536237e+25, weights = [-3.12630650e-01 -2.25862759e+11  2.29301026e+11  3.30652748e+10\n",
      " -7.34981003e+11 -8.78117930e+11 -8.52093648e+11 -8.78059071e+11\n",
      "  4.15528983e+11 -3.52230738e+11 -8.31782650e+11 -5.36254605e+10\n",
      " -5.14302006e+11 -8.78129154e+11 -3.01499401e+11 -6.61990991e+09\n",
      " -4.55222243e+09 -2.59763912e+11 -1.41908666e+10  1.15882799e+09\n",
      " -4.35571992e+11 -7.70817179e+09 -7.82156689e+11 -9.10539886e+11\n",
      " -7.41678919e+11 -7.08341005e+11 -7.08344909e+11 -8.82681548e+11\n",
      " -8.78125410e+11 -8.78120763e+11 -8.54490385e+11]\n",
      "Gradient Descent(75/99): loss=6.952142898082603e+25, weights = [-3.17729450e-01  3.28281001e+11 -3.33278362e+11 -4.80588370e+10\n",
      "  1.06826066e+12  1.27630352e+12  1.23847843e+12  1.27621797e+12\n",
      " -6.03952025e+11  5.11950973e+11  1.20895734e+12  7.79421094e+10\n",
      "  7.47514014e+11  1.27631983e+12  4.38215337e+11  9.62173075e+09\n",
      "  6.61644329e+09  3.77554748e+11  2.06257637e+10 -1.68430252e+09\n",
      "  6.33083604e+11  1.12034687e+10  1.13682832e+12  1.32342732e+12\n",
      "  1.07799577e+12  1.02954066e+12  1.02954634e+12  1.28293652e+12\n",
      "  1.27631439e+12  1.27630763e+12  1.24196198e+12]\n",
      "Gradient Descent(76/99): loss=1.4686588652318316e+26, weights = [-3.10420650e-01 -4.77141147e+11  4.84404578e+11  6.98512813e+10\n",
      " -1.55266712e+12 -1.85504772e+12 -1.80007072e+12 -1.85492338e+12\n",
      "  8.77816141e+11 -7.44096897e+11 -1.75716320e+12 -1.13285226e+11\n",
      " -1.08647681e+12 -1.85507143e+12 -6.36925584e+11 -1.39847375e+10\n",
      " -9.61669218e+09 -5.48758243e+11 -2.99785869e+10  2.44805527e+09\n",
      " -9.20157535e+11 -1.62837200e+10 -1.65232702e+12 -1.92353997e+12\n",
      " -1.56681664e+12 -1.49638940e+12 -1.49639765e+12 -1.86468848e+12\n",
      " -1.85506352e+12 -1.85505370e+12 -1.80513390e+12]\n",
      "Gradient Descent(77/99): loss=3.1025813106041633e+26, weights = [-3.21009050e-01  6.93502435e+11 -7.04059494e+11 -1.01525584e+11\n",
      "  2.25672934e+12  2.69622547e+12  2.61631896e+12  2.69604474e+12\n",
      " -1.27586488e+12  1.08151018e+12  2.55395487e+12  1.64654800e+11\n",
      "  1.57914344e+12  2.69625993e+12  9.25741673e+11  2.03261647e+10\n",
      "  1.39774142e+10  7.97594549e+11  4.35724799e+10 -3.55813432e+09\n",
      "  1.33740612e+12  2.36676286e+10  2.40158037e+12  2.79577577e+12\n",
      "  2.27729502e+12  2.17493231e+12  2.17494430e+12  2.71023787e+12\n",
      "  2.69624843e+12  2.69623416e+12  2.62367805e+12]\n",
      "Gradient Descent(78/99): loss=6.554286374317947e+26, weights = [-3.05597850e-01 -1.00797349e+12  1.02331768e+12  1.47562708e+11\n",
      " -3.28005100e+12 -3.91883815e+12 -3.80269777e+12 -3.91857547e+12\n",
      "  1.85441018e+12 -1.57192469e+12 -3.71205446e+12 -2.39318084e+11\n",
      " -2.29521144e+12 -3.91888823e+12 -1.34552241e+12 -2.95431338e+10\n",
      " -2.03155206e+10 -1.15926653e+12 -6.33305703e+10  5.17158252e+09\n",
      " -1.94385751e+12 -3.43997960e+10 -3.49058522e+12 -4.06352987e+12\n",
      " -3.30994225e+12 -3.16116282e+12 -3.16118024e+12 -3.93920452e+12\n",
      " -3.91887153e+12 -3.91885079e+12 -3.81339385e+12]\n",
      "Gradient Descent(79/99): loss=1.384610605683188e+27, weights = [-3.27894650e-01  1.46504253e+12 -1.48734460e+12 -2.14475524e+11\n",
      "  4.76740136e+12  5.69584874e+12  5.52704410e+12  5.69546695e+12\n",
      " -2.69529884e+12  2.28471932e+12  5.39529828e+12  3.47837689e+11\n",
      "  3.33598294e+12  5.69592154e+12  1.95565416e+12  4.29395690e+10\n",
      "  2.95276630e+10  1.68493991e+12  9.20480344e+10 -7.51665434e+09\n",
      "  2.82530637e+12  4.99985014e+10  5.07340306e+12  5.90615142e+12\n",
      "  4.81084691e+12  4.59460293e+12  4.59462826e+12  5.72545031e+12\n",
      "  5.69589726e+12  5.69586711e+12  5.54259036e+12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/99): loss=2.925027104220557e+27, weights = [-2.95497850e-01 -2.12937108e+12  2.16178610e+12  3.11730186e+11\n",
      " -6.92919585e+12 -8.27865087e+12 -8.03330120e+12 -8.27809596e+12\n",
      "  3.91749133e+12 -3.32073313e+12 -7.84181477e+12 -5.05565882e+11\n",
      " -4.84869584e+12 -8.27875668e+12 -2.84245224e+12 -6.24106638e+10\n",
      " -4.29170831e+10 -2.44898169e+12 -1.33787531e+11  1.09251070e+10\n",
      " -4.10645122e+12 -7.26704932e+10 -7.37395506e+12 -8.58431603e+12\n",
      " -6.99234193e+12 -6.67804138e+12 -6.67807819e+12 -8.32167538e+12\n",
      " -8.27872139e+12 -8.27867758e+12 -8.05589696e+12]\n",
      "Gradient Descent(81/99): loss=6.179198343062931e+27, weights = [-3.42577850e-01  3.09494170e+12 -3.14205542e+12 -4.53085308e+11\n",
      "  1.00712635e+13  1.20326335e+13  1.16760292e+13  1.20318270e+13\n",
      " -5.69389120e+12  4.82653095e+12  1.13977126e+13  7.34816464e+11\n",
      "  7.04735358e+12  1.20327873e+13  4.13137195e+12  9.07109932e+10\n",
      "  6.23779815e+10  3.55948083e+12  1.94453946e+11 -1.58791342e+10\n",
      "  5.96853560e+12  1.05623177e+11  1.07177003e+13  1.24769036e+13\n",
      "  1.01630433e+13  9.70622213e+12  9.70627564e+12  1.20951676e+13\n",
      "  1.20327360e+13  1.20326723e+13  1.17088711e+13]\n",
      "Gradient Descent(82/99): loss=1.3053722513483176e+28, weights = [-2.73806650e-01 -4.49835362e+12  4.56683122e+12  6.58538393e+11\n",
      " -1.46381124e+13 -1.74888724e+13 -1.69705646e+13 -1.74877001e+13\n",
      "  8.27580569e+12 -7.01513795e+12 -1.65660444e+13 -1.06802151e+12\n",
      " -1.02430002e+13 -1.74890959e+13 -6.00475672e+12 -1.31844204e+11\n",
      " -9.06634910e+10 -5.17353962e+12 -2.82629754e+11  2.30795821e+10\n",
      " -8.67498853e+12 -1.53518369e+11 -1.55776782e+13 -1.81345982e+13\n",
      " -1.47715102e+13 -1.41075418e+13 -1.41076196e+13 -1.75797628e+13\n",
      " -1.74890214e+13 -1.74889288e+13 -1.70182988e+13]\n",
      "Gradient Descent(83/99): loss=2.7576339518266902e+28, weights = [-3.74088250e-01  6.53814749e+12 -6.63767649e+12 -9.57154885e+11\n",
      "  2.12758146e+13  2.54192615e+13  2.46659253e+13  2.54175577e+13\n",
      " -1.20284981e+13  1.01961763e+13  2.40779740e+13  1.55231952e+12\n",
      "  1.48877238e+13  2.54195864e+13  8.72763423e+12  1.91629410e+11\n",
      "  1.31775162e+11  7.51949889e+12  4.10789185e+11 -3.35450977e+10\n",
      "  1.26086918e+13  2.23131800e+11  2.26414297e+13  2.63577940e+13\n",
      "  2.14697021e+13  2.05046550e+13  2.05047681e+13  2.55513665e+13\n",
      "  2.54194780e+13  2.54193435e+13  2.47353047e+13]\n",
      "Gradient Descent(84/99): loss=5.825575811353835e+28, weights = [-2.28629050e-01 -9.50289288e+12  9.64755364e+12  1.39118005e+12\n",
      " -3.09234057e+13 -3.69457128e+13 -3.58507737e+13 -3.69432363e+13\n",
      "  1.74828619e+13 -1.48196674e+13 -3.49962139e+13 -2.25622413e+12\n",
      " -2.16386132e+13 -3.69461850e+13 -1.26852099e+13 -2.78524423e+11\n",
      " -1.91529061e+11 -1.09292414e+13 -5.97062949e+11  4.87562372e+10\n",
      " -1.83261463e+13 -3.24311680e+11 -3.29082637e+13 -3.83098260e+13\n",
      " -3.12052121e+13 -2.98025611e+13 -2.98027254e+13 -3.71377213e+13\n",
      " -3.69460275e+13 -3.69458320e+13 -3.59516134e+13]\n",
      "Gradient Descent(85/99): loss=1.2306685414628873e+29, weights = [-4.39176250e-01  1.38120122e+13 -1.40222699e+13 -2.02201541e+12\n",
      "  4.49457301e+13  5.36988729e+13  5.21074299e+13  5.36952735e+13\n",
      " -2.54105255e+13  2.15396964e+13  5.08653670e+13  3.27931669e+12\n",
      "  3.14507165e+13  5.36995592e+13  1.84373618e+13  4.04822278e+11\n",
      "  2.78378570e+11  1.58851434e+13  8.67803189e+11 -7.08649201e+10\n",
      "  2.66362002e+13  4.71371922e+11  4.78306286e+13  5.56815479e+13\n",
      "  4.53553225e+13  4.33166346e+13  4.33168734e+13  5.39779483e+13\n",
      "  5.36993303e+13  5.36990461e+13  5.22539957e+13]\n",
      "Gradient Descent(86/99): loss=2.5998203576625244e+29, weights = [-1.36008250e-01 -2.00751165e+13  2.03807163e+13  2.93890523e+12\n",
      " -6.53265254e+13 -7.80488108e+13 -7.57357226e+13 -7.80435793e+13\n",
      "  3.69330154e+13 -3.13069455e+13 -7.39304420e+13 -4.76633409e+12\n",
      " -4.57121517e+13 -7.80498084e+13 -2.67978467e+13 -5.88390327e+11\n",
      " -4.04610286e+11 -2.30883162e+13 -1.26131152e+12  1.02998861e+11\n",
      " -3.87144765e+13 -6.85117136e+11 -6.95195912e+13 -8.09305367e+13\n",
      " -6.59218490e+13 -6.29587111e+13 -6.29590582e+13 -7.84544339e+13\n",
      " -7.80494757e+13 -7.80490626e+13 -7.59487492e+13]\n",
      "Gradient Descent(87/99): loss=5.492190353774711e+29, weights = [-5.82088250e-01  2.91782469e+13 -2.96224219e+13 -4.27156188e+12\n",
      "  9.49490623e+13  1.13440312e+14  1.10078346e+14  1.13432708e+14\n",
      " -5.36804178e+13  4.55031873e+13  1.07454454e+14  6.92764462e+12\n",
      "  6.64404835e+13  1.13441762e+14  3.89494222e+13  8.55197938e+11\n",
      "  5.88082207e+11  3.35577923e+13  1.83325755e+12 -1.49704048e+11\n",
      "  5.62696886e+13  9.95785850e+11  1.01043488e+14  1.17628766e+14\n",
      "  9.58143374e+13  9.15075544e+13  9.15080589e+13  1.14029866e+14\n",
      "  1.13441278e+14  1.13440678e+14  1.10387970e+14]\n",
      "Gradient Descent(88/99): loss=1.1602399678574851e+30, weights = [ 6.56941500e-02 -4.24092230e+13  4.30548107e+13  6.20851625e+12\n",
      " -1.38004040e+14 -1.64880210e+14 -1.59993749e+14 -1.64869158e+14\n",
      "  7.80219873e+13 -6.61367636e+13 -1.56180045e+14 -1.00690088e+13\n",
      " -9.65681485e+13 -1.64882317e+14 -5.66111711e+13 -1.24299037e+12\n",
      " -8.54750100e+11 -4.87746882e+13 -2.66455447e+12  2.17587861e+11\n",
      " -8.17853718e+13 -1.44732836e+12 -1.46862004e+14 -1.70967933e+14\n",
      " -1.39261677e+14 -1.33001969e+14 -1.33002702e+14 -1.65737099e+14\n",
      " -1.64881614e+14 -1.64880742e+14 -1.60443773e+14]\n",
      "Gradient Descent(89/99): loss=2.4510381037480566e+30, weights = [-8.76693050e-01  6.16398305e+13 -6.25781622e+13 -9.02378923e+12\n",
      "  2.00582445e+14  2.39645706e+14  2.32543462e+14  2.39629643e+14\n",
      " -1.13401325e+14  9.61267057e+13  2.27000422e+14  1.46348354e+13\n",
      "  1.40357306e+14  2.39648769e+14  8.22817007e+13  1.80662862e+12\n",
      "  1.24233946e+12  7.08917378e+13  3.87280583e+12 -3.16253823e+11\n",
      "  1.18871229e+14  2.10362436e+12  2.13457083e+14  2.48493928e+14\n",
      "  2.02410362e+14  1.93312167e+14  1.93313233e+14  2.40891155e+14\n",
      "  2.39647747e+14  2.39646479e+14  2.33197552e+14]\n",
      "Gradient Descent(90/99): loss=5.177883845114065e+30, weights = [ 4.98948550e-01 -8.95906229e+13  9.09544443e+13  1.31156574e+13\n",
      " -2.91537242e+14 -3.48313873e+14 -3.37991093e+14 -3.48290526e+14\n",
      "  1.64823544e+14 -1.39715690e+14 -3.29934541e+14 -2.12710517e+13\n",
      " -2.04002808e+14 -3.48318325e+14 -1.19592620e+14 -2.62585056e+12\n",
      " -1.80568255e+12 -1.03037839e+14 -5.62894291e+12  4.59660203e+11\n",
      " -1.72773795e+14 -3.05752004e+12 -3.10249929e+14 -3.61174352e+14\n",
      " -2.94194034e+14 -2.80970232e+14 -2.80971781e+14 -3.50124075e+14\n",
      " -3.48316840e+14 -3.48314997e+14 -3.38941781e+14]\n",
      "Gradient Descent(91/99): loss=1.0938418734696605e+31, weights = [-1.49027385e+00  1.30215798e+14 -1.32198049e+14 -1.90629972e+13\n",
      "  4.23735804e+14  5.06257992e+14  4.91254312e+14  5.06224059e+14\n",
      " -2.39563345e+14  2.03070248e+14  4.79544490e+14  3.09164829e+13\n",
      "  2.96508581e+14  5.06264463e+14  1.73822303e+14  3.81655150e+12\n",
      "  2.62447549e+12  1.49760701e+14  8.18140637e+12 -6.68094697e+11\n",
      "  2.51118665e+14  4.44396298e+12  4.50933823e+14  5.24950100e+14\n",
      "  4.27597328e+14  4.08377147e+14  4.08379398e+14  5.08889038e+14\n",
      "  5.06262305e+14  5.06259626e+14  4.92636094e+14]\n",
      "Gradient Descent(92/99): loss=2.3107703454658515e+31, weights = [ 1.38890695e+00 -1.89262598e+14  1.92143707e+14  2.77071786e+13\n",
      " -6.15880257e+14 -7.35822414e+14 -7.14015264e+14 -7.35773093e+14\n",
      "  3.48194165e+14 -2.95153149e+14 -6.96995582e+14 -4.49356681e+13\n",
      " -4.30961413e+14 -7.35831819e+14 -2.52642623e+14 -5.54717986e+12\n",
      " -3.81455290e+12 -2.17670204e+14 -1.18912931e+13  9.71044528e+11\n",
      " -3.64989284e+14 -6.45909323e+12 -6.55411311e+14 -7.62990522e+14\n",
      " -6.21492802e+14 -5.93557164e+14 -5.93560436e+14 -7.39646516e+14\n",
      " -7.35828683e+14 -7.35824788e+14 -7.16023619e+14]\n",
      "Gradient Descent(93/99): loss=4.881564437231626e+31, weights = [-2.80724025e+00  2.75084372e+14 -2.79271930e+14 -4.02710937e+13\n",
      "  8.95153270e+14  1.06948361e+15  1.03778793e+15  1.06941192e+15\n",
      " -5.06084003e+14  4.28991355e+14  1.01305062e+15  6.53119008e+13\n",
      "  6.26382343e+14  1.06949728e+15  3.67204286e+14  8.06256758e+12\n",
      "  5.54427499e+12  3.16373504e+14  1.72834408e+13 -1.41136800e+12\n",
      "  5.30494926e+14  9.38799119e+12  9.52609816e+14  1.10897119e+15\n",
      "  9.03310843e+14  8.62707695e+14  8.62712450e+14  1.07504176e+15\n",
      "  1.06949272e+15  1.06948706e+15  1.04070698e+15]\n",
      "Gradient Descent(94/99): loss=1.0312436024463657e+32, weights = [ 3.23702215e+00 -3.99822323e+14  4.05908744e+14  5.85321592e+13\n",
      " -1.30106359e+15 -1.55444462e+15 -1.50837643e+15 -1.55434043e+15\n",
      "  7.35569529e+14 -6.23518956e+14 -1.47242189e+15 -9.49278061e+13\n",
      " -9.10417564e+14 -1.55446449e+15 -5.33714328e+14 -1.17185665e+13\n",
      " -8.05834549e+12 -4.59834153e+14 -2.51206762e+13  2.05135765e+12\n",
      " -7.71049668e+14 -1.36450080e+13 -1.38457400e+15 -1.61183798e+15\n",
      " -1.31292024e+15 -1.25390546e+15 -1.25391237e+15 -1.56252314e+15\n",
      " -1.55445786e+15 -1.55444963e+15 -1.51261913e+15]\n",
      "Gradient Descent(95/99): loss=2.178529816129312e+32, weights = [-5.56077625e+00  5.81123126e+14 -5.89969455e+14 -8.50737674e+13\n",
      "  1.89103533e+15  2.25931286e+15  2.19235489e+15  2.25916142e+15\n",
      " -1.06911605e+15  9.06255764e+14  2.14009665e+15  1.37973145e+14\n",
      "  1.32324953e+15  2.25934174e+15  7.75728920e+14  1.70323906e+13\n",
      "  1.17124299e+13  6.68347527e+14  3.65117330e+13 -2.98155281e+12\n",
      "  1.12068478e+15  1.98323836e+13  2.01241383e+15  2.34273143e+15\n",
      "  1.90826842e+15  1.82249318e+15  1.82250323e+15  2.27105461e+15\n",
      "  2.25933211e+15  2.25932015e+15  2.19852146e+15]\n",
      "Gradient Descent(96/99): loss=4.6022027661609724e+32, weights = [ 7.23021255e+00 -8.44635399e+14  8.57493126e+14  1.23650759e+14\n",
      " -2.74853179e+15 -3.28380602e+15 -3.18648573e+15 -3.28358591e+15\n",
      "  1.55391039e+15 -1.31720055e+15 -3.11053081e+15 -2.00537541e+14\n",
      " -1.92328156e+15 -3.28384799e+15 -1.12748586e+15 -2.47557865e+13\n",
      " -1.70234713e+13 -9.71412003e+14 -5.30681034e+13  4.33354816e+12\n",
      " -1.62886314e+15 -2.88254459e+13 -2.92494978e+15 -3.40505103e+15\n",
      " -2.77357928e+15 -2.64890896e+15 -2.64892357e+15 -3.30087211e+15\n",
      " -3.28383399e+15 -3.28381661e+15 -3.19544856e+15]\n",
      "Gradient Descent(97/99): loss=9.722276988841709e+32, weights = [-1.15524050e+01  1.22763822e+15 -1.24632632e+15 -1.79720619e+14\n",
      "  3.99486295e+15  4.77285912e+15  4.63140861e+15  4.77253920e+15\n",
      " -2.25853639e+15  1.91448965e+15  4.52101167e+15  2.91471976e+14\n",
      "  2.79540018e+15  4.77292012e+15  1.63874818e+15  3.59813828e+13\n",
      "  2.47428228e+13  1.41190211e+15  7.71320168e+13 -6.29861042e+12\n",
      "  2.36747671e+15  4.18964432e+13  4.25127828e+15  4.94908309e+15\n",
      "  4.03126832e+15  3.85006581e+15  3.85008704e+15  4.79766388e+15\n",
      "  4.77289978e+15  4.77287452e+15  4.64443566e+15]\n",
      "Gradient Descent(98/99): loss=2.053857134300236e+33, weights = [ 1.59923758e+01 -1.78431497e+15  1.81147727e+15  2.61215550e+14\n",
      " -5.80634724e+15 -6.93712844e+15 -6.73153671e+15 -6.93666346e+15\n",
      "  3.28267745e+15 -2.78262154e+15 -6.57107990e+15 -4.23640943e+14\n",
      " -4.06298397e+15 -6.93721711e+15 -2.38184417e+15 -5.22972641e+13\n",
      " -3.59625406e+13 -2.05213396e+15 -1.12107794e+14  9.15473691e+12\n",
      " -3.44101715e+15 -6.08945290e+13 -6.17903499e+15 -7.19326178e+15\n",
      " -5.85926075e+15 -5.59589135e+15 -5.59592220e+15 -6.97318101e+15\n",
      " -6.93718754e+15 -6.93715082e+15 -6.75047092e+15]\n",
      "Gradient Descent(99/99): loss=4.338828376271706e+33, weights = [-2.35225554e+01  2.59341871e+15 -2.63289784e+15 -3.79664637e+14\n",
      "  8.43925530e+15  1.00827931e+16  9.78397511e+15  1.00821173e+16\n",
      " -4.77121880e+15  4.04441082e+15  9.55075861e+15  6.15742380e+14\n",
      "  5.90535798e+15  1.00829220e+16  3.46189957e+15  7.60116378e+13\n",
      "  5.22698779e+13  2.98268113e+15  1.62943458e+14 -1.33059837e+13\n",
      "  5.00135816e+15  8.85073619e+13  8.98093959e+15  1.04550710e+16\n",
      "  8.51616262e+15  8.13336747e+15  8.13341230e+15  1.01351938e+16\n",
      "  1.00828790e+16  1.00828256e+16  9.81149510e+15]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.2\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best value of gamma  with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value of gamma = 0.15210526315789474 \n",
      " Loss = 0.8321530963774127 \n",
      " Weights = [-3.14661439e-01 -2.03264072e-03 -1.96002968e-01 -2.82277939e-01\n",
      "  2.08358085e-01  5.95045025e-02  4.87857000e-01 -2.15961526e-01\n",
      "  3.34670780e-01 -4.07955680e-02 -2.94801596e-01 -1.95590556e-01\n",
      "  9.03555119e-02 -1.65702895e-01  2.20102374e-01 -1.95789195e-04\n",
      " -8.02527133e-04  3.01758850e-01 -1.25736275e-03  2.86940278e-03\n",
      "  2.14222076e-02  1.19273918e-03  1.52317552e-02  2.06097772e-01\n",
      "  1.54387340e-01 -6.88034928e-02 -1.14005957e-01 -3.90716766e-02\n",
      "  3.54677448e-02 -9.75509062e-02 -6.83722541e-02]\n"
     ]
    }
   ],
   "source": [
    "from cost import compute_loss_rmse\n",
    "#from plots import *\n",
    "\n",
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.linspace(0.01, 0.16, 20)\n",
    "k_fold = 4\n",
    "seed = 6\n",
    "\n",
    "# prepare storage of the mean of the weights and rmse for train and test data\n",
    "ws = np.zeros((num_features, len(gammas)))\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    # prepare storage of weights and rmse for train and test data for each fold\n",
    "    ws_tmp = np.zeros((num_features, k_fold))\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross-validation\n",
    "    for i,k in enumerate(range(k_fold)):\n",
    "        tx_train, y_train, tx_test, y_test = cross_validation(y, tx, k, k_fold, seed)\n",
    "        w,_ = least_squares_GD(y_train, tx_train, w_initial, max_iters, gamma, printing=False)\n",
    "        # store weights and rmse for train and test data for each fold\n",
    "        ws_tmp[:, i] = w\n",
    "        rmse_tr.append(compute_loss_rmse(y_train, tx_train, w))\n",
    "        rmse_te.append(compute_loss_rmse(y_test, tx_test, w))\n",
    "    # store the mean of the weights and rmse for train and test data\n",
    "    ws[:, ind] = np.mean(ws_tmp, 1)\n",
    "    rmse_train.append(np.mean(rmse_tr))\n",
    "    rmse_test.append(np.mean(rmse_te))\n",
    "    \n",
    "loss = np.amin(rmse_test)\n",
    "weights = ws[:,np.argmin(rmse_test)]\n",
    "gamma_star = gammas[np.argmin(rmse_test)]\n",
    "\n",
    "#plot_cross_validation(gamma,\"Gamma\", gamma_star, rmse_train, rmse_test, loss)\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss, we = weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tx_test = build_model_data(tX_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "y_pred = predict_labels(weights, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
