{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "\n",
    "from cross_validation import *\n",
    "\n",
    "from plots import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21858528e+02  4.92398193e+01  8.11819816e+01  5.78959617e+01\n",
      "  2.40373503e+00  3.71783360e+02 -8.21688171e-01  2.37309984e+00\n",
      "  1.89173324e+01  1.58432217e+02  1.43760943e+00 -1.28304708e-01\n",
      "  4.58289801e-01  3.87074191e+01 -1.09730480e-02 -8.17107200e-03\n",
      "  4.66602072e+01 -1.95074680e-02  4.35429640e-02  4.17172345e+01\n",
      " -1.01191920e-02  2.09797178e+02  9.79176000e-01  8.48221045e+01\n",
      " -3.27458741e-03 -1.23928255e-02  5.76794744e+01 -1.18452642e-02\n",
      " -1.58228913e-03  7.30645914e+01]\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "tx_clean = remove_undefined_values (tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  3.14910656e-01,  6.83319669e-02, ...,\n",
       "         1.14381874e+00, -2.52714288e+00,  4.12510497e-01],\n",
       "       [ 1.00000000e+00,  7.40827026e-01,  5.52504823e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -2.73819964e-01],\n",
       "       [ 1.00000000e+00, -1.00190288e-12,  3.19515553e+00, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -2.93969845e-01],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -3.10930673e-01,  3.19316447e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -3.17017229e-01],\n",
       "       [ 1.00000000e+00, -5.10097335e-01, -8.45323970e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -7.45439413e-01],\n",
       "       [ 1.00000000e+00, -1.00190288e-12,  6.65336083e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -7.45439413e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODEL BUILDING\n",
    "\n",
    "tx, mean, std = standardize(tx_clean,0)\n",
    "y, tx = build_model_data(tx,y)\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00, -9.73058263e-13,  4.50019089e-15, -3.48448848e-15,\n",
       "        7.19675786e-15, -7.23439743e-12, -6.30188859e-12,  6.80668756e-13,\n",
       "        2.16429719e-14,  6.39742126e-15,  2.86409207e-15, -7.00447966e-15,\n",
       "        4.45924897e-15,  5.55781326e-13, -5.96492045e-15,  1.35646161e-16,\n",
       "        7.13136217e-17,  2.58030370e-14, -1.06327391e-16, -1.87188487e-16,\n",
       "        8.24369382e-15,  1.41040513e-16, -9.00283004e-15, -6.01698247e-16,\n",
       "        2.89129663e-12, -2.76637638e-15,  2.53942997e-14, -8.40723516e-12,\n",
       "        2.10209062e-14, -5.90180110e-15, -8.76751116e-16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] == -1:\n",
    "        y[i] = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compute_gradient import *\n",
    "from cost import *\n",
    "from implementations import *\n",
    "w = np.zeros((num_features,1))\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "\n",
    "gamma = 0.001\n",
    "losses = []\n",
    "w.shape,tx.shape\n",
    "y = y.reshape(num_samples,1)\n",
    "gradient = 0\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=1.5164181848147633\n",
      "loss=125102.92019608324\n",
      "Gradient Descent(1/99): loss=0.447014484520141\n",
      "loss=125158.64969908416\n",
      "Gradient Descent(2/99): loss=0.08908384627271973\n",
      "loss=125178.59546827854\n",
      "Gradient Descent(3/99): loss=0.06436608509869615\n",
      "loss=125286.84023782398\n",
      "Gradient Descent(4/99): loss=0.13409170465664452\n",
      "loss=125213.14045440257\n",
      "Gradient Descent(5/99): loss=0.13034417106402357\n",
      "loss=125243.31249051802\n",
      "Gradient Descent(6/99): loss=0.28414573113524527\n",
      "loss=125107.578425168\n",
      "Gradient Descent(7/99): loss=1.4492882919691072\n",
      "loss=125044.79285659581\n",
      "Gradient Descent(8/99): loss=0.8393079326212982\n",
      "loss=125050.1389223676\n",
      "Gradient Descent(9/99): loss=0.05430489293743843\n",
      "loss=125109.1972910757\n",
      "Gradient Descent(10/99): loss=0.03419215114925409\n",
      "loss=125108.54898459098\n",
      "Gradient Descent(11/99): loss=0.5716694037657313\n",
      "loss=125260.27247877914\n",
      "Gradient Descent(12/99): loss=0.5103258634603682\n",
      "loss=125116.12283455356\n",
      "Gradient Descent(13/99): loss=0.048056810526393004\n",
      "loss=125026.69275874214\n",
      "Gradient Descent(14/99): loss=0.3950634624856581\n",
      "loss=125023.82845485583\n",
      "Gradient Descent(15/99): loss=0.0065945777257760685\n",
      "loss=125015.78153133004\n",
      "Gradient Descent(16/99): loss=0.10098340736121113\n",
      "loss=125036.62724043152\n",
      "Gradient Descent(17/99): loss=0.7309343807484877\n",
      "loss=125377.25381254916\n",
      "Gradient Descent(18/99): loss=0.09431198258987851\n",
      "loss=125210.39625344942\n",
      "Gradient Descent(19/99): loss=0.9366772255371023\n",
      "loss=125074.02374777847\n",
      "Gradient Descent(20/99): loss=0.04078962038817026\n",
      "loss=125105.4525582915\n",
      "Gradient Descent(21/99): loss=0.5769994828006012\n",
      "loss=125049.57432731596\n",
      "Gradient Descent(22/99): loss=0.2937364879755172\n",
      "loss=125095.90255728315\n",
      "Gradient Descent(23/99): loss=0.9890687377766634\n",
      "loss=125075.04889876297\n",
      "Gradient Descent(24/99): loss=0.208988606751399\n",
      "loss=125110.19417726075\n",
      "Gradient Descent(25/99): loss=0.6873830022170202\n",
      "loss=125265.67836486216\n",
      "Gradient Descent(26/99): loss=0.044749010838081076\n",
      "loss=125051.25213948572\n",
      "Gradient Descent(27/99): loss=0.4863513296758393\n",
      "loss=125219.08036169643\n",
      "Gradient Descent(28/99): loss=1.0404741386733274\n",
      "loss=125502.45067472922\n",
      "Gradient Descent(29/99): loss=0.027772306740258913\n",
      "loss=125113.23445491903\n",
      "Gradient Descent(30/99): loss=0.4624625665963355\n",
      "loss=125114.12755017447\n",
      "Gradient Descent(31/99): loss=0.07490222417477208\n",
      "loss=125056.58387409891\n",
      "Gradient Descent(32/99): loss=0.16958596695036374\n",
      "loss=125128.47284588382\n",
      "Gradient Descent(33/99): loss=0.6094519239733613\n",
      "loss=125083.08256248283\n",
      "Gradient Descent(34/99): loss=0.8554099586279653\n",
      "loss=125004.41035640684\n",
      "Gradient Descent(35/99): loss=0.675863946110098\n",
      "loss=125162.31935417803\n",
      "Gradient Descent(36/99): loss=0.09762234878975641\n",
      "loss=125047.4294867912\n",
      "Gradient Descent(37/99): loss=0.8459671924086739\n",
      "loss=125154.91171072047\n",
      "Gradient Descent(38/99): loss=0.2824857640435744\n",
      "loss=125135.04049835485\n",
      "Gradient Descent(39/99): loss=0.8581711710380956\n",
      "loss=125252.12587330103\n",
      "Gradient Descent(40/99): loss=0.9704135323890808\n",
      "loss=125268.00989848518\n",
      "Gradient Descent(41/99): loss=0.8783968297882275\n",
      "loss=125080.84002846145\n",
      "Gradient Descent(42/99): loss=0.8800200485056051\n",
      "loss=125255.0254527019\n",
      "Gradient Descent(43/99): loss=0.4536613694324598\n",
      "loss=125002.81040975839\n",
      "Gradient Descent(44/99): loss=1.328926433672933\n",
      "loss=125401.42269931831\n",
      "Gradient Descent(45/99): loss=0.5539142546815239\n",
      "loss=125023.0945806392\n",
      "Gradient Descent(46/99): loss=0.2453766920757094\n",
      "loss=125312.18787959333\n",
      "Gradient Descent(47/99): loss=0.5517964062383063\n",
      "loss=125305.09499477688\n",
      "Gradient Descent(48/99): loss=0.16566165801452837\n",
      "loss=125131.24228537595\n",
      "Gradient Descent(49/99): loss=1.7228840100460932\n",
      "loss=125547.84095711652\n",
      "Gradient Descent(50/99): loss=0.4011140064993164\n",
      "loss=125195.45682900131\n",
      "Gradient Descent(51/99): loss=0.1126630371109729\n",
      "loss=125045.47098397126\n",
      "Gradient Descent(52/99): loss=0.19157359797238824\n",
      "loss=125295.07165052442\n",
      "Gradient Descent(53/99): loss=1.19156793766107\n",
      "loss=125131.1740247923\n",
      "Gradient Descent(54/99): loss=0.267281125404771\n",
      "loss=125070.94513648785\n",
      "Gradient Descent(55/99): loss=0.3713494586557593\n",
      "loss=125484.47866287112\n",
      "Gradient Descent(56/99): loss=0.44413384356757923\n",
      "loss=125260.17418178153\n",
      "Gradient Descent(57/99): loss=0.16943114404451914\n",
      "loss=125139.21532001726\n",
      "Gradient Descent(58/99): loss=0.01849152054909563\n",
      "loss=125215.68850790254\n",
      "Gradient Descent(59/99): loss=0.35664104722272827\n",
      "loss=125399.20166525414\n",
      "Gradient Descent(60/99): loss=0.48550127652077607\n",
      "loss=125110.50087400826\n",
      "Gradient Descent(61/99): loss=0.09597917744691024\n",
      "loss=125103.45692264927\n",
      "Gradient Descent(62/99): loss=0.22919994026789672\n",
      "loss=125022.9777234076\n",
      "Gradient Descent(63/99): loss=0.2986247096039172\n",
      "loss=125094.21831478644\n",
      "Gradient Descent(64/99): loss=0.42220134863437003\n",
      "loss=125566.69598411267\n",
      "Gradient Descent(65/99): loss=0.24790929286030283\n",
      "loss=125055.56120653436\n",
      "Gradient Descent(66/99): loss=0.20720091101379698\n",
      "loss=125107.51675478727\n",
      "Gradient Descent(67/99): loss=0.0031409803282735284\n",
      "loss=125134.00175275476\n",
      "Gradient Descent(68/99): loss=0.7449669864532696\n",
      "loss=125013.6754618878\n",
      "Gradient Descent(69/99): loss=1.5884961965746296\n",
      "loss=125487.93773511492\n"
     ]
    }
   ],
   "source": [
    "    for iter in range(max_iter):\n",
    "        for batch_y, batch_tx in batch_iter(y, tx, batch_size=1, num_batches = num_samples):\n",
    "            \n",
    "            gradient = logistic_gradient (batch_y,batch_tx,w)\n",
    "            w -= gamma*gradient\n",
    "            #print(\"Gradient={g}\".format(g=gradient))\n",
    "            #print(w)\n",
    "            loss = logistic_loss (batch_y,batch_tx,w)\n",
    "            # log info\n",
    "            #if iter % 100 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "            losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "                  bi=iter, ti=max_iter - 1, l=loss))\n",
    "            #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "\n",
    "        print(\"loss={l}\".format(l=logistic_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test_clean = remove_undefined_values (tX_test)\n",
    "tx_test, _, _ = standardize(tx_test_clean,0)\n",
    "y, tx_test = build_model_data(tx_test,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission_ridge.csv'\n",
    "y_pred = predict_labels(w, tx_test,'logistic')\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
