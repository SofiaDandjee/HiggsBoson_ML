{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "\n",
    "from cross_validation import *\n",
    "\n",
    "from plots import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21858528e+02  4.92398193e+01  8.11819816e+01  5.78959617e+01\n",
      "  2.40373503e+00  3.71783360e+02 -8.21688171e-01  2.37309984e+00\n",
      "  1.89173324e+01  1.58432217e+02  1.43760943e+00 -1.28304708e-01\n",
      "  4.58289801e-01  3.87074191e+01 -1.09730480e-02 -8.17107200e-03\n",
      "  4.66602072e+01 -1.95074680e-02  4.35429640e-02  4.17172345e+01\n",
      " -1.01191920e-02  2.09797178e+02  9.79176000e-01  8.48221045e+01\n",
      " -3.27458741e-03 -1.23928255e-02  5.76794744e+01 -1.18452642e-02\n",
      " -1.58228913e-03  7.30645914e+01]\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "tx_clean = remove_undefined_values (tX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  3.14910656e-01,  6.83319669e-02, ...,\n",
       "         1.14381874e+00, -2.52714288e+00,  4.12510497e-01],\n",
       "       [ 1.00000000e+00,  7.40827026e-01,  5.52504823e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -2.73819964e-01],\n",
       "       [ 1.00000000e+00, -1.00190288e-12,  3.19515553e+00, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -2.93969845e-01],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -3.10930673e-01,  3.19316447e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -3.17017229e-01],\n",
       "       [ 1.00000000e+00, -5.10097335e-01, -8.45323970e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -7.45439413e-01],\n",
       "       [ 1.00000000e+00, -1.00190288e-12,  6.65336083e-01, ...,\n",
       "         2.27720906e-14, -9.07315481e-15, -7.45439413e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODEL BUILDING\n",
    "\n",
    "tx, mean, std = standardize(tx_clean,0)\n",
    "y, tx = build_model_data(tx,y)\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00, -9.73058263e-13,  4.50019089e-15, -3.48448848e-15,\n",
       "        7.19675786e-15, -7.23439743e-12, -6.30188859e-12,  6.80668756e-13,\n",
       "        2.16429719e-14,  6.39742126e-15,  2.86409207e-15, -7.00447966e-15,\n",
       "        4.45924897e-15,  5.55781326e-13, -5.96492045e-15,  1.35646161e-16,\n",
       "        7.13136217e-17,  2.58030370e-14, -1.06327391e-16, -1.87188487e-16,\n",
       "        8.24369382e-15,  1.41040513e-16, -9.00283004e-15, -6.01698247e-16,\n",
       "        2.89129663e-12, -2.76637638e-15,  2.53942997e-14, -8.40723516e-12,\n",
       "        2.10209062e-14, -5.90180110e-15, -8.76751116e-16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] == -1:\n",
    "        y[i] = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compute_gradient import *\n",
    "from cost import *\n",
    "from implementations import *\n",
    "w = np.zeros((num_features,1))\n",
    "max_iter = 100\n",
    "threshold = 1e-8\n",
    "\n",
    "gamma = 0.001\n",
    "losses = []\n",
    "w.shape,tx.shape\n",
    "y = y.reshape(num_samples,1)\n",
    "gradient = 0\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=1.5164181848147633\n",
      "loss=125102.92019608324\n",
      "Gradient Descent(1/99): loss=0.447014484520141\n",
      "loss=125158.64969908416\n",
      "Gradient Descent(2/99): loss=0.08908384627271973\n",
      "loss=125178.59546827854\n",
      "Gradient Descent(3/99): loss=0.06436608509869615\n",
      "loss=125286.84023782398\n",
      "Gradient Descent(4/99): loss=0.13409170465664452\n",
      "loss=125213.14045440257\n",
      "Gradient Descent(5/99): loss=0.13034417106402357\n",
      "loss=125243.31249051802\n",
      "Gradient Descent(6/99): loss=0.28414573113524527\n",
      "loss=125107.578425168\n",
      "Gradient Descent(7/99): loss=1.4492882919691072\n",
      "loss=125044.79285659581\n",
      "Gradient Descent(8/99): loss=0.8393079326212982\n",
      "loss=125050.1389223676\n",
      "Gradient Descent(9/99): loss=0.05430489293743843\n",
      "loss=125109.1972910757\n",
      "Gradient Descent(10/99): loss=0.03419215114925409\n",
      "loss=125108.54898459098\n",
      "Gradient Descent(11/99): loss=0.5716694037657313\n",
      "loss=125260.27247877914\n",
      "Gradient Descent(12/99): loss=0.5103258634603682\n",
      "loss=125116.12283455356\n",
      "Gradient Descent(13/99): loss=0.048056810526393004\n",
      "loss=125026.69275874214\n",
      "Gradient Descent(14/99): loss=0.3950634624856581\n",
      "loss=125023.82845485583\n",
      "Gradient Descent(15/99): loss=0.0065945777257760685\n",
      "loss=125015.78153133004\n",
      "Gradient Descent(16/99): loss=0.10098340736121113\n",
      "loss=125036.62724043152\n",
      "Gradient Descent(17/99): loss=0.7309343807484877\n",
      "loss=125377.25381254916\n",
      "Gradient Descent(18/99): loss=0.09431198258987851\n",
      "loss=125210.39625344942\n",
      "Gradient Descent(19/99): loss=0.9366772255371023\n",
      "loss=125074.02374777847\n",
      "Gradient Descent(20/99): loss=0.04078962038817026\n",
      "loss=125105.4525582915\n",
      "Gradient Descent(21/99): loss=0.5769994828006012\n",
      "loss=125049.57432731596\n",
      "Gradient Descent(22/99): loss=0.2937364879755172\n",
      "loss=125095.90255728315\n",
      "Gradient Descent(23/99): loss=0.9890687377766634\n",
      "loss=125075.04889876297\n",
      "Gradient Descent(24/99): loss=0.208988606751399\n",
      "loss=125110.19417726075\n",
      "Gradient Descent(25/99): loss=0.6873830022170202\n",
      "loss=125265.67836486216\n",
      "Gradient Descent(26/99): loss=0.044749010838081076\n",
      "loss=125051.25213948572\n",
      "Gradient Descent(27/99): loss=0.4863513296758393\n",
      "loss=125219.08036169643\n",
      "Gradient Descent(28/99): loss=1.0404741386733274\n",
      "loss=125502.45067472922\n",
      "Gradient Descent(29/99): loss=0.027772306740258913\n",
      "loss=125113.23445491903\n",
      "Gradient Descent(30/99): loss=0.4624625665963355\n",
      "loss=125114.12755017447\n",
      "Gradient Descent(31/99): loss=0.07490222417477208\n",
      "loss=125056.58387409891\n",
      "Gradient Descent(32/99): loss=0.16958596695036374\n",
      "loss=125128.47284588382\n",
      "Gradient Descent(33/99): loss=0.6094519239733613\n",
      "loss=125083.08256248283\n",
      "Gradient Descent(34/99): loss=0.8554099586279653\n",
      "loss=125004.41035640684\n",
      "Gradient Descent(35/99): loss=0.675863946110098\n",
      "loss=125162.31935417803\n",
      "Gradient Descent(36/99): loss=0.09762234878975641\n",
      "loss=125047.4294867912\n",
      "Gradient Descent(37/99): loss=0.8459671924086739\n",
      "loss=125154.91171072047\n",
      "Gradient Descent(38/99): loss=0.2824857640435744\n",
      "loss=125135.04049835485\n",
      "Gradient Descent(39/99): loss=0.8581711710380956\n",
      "loss=125252.12587330103\n",
      "Gradient Descent(40/99): loss=0.9704135323890808\n",
      "loss=125268.00989848518\n",
      "Gradient Descent(41/99): loss=0.8783968297882275\n",
      "loss=125080.84002846145\n",
      "Gradient Descent(42/99): loss=0.8800200485056051\n",
      "loss=125255.0254527019\n",
      "Gradient Descent(43/99): loss=0.4536613694324598\n",
      "loss=125002.81040975839\n",
      "Gradient Descent(44/99): loss=1.328926433672933\n",
      "loss=125401.42269931831\n",
      "Gradient Descent(45/99): loss=0.5539142546815239\n",
      "loss=125023.0945806392\n",
      "Gradient Descent(46/99): loss=0.2453766920757094\n",
      "loss=125312.18787959333\n",
      "Gradient Descent(47/99): loss=0.5517964062383063\n",
      "loss=125305.09499477688\n",
      "Gradient Descent(48/99): loss=0.16566165801452837\n",
      "loss=125131.24228537595\n",
      "Gradient Descent(49/99): loss=1.7228840100460932\n",
      "loss=125547.84095711652\n",
      "Gradient Descent(50/99): loss=0.4011140064993164\n",
      "loss=125195.45682900131\n",
      "Gradient Descent(51/99): loss=0.1126630371109729\n",
      "loss=125045.47098397126\n",
      "Gradient Descent(52/99): loss=0.19157359797238824\n",
      "loss=125295.07165052442\n",
      "Gradient Descent(53/99): loss=1.19156793766107\n",
      "loss=125131.1740247923\n",
      "Gradient Descent(54/99): loss=0.267281125404771\n",
      "loss=125070.94513648785\n",
      "Gradient Descent(55/99): loss=0.3713494586557593\n",
      "loss=125484.47866287112\n",
      "Gradient Descent(56/99): loss=0.44413384356757923\n",
      "loss=125260.17418178153\n",
      "Gradient Descent(57/99): loss=0.16943114404451914\n",
      "loss=125139.21532001726\n",
      "Gradient Descent(58/99): loss=0.01849152054909563\n",
      "loss=125215.68850790254\n",
      "Gradient Descent(59/99): loss=0.35664104722272827\n",
      "loss=125399.20166525414\n",
      "Gradient Descent(60/99): loss=0.48550127652077607\n",
      "loss=125110.50087400826\n",
      "Gradient Descent(61/99): loss=0.09597917744691024\n",
      "loss=125103.45692264927\n",
      "Gradient Descent(62/99): loss=0.22919994026789672\n",
      "loss=125022.9777234076\n",
      "Gradient Descent(63/99): loss=0.2986247096039172\n",
      "loss=125094.21831478644\n",
      "Gradient Descent(64/99): loss=0.42220134863437003\n",
      "loss=125566.69598411267\n",
      "Gradient Descent(65/99): loss=0.24790929286030283\n",
      "loss=125055.56120653436\n",
      "Gradient Descent(66/99): loss=0.20720091101379698\n",
      "loss=125107.51675478727\n",
      "Gradient Descent(67/99): loss=0.0031409803282735284\n",
      "loss=125134.00175275476\n",
      "Gradient Descent(68/99): loss=0.7449669864532696\n",
      "loss=125013.6754618878\n",
      "Gradient Descent(69/99): loss=1.5884961965746296\n",
      "loss=125487.93773511492\n",
      "Gradient Descent(70/99): loss=0.05249888559063421\n",
      "loss=125195.95934813308\n",
      "Gradient Descent(71/99): loss=0.7203787903447447\n",
      "loss=125347.02744979285\n",
      "Gradient Descent(72/99): loss=0.318479276654183\n",
      "loss=125262.81413562095\n",
      "Gradient Descent(73/99): loss=0.12522099392934594\n",
      "loss=125244.84629915007\n",
      "Gradient Descent(74/99): loss=0.4363526341741607\n",
      "loss=125105.57053724072\n",
      "Gradient Descent(75/99): loss=1.2879931557421738\n",
      "loss=125295.64805617515\n",
      "Gradient Descent(76/99): loss=0.5023826445090058\n",
      "loss=125112.45962064936\n",
      "Gradient Descent(77/99): loss=0.16392380409093876\n",
      "loss=125103.47017710996\n",
      "Gradient Descent(78/99): loss=0.4462317888997559\n",
      "loss=125182.7041889987\n",
      "Gradient Descent(79/99): loss=0.059432151669769194\n",
      "loss=125179.32423551378\n",
      "Gradient Descent(80/99): loss=1.0510665591086137\n",
      "loss=125154.14967649781\n",
      "Gradient Descent(81/99): loss=1.034791483152759\n",
      "loss=125351.85140383229\n",
      "Gradient Descent(82/99): loss=1.3145165182406577\n",
      "loss=125156.03097181629\n",
      "Gradient Descent(83/99): loss=0.6719369826827616\n",
      "loss=125094.30099673079\n",
      "Gradient Descent(84/99): loss=0.8981911127158898\n",
      "loss=125059.89579032255\n",
      "Gradient Descent(85/99): loss=0.4150926034198633\n",
      "loss=125163.9682360397\n",
      "Gradient Descent(86/99): loss=0.09239412641573004\n",
      "loss=125428.27215298315\n",
      "Gradient Descent(87/99): loss=0.0744912961294479\n",
      "loss=125144.72822447555\n",
      "Gradient Descent(88/99): loss=0.8649074110518792\n",
      "loss=125040.42184781142\n",
      "Gradient Descent(89/99): loss=0.1363121892117245\n",
      "loss=125219.31254877325\n",
      "Gradient Descent(90/99): loss=0.20079872107788074\n",
      "loss=125185.99191502282\n",
      "Gradient Descent(91/99): loss=0.863628392938021\n",
      "loss=125220.67076214067\n",
      "Gradient Descent(92/99): loss=0.4337429997755979\n",
      "loss=125977.98452010102\n",
      "Gradient Descent(93/99): loss=0.25777297224592316\n",
      "loss=125156.50992691917\n",
      "Gradient Descent(94/99): loss=0.6246909161324209\n",
      "loss=125063.51282669103\n",
      "Gradient Descent(95/99): loss=0.47839252111858405\n",
      "loss=125063.09716558589\n",
      "Gradient Descent(96/99): loss=0.2110861706230749\n",
      "loss=124991.73908606492\n",
      "Gradient Descent(97/99): loss=0.416447144364389\n",
      "loss=125223.20252669544\n",
      "Gradient Descent(98/99): loss=0.663892551404959\n",
      "loss=125338.3849741071\n",
      "Gradient Descent(99/99): loss=0.7647110254635833\n",
      "loss=125201.7448973348\n"
     ]
    }
   ],
   "source": [
    "    for iter in range(max_iter):\n",
    "        for batch_y, batch_tx in batch_iter(y, tx, batch_size=1, num_batches = num_samples):\n",
    "            \n",
    "            gradient = logistic_gradient (batch_y,batch_tx,w)\n",
    "            w -= gamma*gradient\n",
    "            #print(\"Gradient={g}\".format(g=gradient))\n",
    "            #print(w)\n",
    "            loss = logistic_loss (batch_y,batch_tx,w)\n",
    "            # log info\n",
    "            #if iter % 100 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "            \n",
    "            losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "                  bi=iter, ti=max_iter - 1, l=loss))\n",
    "            #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "\n",
    "        print(\"loss={l}\".format(l=logistic_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.058844101854333e-06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w\n",
    "loss/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-74c7386bdc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtx_test_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_undefined_values\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_test_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ML_Project1/data_helpers.py\u001b[0m in \u001b[0;36mbuild_model_data\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m\"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "tx_test_clean = remove_undefined_values (tX_test)\n",
    "tx_test, _, _ = standardize(tx_test_clean,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest, tx_test = build_model_data(tx_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission_ridge.csv'\n",
    "y_pred = predict_labels(w, tx_test,'logistic')\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
