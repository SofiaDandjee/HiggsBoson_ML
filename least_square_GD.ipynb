{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=27.02940841655894, weights = [ 0.43739267  0.39970125  0.92648525  0.53762591 -0.54406452 -0.98013542\n",
      " -0.59557737 -0.87902343  1.22890082  0.09841009 -0.61369318  0.20910643\n",
      " -0.18273795 -0.49359806 -0.28009072  0.57379387  0.22940913 -0.34305217\n",
      "  0.20114139  0.65066689  0.08856085  0.69394787 -0.65244013 -0.69438489\n",
      " -0.81558197 -0.42603332 -0.10120993 -0.26692919 -0.74771819 -1.04227653\n",
      " -0.82553497]\n",
      "Gradient Descent(1/99): loss=47.790307707198856, weights = [0.28698133 0.77136642 0.29230917 0.23548399 0.92602496 0.76152672\n",
      " 1.10962241 0.862233   0.20734665 0.69536007 1.01079639 0.1667187\n",
      " 0.84185587 1.24800728 0.36703077 0.44470751 0.21573452 0.03489363\n",
      " 0.11883317 0.52644211 0.90796039 0.56681715 0.87098089 1.07460292\n",
      " 0.61143141 0.93057282 1.25566607 1.48340918 0.99382666 0.69933457\n",
      " 0.85910085]\n",
      "Gradient Descent(2/99): loss=96.52808673348356, weights = [ 0.16665227  0.07003793  0.82688419  0.18458714 -1.1625269  -1.74909104\n",
      " -1.31434257 -1.64844478  1.26026458 -0.38835636 -1.3734024  -0.06061331\n",
      " -0.63244247 -1.26270353 -0.45745843  0.32478172  0.17972756 -0.76160724\n",
      "  0.00653336  0.4315061  -0.3765836   0.43109955 -1.37841668 -1.55474015\n",
      " -1.53846771 -1.12755057 -0.8022653  -1.04044935 -1.51691212 -1.81133359\n",
      " -1.58758092]\n",
      "Gradient Descent(3/99): loss=201.45846510130775, weights = [ 0.07038901  0.96944723 -0.21568666 -0.04414184  1.91145567  1.9106794\n",
      "  2.24717979  2.01088967 -0.56588552  1.02049159  2.09596784  0.12859348\n",
      "  1.50343131  2.39706349  0.82848374  0.279937    0.1765227   0.31045623\n",
      "  0.02011483  0.34741594  1.40276322  0.37260429  1.87536536  2.22137357\n",
      "  1.53287744  1.80208401  2.12754391  2.63821087  2.14281013  1.84841314\n",
      "  1.97258195]\n",
      "Gradient Descent(4/99): loss=424.0153114802794, weights = [-0.00662159 -0.43038787  1.10575014  0.09801407 -2.52802413 -3.40311112\n",
      " -2.90036007 -3.30271094  1.88136509 -1.15725582 -2.93114568 -0.20953248\n",
      " -1.61778351 -2.91683844 -0.97252241  0.18729961  0.12824773 -1.25050895\n",
      " -0.0937312   0.29038947 -1.26628215  0.25378705 -2.86047678 -3.30247322\n",
      " -2.96867397 -2.49982161 -2.17426133 -2.70331187 -3.17109196 -3.4654271\n",
      " -3.19864173]\n",
      "Gradient Descent(5/99): loss=894.6214948440957, weights = [-0.06823007  1.52958479 -0.96462887 -0.23061482  3.94444928  4.32304777\n",
      "  4.6045436   4.42278049 -1.82365846  1.90542891  4.39467369  0.25903804\n",
      "  2.89809759  4.80938015  1.69930129  0.20682948  0.1493461   1.05587946\n",
      "  0.01490363  0.2283625   2.53567551  0.26416785  4.02007589  4.69852102\n",
      "  3.54793884  3.72191522  4.04760744  5.06288091  4.55507516  4.26072522\n",
      "  4.31924557]\n",
      "Gradient Descent(6/99): loss=1889.0402191515823, weights = [-0.11751686 -1.38192798  1.92349245  0.16707562 -5.44982265 -6.90519577\n",
      " -6.28399078 -6.80484805  3.45321527 -2.62715756 -6.23385278 -0.42506502\n",
      " -3.68703594 -6.41904286 -2.14019763  0.09319867  0.07407816 -2.24104024\n",
      " -0.17499959  0.20115145 -3.06138677  0.11991562 -5.981838   -6.95228175\n",
      " -5.94181297 -5.34268535 -5.01696592 -6.22384252 -6.67331556 -6.96758581\n",
      " -6.607458  ]\n",
      "Gradient Descent(7/99): loss=3989.9275427276903, weights = [-0.15694629  2.79500216 -2.37565271 -0.4645886   8.21238332  9.41510241\n",
      "  9.55898822  9.51422758 -4.2971179   3.89667572  9.23193333  0.57512468\n",
      "  5.86321955  9.90143025  3.47635585  0.19413058  0.14351127  2.6120472\n",
      "  0.08537719  0.14546834  5.0093575   0.22684848  8.55446168  9.96437852\n",
      "  7.83864329  7.81735429  8.14320751 10.18114087  9.64707444  9.35273362\n",
      "  9.27290611]\n",
      "Gradient Descent(8/99): loss=8428.20476449986, weights = [-1.88489829e-01 -3.32521137e+00  3.78586011e+00  4.16624959e-01\n",
      " -1.16406917e+01 -1.43055871e+01 -1.34528035e+01 -1.42049936e+01\n",
      "  6.90650962e+00 -5.63580184e+00 -1.32311866e+01 -8.69477231e-01\n",
      " -8.03733430e+00 -1.38195944e+01 -4.65720113e+00 -1.88669213e-03\n",
      "  7.17203346e-03 -4.38064395e+00 -2.98072349e-01  1.48977125e-01\n",
      " -6.77854916e+00 -1.02460885e-02 -1.25744980e+01 -1.46370412e+01\n",
      " -1.21989977e+01 -1.13203019e+01 -1.09945054e+01 -1.36629618e+01\n",
      " -1.40738612e+01 -1.43680641e+01 -1.38105195e+01]\n",
      "Gradient Descent(9/99): loss=17804.24855081779, weights = [-0.21372466  5.52513026 -5.24578136 -0.89018454 17.21639995 20.17114729\n",
      " 20.00770471 20.26931398 -9.42439358  8.17984976 19.43148806  1.23969585\n",
      " 12.1482489  20.6575503   7.18941     0.24437401  0.17420655  5.84100516\n",
      "  0.261011    0.080811   10.30383062  0.26945243 18.13408288 21.1087073\n",
      " 16.9193021  16.488683   16.81467177 20.9928128  20.40312543 20.10874972\n",
      " 19.73742649]\n",
      "Gradient Descent(10/99): loss=37611.436668564405, weights = [ -0.23391253  -7.38025367   7.81381675   0.99038963 -24.72642731\n",
      " -29.93955081 -28.61264076 -29.83813651  14.27539006 -11.931015\n",
      " -28.03057289  -1.81734408 -17.20724824 -29.45381751 -10.00830378\n",
      "  -0.1444586   -0.09574115  -8.9616853   -0.54567605   0.1284193\n",
      " -14.56915301  -0.18864352 -26.50105332 -30.85538956 -25.40617593\n",
      " -23.93460131 -23.60880415 -29.37844936 -29.70803912 -30.00214205\n",
      " -29.02614751]\n",
      "Gradient Descent(11/99): loss=79454.68436596426, weights = [-2.50062824e-01  1.13380681e+01 -1.12280241e+01 -1.75689900e+00\n",
      "  3.62337401e+01  4.28935169e+01  4.20669254e+01  4.29899421e+01\n",
      " -2.01995687e+01  1.72759096e+01  4.09631971e+01  2.63294798e+00\n",
      "  2.54443994e+01  4.33801530e+01  1.50051660e+01  3.95474508e-01\n",
      "  2.73028323e-01  1.26029521e+01  6.35138398e-01  1.71431185e-02\n",
      "  2.15437646e+01  4.36220547e-01  3.83721662e+01  4.46641765e+01\n",
      "  3.61100591e+01  3.48163404e+01  3.51424866e+01  4.38330001e+01\n",
      "  4.31256111e+01  4.28311286e+01  4.18459467e+01]\n",
      "Gradient Descent(12/99): loss=167849.6912672129, weights = [ -0.26298306 -15.90472659  16.3940999    2.22552503 -52.37177667\n",
      " -62.96654322 -60.65101126 -62.86312629  29.88594227 -25.1925021\n",
      " -59.30766652  -3.82997705 -36.56157756 -62.48128752 -21.33619538\n",
      "  -0.41026065  -0.28334476 -18.69487377  -1.07152165   0.14441304\n",
      " -30.97831256  -0.50451197 -55.92008479 -65.10668765 -53.3017091\n",
      " -50.57653166 -50.25083609 -62.57735222 -62.73538758 -63.0293052\n",
      " -61.16700445]\n",
      "Gradient Descent(13/99): loss=354586.5342666473, weights = [-2.73319248e-01  2.36570957e+01 -2.38025187e+01 -3.57105378e+00\n",
      "  7.64087913e+01  9.08954719e+01  8.86557195e+01  9.09884748e+01\n",
      " -4.29283801e+01  3.65204777e+01  8.64384446e+01  5.56734868e+00\n",
      "  5.35487322e+01  9.13826676e+01  3.14964360e+01  7.43104358e-01\n",
      "  5.07767744e-01  2.68365109e+01  1.41914177e+00 -6.88436359e-02\n",
      "  4.53310584e+01  8.36981063e-01  8.11271392e+01  9.44343303e+01\n",
      "  7.66541343e+01  7.35378618e+01  7.38642578e+01  9.20841605e+01\n",
      "  9.11279026e+01  9.08331742e+01  8.85537702e+01]\n",
      "Gradient Descent(14/99): loss=749073.1440427362, weights = [  -0.2815882   -33.87634005   34.57672428    4.84733072 -110.7716438\n",
      " -132.73649876 -128.34411066 -132.6286043    62.89036529  -53.18564875\n",
      " -125.39075111   -8.08891839  -77.43370126 -132.25218708  -45.283208\n",
      "   -0.94848098   -0.65713982  -39.30330765   -2.19077805    0.21787944\n",
      "  -65.60600379   -1.13330864 -118.06686821 -137.45626062 -112.23006165\n",
      " -106.85611554 -106.5307008  -132.71016256 -132.50600769 -132.79954978\n",
      " -129.06218644]\n",
      "Gradient Descent(15/99): loss=1582436.7147778512, weights = [ -0.28820336  49.71569682 -50.31552933  -7.39390603 161.28222854\n",
      " 192.30142717 187.06520539 192.387439   -90.92260568  77.19177783\n",
      " 182.49818591  11.76085537 112.93295877 192.78986815  66.32083042\n",
      "   1.49692588   1.02310615  56.86264118   3.06594554  -0.21798224\n",
      "  95.61448768   1.7141532  171.45005379 199.58127762 162.30529693\n",
      " 155.33913183 155.66600254 194.01687542 192.53465328 192.23939298\n",
      " 187.22869675]\n",
      "Gradient Descent(16/99): loss=3342939.6545918505, weights = [  -0.29349549  -71.80962345   73.03413725   10.39348755 -234.13975962\n",
      " -280.12680853 -271.35770763 -280.00922419  132.62957092 -112.30967385\n",
      " -264.99975719  -17.08994882 -163.76533673 -279.64443033  -95.88350593\n",
      "   -2.06898375   -1.43006372  -82.87803695   -4.5650338     0.39976123\n",
      " -138.73021257   -2.43740467 -249.35161039 -290.29132549 -236.71779677\n",
      " -225.74809976 -225.42332122 -280.86671215 -279.89763986 -280.19039833\n",
      " -272.48911078]\n",
      "Gradient Descent(17/99): loss=7062049.41912759, weights = [-2.97729189e-01  1.04795438e+02 -1.06283039e+02 -1.54637402e+01\n",
      "  3.40583228e+02  4.06524871e+02  3.94948194e+02  4.06596338e+02\n",
      " -1.92298847e+02  1.63119742e+02  3.85421293e+02  2.48423226e+01\n",
      "  2.38394418e+02  4.07016001e+02  1.39878631e+02  3.10360230e+00\n",
      "  2.12612002e+00  1.20258163e+02  6.53511455e+00 -5.11108161e-01\n",
      "  2.01863410e+02  3.58645138e+00  3.62261723e+02  4.21711759e+02\n",
      "  3.43245151e+02  3.28146034e+02  3.28473873e+02  4.09353407e+02\n",
      "  4.06759856e+02  4.06463465e+02  3.95685627e+02]\n",
      "Gradient Descent(18/99): loss=14918768.720796805, weights = [-3.01116152e-01 -1.51916358e+02  1.54314710e+02  2.21148773e+01\n",
      " -4.94754369e+02 -5.91492130e+02 -5.73487116e+02 -5.91353857e+02\n",
      "  2.79965434e+02 -2.37204794e+02 -5.59932336e+02 -3.61061125e+01\n",
      " -3.46133585e+02 -5.91013779e+02 -2.02786029e+02 -4.42369365e+00\n",
      " -3.05061194e+00 -1.74963334e+02 -9.59015272e+00  8.02048579e-01\n",
      " -2.93186656e+02 -5.17706880e+00 -5.26692029e+02 -6.13155256e+02\n",
      " -4.99702384e+02 -4.76911540e+02 -4.76588134e+02 -5.93850448e+02\n",
      " -5.91265678e+02 -5.91556786e+02 -5.75478855e+02]\n",
      "Gradient Descent(19/99): loss=31516299.33213806, weights = [-3.03825721e-01  2.21179397e+02 -2.24480912e+02 -3.25072586e+01\n",
      "  7.19365420e+02  8.59078338e+02  8.34097556e+02  8.59119293e+02\n",
      " -4.06451349e+02  3.44648654e+02  8.14097743e+02  5.24770900e+01\n",
      "  5.03443837e+02  8.59575206e+02  2.95264817e+02  6.50864841e+00\n",
      "  4.46674385e+00  2.54153004e+02  1.38548486e+01 -1.11545730e+00\n",
      "  4.26334540e+02  7.55385022e+00  7.65358639e+02  8.90972249e+02\n",
      "  7.25484286e+02  6.93204007e+02  6.93533869e+02  8.64258585e+02\n",
      "  8.59317116e+02  8.59018330e+02  8.36059945e+02]\n",
      "Gradient Descent(20/99): loss=66579028.67000264, weights = [-3.05993377e-01 -3.21119204e+02  3.26054196e+02  4.68803057e+01\n",
      " -1.04530563e+03 -1.24925904e+03 -1.21175237e+03 -1.24907685e+03\n",
      "  5.91221980e+02 -5.01047032e+02 -1.18298877e+03 -7.62775495e+01\n",
      " -7.31383542e+02 -1.24878914e+03 -4.28625941e+02 -9.38841101e+00\n",
      " -6.46513715e+00 -3.69523613e+02 -2.02142873e+01  1.66418171e+00\n",
      " -6.19465222e+02 -1.09550680e+01 -1.11257898e+03 -1.29521039e+03\n",
      " -1.05526574e+03 -1.00750253e+03 -1.00718204e+03 -1.25503602e+03\n",
      " -1.24903825e+03 -1.24932588e+03 -1.21554943e+03]\n",
      "Gradient Descent(21/99): loss=140649986.41340613, weights = [-3.07727502e-01  4.67066757e+02 -4.74147598e+02 -6.85088590e+01\n",
      "  1.51955712e+03  1.81511040e+03  1.76180386e+03  1.81508710e+03\n",
      " -8.58850295e+02  7.28134077e+02  1.71968535e+03  1.10857763e+02\n",
      "  1.06337496e+03  1.81561944e+03  6.23517859e+02  1.37105349e+01\n",
      "  9.41896695e+00  5.36983842e+02  2.93101254e+01 -2.38200193e+00\n",
      "  9.00548382e+02  1.59427157e+01  1.61691381e+03  1.88230101e+03\n",
      "  1.53297325e+03  1.46439716e+03  1.46473128e+03  1.82525897e+03\n",
      "  1.81535726e+03  1.81505341e+03  1.76636586e+03]\n",
      "Gradient Descent(22/99): loss=297126874.6797767, weights = [-3.09114801e-01 -6.78543220e+02  6.88885633e+02  9.92009581e+01\n",
      " -2.20835582e+03 -2.63880860e+03 -2.56011319e+03 -2.63853344e+03\n",
      "  1.24876244e+03 -1.05842116e+03 -2.49921471e+03 -1.61138660e+02\n",
      " -1.54522800e+03 -2.63835651e+03 -9.05722416e+02 -1.98687611e+01\n",
      " -1.36719653e+01 -7.80561282e+02 -4.26653318e+01  3.49386857e+00\n",
      " -1.30872688e+03 -2.31551924e+01 -2.35027885e+03 -2.73606698e+03\n",
      " -2.22890982e+03 -2.12839196e+03 -2.12807765e+03 -2.65180736e+03\n",
      " -2.63859971e+03 -2.63887998e+03 -2.56771244e+03]\n",
      "Gradient Descent(23/99): loss=627688505.1389538, weights = [-3.10224641e-01  9.86530458e+02 -1.00155013e+03 -1.44560371e+02\n",
      "  3.20998769e+03  3.83475451e+03  3.72160023e+03  3.83459567e+03\n",
      " -1.81455366e+03  1.53825625e+03  3.63276139e+03  2.34191175e+02\n",
      "  2.24625032e+03  3.83528932e+03  1.31695837e+03  2.89316936e+01\n",
      "  1.98861404e+01  1.13444929e+03  6.19531081e+01 -5.05064607e+00\n",
      "  1.90234673e+03  3.36692250e+01  3.41584899e+03  3.97651273e+03\n",
      "  3.23881378e+03  3.09356165e+03  3.09390475e+03  3.85539904e+03\n",
      "  3.83501851e+03  3.83470398e+03  3.73166531e+03]\n",
      "Gradient Descent(24/99): loss=1326008830.5372114, weights = [-3.11112513e-01 -1.43359242e+03  1.45539993e+03  2.09732459e+02\n",
      " -4.66532652e+03 -5.57426870e+03 -5.40856956e+03 -5.57379694e+03\n",
      "  2.63783500e+03 -2.23589064e+03 -5.27977915e+03 -3.40406908e+02\n",
      " -3.26449102e+03 -5.57385417e+03 -1.91360377e+03 -4.20024764e+01\n",
      " -2.88919501e+01 -1.64891033e+03 -9.00999263e+01  7.36490779e+00\n",
      " -2.76480329e+03 -4.89244669e+01 -4.96495114e+03 -5.77991228e+03\n",
      " -4.70826609e+03 -4.49630281e+03 -4.49600155e+03 -5.60252337e+03\n",
      " -5.57408487e+03 -5.57434961e+03 -5.42419001e+03]\n",
      "Gradient Descent(25/99): loss=2801229279.4027104, weights = [-3.11822810e-01  2.08392879e+03 -2.11568074e+03 -3.05218586e+02\n",
      "  6.78107172e+03  8.10130763e+03  7.86171497e+03  8.10086265e+03\n",
      " -3.83350277e+03  3.24965901e+03  7.67418442e+03  4.94739833e+02\n",
      "  4.74511106e+03  8.10189694e+03  2.78186836e+03  6.10925094e+01\n",
      "  4.20022461e+01  2.39659137e+03  1.30906640e+02 -1.06834334e+01\n",
      "  4.01867872e+03  7.11199847e+01  7.21614977e+03  8.40059412e+03\n",
      "  6.84244610e+03  6.53521358e+03  6.53557566e+03  8.14412543e+03\n",
      "  8.10160793e+03  8.10127081e+03  7.88341624e+03]\n",
      "Gradient Descent(26/99): loss=5917672111.721294, weights = [-3.12391048e-01 -3.02863808e+03  3.07470181e+03  4.43235838e+02\n",
      " -9.85573916e+03 -1.17755063e+04 -1.14260178e+04 -1.17746191e+04\n",
      "  5.57228743e+03 -4.72332894e+03 -1.11537975e+04 -7.19112066e+02\n",
      " -6.89647601e+03 -1.17751711e+04 -4.04278148e+03 -8.87553673e+01\n",
      " -6.10412626e+01 -3.48333966e+03 -1.90311948e+02  1.55465660e+01\n",
      " -5.84079886e+03 -1.03360340e+02 -1.04885148e+04 -1.22101144e+04\n",
      " -9.94597444e+03 -9.49857989e+03 -9.49830620e+03 -1.18359893e+04\n",
      " -1.17753754e+04 -1.17756073e+04 -1.14585726e+04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/99): loss=12501241323.07319, weights = [-3.12845639e-01  4.40222716e+03 -4.46929321e+03 -6.44610779e+02\n",
      "  1.43250866e+04  1.71145164e+04  1.66078111e+04  1.71134671e+04\n",
      " -8.09858816e+03  6.86504073e+03  1.62117981e+04  1.04515997e+03\n",
      "  1.00240254e+04  1.71152209e+04  5.87652920e+03  1.29037798e+02\n",
      "  8.87259129e+01  5.06288277e+03  2.76568107e+02 -2.25795356e+01\n",
      "  8.48949089e+03  1.50237641e+02  1.52443890e+04  1.77465877e+04\n",
      "  1.44552146e+04  1.38057940e+04  1.38061962e+04  1.72041760e+04\n",
      "  1.71148934e+04  1.71145086e+04  1.66541045e+04]\n",
      "Gradient Descent(28/99): loss=26409208159.869232, weights = [-3.13209311e-01 -6.39820106e+03  6.49553760e+03  9.36520611e+02\n",
      " -2.08206215e+04 -2.48757868e+04 -2.41380461e+04 -2.48740216e+04\n",
      "  1.17713955e+04 -9.97811191e+03 -2.35628200e+04 -1.51913256e+03\n",
      " -1.45691383e+04 -2.48756191e+04 -8.54072714e+03 -1.87517833e+02\n",
      " -1.28955187e+02 -7.35863769e+03 -4.02016847e+02  3.28332948e+01\n",
      " -1.23389190e+04 -2.18356053e+02 -2.21571897e+04 -2.57940866e+04\n",
      " -2.10107756e+04 -2.00660264e+04 -2.00658109e+04 -2.50043528e+04\n",
      " -2.48757675e+04 -2.48759301e+04 -2.42063653e+04]\n",
      "Gradient Descent(29/99): loss=55790161761.73551, weights = [-3.13500249e-01  9.29970755e+03 -9.44134694e+03 -1.36158327e+03\n",
      "  3.02620244e+04  3.61551627e+04  3.50841671e+04  3.61528370e+04\n",
      " -1.71086976e+04  1.45026290e+04  3.42477355e+04  2.20794182e+03\n",
      "  2.11758772e+04  3.61561105e+04  1.24140831e+04  2.72577931e+02\n",
      "  1.87432905e+02  1.06954790e+04  5.84277937e+02 -4.77080763e+01\n",
      "  1.79342041e+04  3.17377039e+02  3.22042617e+04  3.74902528e+04\n",
      "  3.05373910e+04  2.91650887e+04  2.91655756e+04  3.63437772e+04\n",
      "  3.61557018e+04  3.61552163e+04  3.51824219e+04]\n",
      "Gradient Descent(30/99): loss=117858215610.64096, weights = [-3.13732999e-01 -1.35164815e+04  1.37221598e+04  1.97860061e+03\n",
      " -4.39842263e+04 -5.25504807e+04 -5.09925544e+04 -5.25468607e+04\n",
      "  2.48671755e+04 -2.10789849e+04 -4.97772141e+04 -3.20919269e+03\n",
      " -3.07778405e+04 -5.25506667e+04 -1.80427582e+04 -3.96152659e+02\n",
      " -2.72423500e+02 -1.55453242e+04 -8.49252720e+02  6.93538855e+01\n",
      " -2.60663707e+04 -4.61286606e+02 -4.68075775e+04 -5.44905900e+04\n",
      " -4.43854670e+04 -4.23900426e+04 -4.23899502e+04 -5.28228734e+04\n",
      " -5.25506972e+04 -5.25507134e+04 -5.11364170e+04]\n",
      "Gradient Descent(31/99): loss=248978646921.14215, weights = [-3.13919199e-01  1.96457774e+04 -1.99449303e+04 -2.87620422e+03\n",
      "  6.39292375e+04  7.63790401e+04  7.41159586e+04  7.63740181e+04\n",
      " -3.61427981e+04  3.06372365e+04  7.23491372e+04  4.66435449e+03\n",
      "  4.47344663e+04  7.63805020e+04  2.62248415e+04  5.75813537e+02\n",
      "  3.95955490e+02  2.25944752e+04  1.23431995e+03 -1.00791200e+02\n",
      "  3.78864157e+04  6.70464293e+02  6.80324497e+04  7.91992785e+04\n",
      "  6.45114193e+04  6.16120099e+04  6.16126756e+04  7.67766997e+04\n",
      "  7.63799219e+04  7.63792234e+04  7.43239926e+04]\n",
      "Gradient Descent(32/99): loss=525974080818.6497, weights = [-3.14068159e-01 -2.85540301e+04  2.89886062e+04  4.18002535e+03\n",
      " -9.29179601e+04 -1.11014015e+05 -1.07723432e+05 -1.11006476e+05\n",
      "  5.25323603e+04 -4.45298801e+04 -1.05155826e+05 -6.77948523e+03\n",
      " -6.50191570e+04 -1.11014948e+05 -3.81160539e+04 -8.36896404e+02\n",
      " -5.75502980e+02 -3.28399334e+04 -1.79405350e+03  1.46505943e+02\n",
      " -5.50659804e+04 -9.74483294e+02 -9.88821826e+04 -1.15112722e+05\n",
      " -9.37651318e+04 -8.95501282e+04 -8.95502958e+04 -1.11590245e+05\n",
      " -1.11014729e+05 -1.11014436e+05 -1.08026872e+05]\n",
      "Gradient Descent(33/99): loss=1111134376839.7666, weights = [-3.14187328e-01  4.15021374e+04 -4.21340205e+04 -6.07587888e+03\n",
      "  1.35052133e+05  1.61353063e+05  1.56571660e+05  1.61342345e+05\n",
      " -7.63528494e+04  6.47220258e+04  1.52839372e+05  9.85359723e+03\n",
      "  9.45026211e+04  1.61355610e+05  5.54004402e+04  1.21640957e+03\n",
      "  8.36466048e+02  4.77314108e+04  2.60754845e+03 -2.12929618e+02\n",
      "  8.00359991e+04  1.41637112e+03  1.43720461e+05  1.67310718e+05\n",
      "  1.36282465e+05  1.30157001e+05  1.30158045e+05  1.62192336e+05\n",
      "  1.61354668e+05  1.61353520e+05  1.57011615e+05]\n",
      "Gradient Descent(34/99): loss=2347301223424.723, weights = [-3.14282661e-01 -6.03212468e+04  6.12393954e+04  8.83059707e+03\n",
      " -1.96291784e+05 -2.34519800e+05 -2.27568940e+05 -2.34503983e+05\n",
      "  1.10975804e+05 -9.40705288e+04 -2.22144633e+05 -1.43218191e+04\n",
      " -1.37354856e+05 -2.34522312e+05 -8.05214296e+04 -1.76797698e+03\n",
      " -1.21576569e+03 -6.93752730e+04 -3.78997301e+03  3.09492663e+02\n",
      " -1.16328435e+05 -2.05862467e+03 -2.08891184e+05 -2.43178601e+05\n",
      " -1.98081002e+05 -1.89177078e+05 -1.89177794e+05 -2.35737895e+05\n",
      " -2.34521566e+05 -2.34520620e+05 -2.28209485e+05]\n",
      "Gradient Descent(35/99): loss=4958736898378.201, weights = [-3.14358930e-01  8.76742932e+04 -8.90090593e+04 -1.28352747e+04\n",
      "  2.85301164e+05  3.40862968e+05  3.30761539e+05  3.40840218e+05\n",
      " -1.61297666e+05  1.36727056e+05  3.22877154e+05  2.08160170e+04\n",
      "  1.99639187e+05  3.40867811e+05  1.17034677e+05  2.56968811e+03\n",
      "  1.76705714e+03  1.00833850e+05  5.50852829e+03 -4.49824266e+02\n",
      "  1.69078126e+05  2.99211966e+03  3.03613419e+05  3.53448505e+05\n",
      "  2.87900727e+05  2.74960135e+05  2.74961977e+05  3.42635162e+05\n",
      "  3.40866103e+05  3.40864005e+05  3.31691441e+05]\n",
      "Gradient Descent(36/99): loss=10475464921993.46, weights = [-3.14419943e-01 -1.27430338e+05  1.29370064e+05  1.86550577e+04\n",
      " -4.14671761e+05 -4.95429089e+05 -4.80745803e+05 -4.95395784e+05\n",
      "  2.34439145e+05 -1.98726483e+05 -4.69286631e+05 -3.02551970e+04\n",
      " -2.90165961e+05 -4.95434935e+05 -1.70103924e+05 -3.73490769e+03\n",
      " -2.56833728e+03 -1.46557171e+05 -8.00640864e+03  6.53807131e+02\n",
      " -2.45747017e+05 -4.34890217e+03 -4.41288154e+05 -5.13721210e+05\n",
      " -4.18451081e+05 -3.99641686e+05 -3.99643563e+05 -4.98003142e+05\n",
      " -4.95433078e+05 -4.95430750e+05 -4.82098482e+05]\n",
      "Gradient Descent(37/99): loss=22129701087349.28, weights = [-3.14468757e-01  1.85214220e+05 -1.88033829e+05 -2.71146772e+04\n",
      "  6.02706283e+05  7.20082468e+05  6.98742331e+05  7.20034299e+05\n",
      " -3.40745876e+05  2.88839616e+05  6.82086538e+05  4.39744284e+04\n",
      "  4.21743009e+05  7.20092157e+05  2.47238659e+05  5.42852722e+03\n",
      "  3.73295630e+03  2.13014169e+05  1.16369243e+04 -9.50270036e+02\n",
      "  3.57182011e+05  6.32093102e+03  6.41391589e+05  7.46669582e+05\n",
      "  6.08198377e+05  5.80860645e+05  5.80864173e+05  7.23825482e+05\n",
      "  7.20088833e+05  7.20084727e+05  7.00707269e+05]\n",
      "Gradient Descent(38/99): loss=46749588095822.4, weights = [-3.14507802e-01 -2.69200090e+05  2.73297940e+05  3.94094975e+04\n",
      " -8.76005325e+05 -1.04660697e+06 -1.01558873e+06 -1.04653672e+06\n",
      "  4.95258767e+05 -4.19814999e+05 -9.91380740e+05 -6.39148806e+04\n",
      " -6.12983526e+05 -1.04661986e+06 -3.59349350e+05 -7.89010000e+03\n",
      " -5.42568070e+03 -3.09606017e+05 -1.69137439e+04  1.38118115e+03\n",
      " -5.19147250e+05 -9.18717473e+03 -9.32232995e+05 -1.08524974e+06\n",
      " -8.83988796e+05 -8.44253828e+05 -8.44258156e+05 -1.05204552e+06\n",
      " -1.04661566e+06 -1.04661041e+06 -1.01844581e+06]\n",
      "Gradient Descent(39/99): loss=98759760852731.98, weights = [-3.14539047e-01  3.91269941e+05 -3.97226308e+05 -5.72802955e+04\n",
      "  1.27323313e+06  1.52119385e+06  1.47611163e+06  1.52109198e+06\n",
      " -7.19835045e+05  6.10181482e+05  1.44092601e+06  9.28971999e+04\n",
      "  8.90943302e+05  1.52121378e+06  5.22298070e+05  1.14679032e+04\n",
      "  7.88597144e+03  4.49998094e+05  2.45833239e+04 -2.00747480e+03\n",
      "  7.54556520e+05  1.33531346e+04  1.35495705e+06  1.57735967e+06\n",
      "  1.28483577e+06  1.22708369e+06  1.22709078e+06  1.52910027e+06\n",
      "  1.52120704e+06  1.52119869e+06  1.48026312e+06]\n",
      "Gradient Descent(40/99): loss=208632648135790.3, weights = [-3.14564029e-01 -5.68692494e+05  5.77349452e+05  8.32538123e+04\n",
      " -1.85058487e+06 -2.21098505e+06 -2.14545880e+06 -2.21083676e+06\n",
      "  1.04624722e+06 -8.86870432e+05 -2.09431857e+06 -1.35021862e+05\n",
      " -1.29494430e+06 -2.21101283e+06 -7.59135396e+05 -1.66680540e+04\n",
      " -1.14618951e+04 -6.54051094e+05 -3.57307304e+04  2.91777860e+03\n",
      " -1.09671259e+06 -1.94081554e+04 -1.96936716e+06 -2.29261911e+06\n",
      " -1.86744973e+06 -1.78350892e+06 -1.78351843e+06 -2.22247493e+06\n",
      " -2.21100366e+06 -2.21099225e+06 -2.15149393e+06]\n",
      "Gradient Descent(41/99): loss=440742074427049.94, weights = [-3.14584035e-01  8.26568193e+05 -8.39151033e+05 -1.21005972e+05\n",
      "  2.68973910e+06  3.21356316e+06  3.11832517e+06  3.21334786e+06\n",
      " -1.52067110e+06  1.28902481e+06  3.04399479e+06  1.96247874e+05\n",
      "  1.88214153e+06  3.21360472e+06  1.10336847e+06  2.42262484e+04\n",
      "  1.66593277e+04  9.50632993e+05  5.19329395e+04 -4.24084799e+03\n",
      "  1.59402084e+06  2.82088538e+04  2.86238326e+06  3.33221481e+06\n",
      "  2.71425043e+06  2.59224723e+06  2.59226185e+06  3.23026491e+06\n",
      "  3.21359077e+06  3.21357347e+06  3.12709580e+06]\n",
      "Gradient Descent(42/99): loss=931079473447636.6, weights = [-3.14600011e-01 -1.20137824e+06  1.21966645e+06  1.75876108e+05\n",
      " -3.90941034e+06 -4.67076504e+06 -4.53233968e+06 -4.67045187e+06\n",
      "  2.21022514e+06 -1.87353758e+06 -4.42430438e+06 -2.85237264e+05\n",
      " -2.73560476e+06 -4.67082425e+06 -1.60369416e+06 -3.52117165e+04\n",
      " -2.42135602e+04 -1.38170058e+06 -7.54821211e+04  6.16388224e+03\n",
      " -2.31683489e+06 -4.10002515e+04 -4.16034095e+06 -4.84321933e+06\n",
      " -3.94503737e+06 -3.76771055e+06 -3.76773099e+06 -4.69503850e+06\n",
      " -4.67080460e+06 -4.67078017e+06 -4.54508855e+06]\n",
      "Gradient Descent(43/99): loss=1966930402554570.2, weights = [-3.14612834e-01  1.74614745e+06 -1.77272888e+06 -2.55628172e+05\n",
      "  5.68214604e+06  6.78873881e+06  6.58754537e+06  6.78828387e+06\n",
      " -3.21245879e+06  2.72309956e+06  6.43052064e+06  4.14578955e+05\n",
      "  3.97607441e+06  6.78882607e+06  2.33089529e+06  5.11785965e+04\n",
      "  3.51932788e+04  2.00823766e+06  1.09709736e+05 -8.95890869e+03\n",
      "  3.36741178e+06  5.95919597e+04  6.04686162e+06  7.03939345e+06\n",
      "  5.73392731e+06  5.47619189e+06  5.47622240e+06  6.82402090e+06\n",
      "  6.78879687e+06  6.78876065e+06  6.60607409e+06]\n",
      "Gradient Descent(44/99): loss=4155193320037323.5, weights = [-3.14623031e-01 -2.53794387e+06  2.57657836e+06  3.71543200e+05\n",
      " -8.25873447e+06 -9.86711634e+06 -9.57468967e+06 -9.86645487e+06\n",
      "  4.66915976e+06 -3.95789845e+06 -9.34646200e+06 -6.02571333e+05\n",
      " -5.77903869e+06 -9.86724197e+06 -3.38784723e+06 -7.43857008e+04\n",
      " -5.11517950e+04 -2.91887969e+06 -1.59458003e+05  1.30213635e+04\n",
      " -4.89437602e+06 -8.66141345e+04 -8.78883195e+06 -1.02314308e+07\n",
      " -8.33399707e+06 -7.95938979e+06 -7.95943334e+06 -9.91839547e+06\n",
      " -9.86720016e+06 -9.86714824e+06 -9.60162147e+06]\n",
      "Gradient Descent(45/99): loss=8777957524302303.0, weights = [-3.14631276e-01  3.68878334e+06 -3.74493716e+06 -5.40021183e+05\n",
      "  1.20036861e+07  1.43413934e+07  1.39163663e+07  1.43404322e+07\n",
      " -6.78640573e+06  5.75262101e+06  1.35846475e+07  8.75809234e+05\n",
      "  8.39956388e+06  1.43415772e+07  4.92407869e+06  1.08116157e+05\n",
      "  7.43467483e+04  4.24245592e+06  2.31764771e+05 -1.89259385e+04\n",
      "  7.11374778e+06  1.25889614e+05  1.27741577e+07  1.48709079e+07\n",
      "  1.21130758e+07  1.15686025e+07  1.15686665e+07  1.44159270e+07\n",
      "  1.43415158e+07  1.43414396e+07  1.39555092e+07]\n",
      "Gradient Descent(46/99): loss=1.8543671103553812e+16, weights = [-3.14637745e-01 -5.36147467e+06  5.44309124e+06  7.84895190e+05\n",
      " -1.74467985e+07 -2.08445481e+07 -2.02267895e+07 -2.08431508e+07\n",
      "  9.86372520e+06 -8.36116679e+06 -1.97446522e+07 -1.27294807e+06\n",
      " -1.22083746e+07 -2.08448140e+07 -7.15691853e+06 -1.57141794e+05\n",
      " -1.08059540e+05 -6.16621186e+06 -3.36859310e+05  2.75079771e+04\n",
      " -1.03395009e+07 -1.82974688e+05 -1.85666435e+07 -2.16141724e+07\n",
      " -1.76057924e+07 -1.68144248e+07 -1.68145172e+07 -2.09528774e+07\n",
      " -2.08447254e+07 -2.08446154e+07 -2.02836831e+07]\n",
      "Gradient Descent(47/99): loss=3.917400341078874e+16, weights = [-3.14643105e-01  7.79265385e+06 -7.91128014e+06 -1.14080906e+06\n",
      "  2.53581093e+07  3.02965792e+07  2.93986973e+07  3.02945486e+07\n",
      " -1.43364647e+07  1.21525665e+07  2.86979327e+07  1.85017060e+06\n",
      "  1.77443042e+07  3.02969669e+07  1.04022483e+07  2.28398283e+05\n",
      "  1.57059505e+05  8.96230200e+06  4.89609310e+05 -3.99815568e+04\n",
      "  1.50279832e+07  2.65945191e+05  2.69857515e+07  3.14151926e+07\n",
      "  2.55891985e+07  2.44389835e+07  2.44391185e+07  3.04540326e+07\n",
      "  3.02968375e+07  3.02966769e+07  2.94813883e+07]\n",
      "Gradient Descent(48/99): loss=8.275613467574862e+16, weights = [-3.14647127e-01 -1.13262593e+07  1.14986767e+07  1.65811238e+06\n",
      " -3.68568307e+07 -4.40346675e+07 -4.27296366e+07 -4.40317159e+07\n",
      "  2.08373842e+07 -1.76631894e+07 -4.17111084e+07 -2.68913694e+06\n",
      " -2.57905195e+07 -4.40352299e+07 -1.51191829e+07 -3.31966265e+05\n",
      " -2.28278679e+05 -1.30262883e+07 -7.11624338e+05  5.81113375e+04\n",
      " -2.18424735e+07 -3.86538945e+05 -3.92225332e+07 -4.56605197e+07\n",
      " -3.71927090e+07 -3.55209242e+07 -3.55211197e+07 -4.42635169e+07\n",
      " -4.40350424e+07 -4.40348097e+07 -4.28498254e+07]\n",
      "Gradient Descent(49/99): loss=1.7482455787463562e+17, weights = [-3.14650733e-01  1.64621906e+07 -1.67127916e+07 -2.40998958e+06\n",
      "  5.35696870e+07  6.40023378e+07  6.21055375e+07  6.39980479e+07\n",
      " -3.02861670e+07  2.56726232e+07  6.06251537e+07  3.90853515e+06\n",
      "  3.74853193e+07  6.40031563e+07  2.19750287e+07  4.82497508e+05\n",
      "  3.31792425e+05  1.89331037e+07  1.03431283e+06 -8.44621160e+04\n",
      "  3.17470182e+07  5.61816359e+05  5.70081251e+07  6.63654386e+07\n",
      "  5.40578697e+07  5.16280091e+07  5.16282940e+07  6.43349615e+07\n",
      "  6.40028833e+07  6.40025442e+07  6.22802251e+07]\n",
      "Gradient Descent(50/99): loss=3.69321575443496e+17, weights = [-3.14653050e-01 -2.39270272e+07  2.42912637e+07  3.50280715e+06\n",
      " -7.78610451e+07 -9.30244179e+07 -9.02675056e+07 -9.30181826e+07\n",
      "  4.40195338e+07 -3.73139622e+07 -8.81158372e+07 -5.68087398e+06\n",
      " -5.44831655e+07 -9.30256064e+07 -3.19396803e+07 -7.01287655e+05\n",
      " -4.82244841e+05 -2.75183842e+07 -1.50332553e+06  1.22761759e+05\n",
      " -4.61428123e+07 -8.16573909e+05 -8.28586550e+07 -9.64590745e+07\n",
      " -7.85705967e+07 -7.50389067e+07 -7.50393200e+07 -9.35078692e+07\n",
      " -9.30252101e+07 -9.30247181e+07 -9.05214071e+07]\n",
      "Gradient Descent(51/99): loss=7.802017505222399e+17, weights = [-3.14655729e-01  3.47768198e+07 -3.53062210e+07 -5.09116753e+06\n",
      "  1.13167403e+08  1.35206658e+08  1.31199616e+08  1.35197596e+08\n",
      " -6.39803417e+07  5.42341063e+07  1.28072266e+08  8.25688559e+06\n",
      "  7.91887440e+07  1.35208387e+08  4.64228384e+07  1.01928895e+06\n",
      "  7.00920414e+05  3.99966900e+07  2.18501364e+06 -1.78428492e+05\n",
      "  6.70664289e+07  1.18685215e+06  1.20431196e+08  1.40198772e+08\n",
      "  1.14198702e+08  1.09065556e+08  1.09066158e+08  1.35909334e+08\n",
      "  1.35207810e+08  1.35207095e+08  1.31568649e+08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(52/99): loss=1.6481971593102646e+18, weights = [-3.14656669e-01 -5.05464879e+07  5.13159474e+07  7.39977443e+06\n",
      " -1.64483550e+08 -1.96516582e+08 -1.90692531e+08 -1.96503410e+08\n",
      "  9.29924476e+07 -7.88267477e+07 -1.86147074e+08 -1.20009988e+07\n",
      " -1.15097151e+08 -1.96519093e+08 -6.74734331e+07 -1.48148901e+06\n",
      " -1.01875518e+06 -5.81333260e+07 -3.17581557e+06  2.59337510e+05\n",
      " -9.74779307e+07 -1.72503432e+06 -1.75041135e+08 -2.03772386e+08\n",
      " -1.65982496e+08 -1.58521707e+08 -1.58522580e+08 -1.97537886e+08\n",
      " -1.96518256e+08 -1.96517216e+08 -1.91228904e+08]\n",
      "Gradient Descent(53/99): loss=3.4818607804200504e+18, weights = [-3.14659161e-01  7.34669661e+07 -7.45853403e+07 -1.07552280e+07\n",
      "  2.39069179e+08  2.85627699e+08  2.77162714e+08  2.85608554e+08\n",
      " -1.35160191e+08  1.14571007e+08  2.70556102e+08  1.74428927e+07\n",
      "  1.67288349e+08  2.85631350e+08  9.80694931e+07  2.15327528e+06\n",
      "  1.48071321e+06  8.44940822e+07  4.61590002e+06 -3.76934987e+05\n",
      "  1.41679633e+08  2.50725705e+06  2.54414137e+08  2.96173673e+08\n",
      "  2.41247827e+08  2.30403918e+08  2.30405188e+08  2.87112119e+08\n",
      "  2.85630132e+08  2.85628620e+08  2.77942306e+08]\n",
      "Gradient Descent(54/99): loss=7.355524444236112e+18, weights = [-3.14658629e-01 -1.06780814e+08  1.08406318e+08  1.56322225e+07\n",
      " -3.47475918e+08 -4.15146559e+08 -4.02843095e+08 -4.15118732e+08\n",
      "  1.96449044e+08 -1.66523622e+08 -3.93240694e+08 -2.53524324e+07\n",
      " -2.43145824e+08 -4.15151864e+08 -1.42539440e+08 -3.12968534e+06\n",
      " -2.15214769e+06 -1.22808213e+08 -6.70899571e+06  5.47857455e+05\n",
      " -2.05924748e+08 -3.64418135e+06 -3.69779100e+08 -4.30474640e+08\n",
      " -3.50642482e+08 -3.34881364e+08 -3.34883210e+08 -4.17304093e+08\n",
      " -4.15150095e+08 -4.15147898e+08 -4.03976198e+08]\n",
      "Gradient Descent(55/99): loss=1.553874300603946e+19, weights = [-3.14661854e-01  1.55200940e+08 -1.57563535e+08 -2.27207078e+07\n",
      "  5.05040064e+08  6.03396188e+08  5.85513679e+08  6.03355743e+08\n",
      " -2.85529535e+08  2.42034330e+08  5.71557035e+08  3.68485796e+07\n",
      "  3.53401132e+08  6.03403901e+08  2.07174439e+08  4.54885189e+06\n",
      "  3.12804643e+06  1.78496019e+08  9.75121280e+06 -7.96285290e+05\n",
      "  2.99302031e+08  5.29664788e+06  5.37456700e+08  6.25674841e+08\n",
      "  5.09642517e+08  4.86734466e+08  4.86737150e+08  6.06532065e+08\n",
      "  6.03401328e+08  6.03398134e+08  5.87160589e+08]\n",
      "Gradient Descent(56/99): loss=3.2826012072728056e+19, weights = [-3.14659123e-01 -2.25577338e+08  2.29011259e+08  3.30234902e+07\n",
      " -7.34052212e+08 -8.77008258e+08 -8.51016862e+08 -8.76949474e+08\n",
      "  4.15003882e+08 -3.51785626e+08 -8.30731531e+08 -5.35576945e+07\n",
      " -5.13652086e+08 -8.77019467e+08 -3.01118398e+08 -6.61154437e+06\n",
      " -4.54646981e+06 -2.59435650e+08 -1.41729337e+07  1.15736359e+06\n",
      " -4.35021562e+08 -7.69843102e+06 -7.81168283e+08 -9.09389243e+08\n",
      " -7.40741664e+08 -7.07445879e+08 -7.07449778e+08 -8.81566108e+08\n",
      " -8.77015729e+08 -8.77011088e+08 -8.53410571e+08]\n",
      "Gradient Descent(57/99): loss=6.93458324254463e+19, weights = [-3.14664653e-01  3.27866155e+08 -3.32857200e+08 -4.79981055e+07\n",
      "  1.06691070e+09  1.27469066e+09  1.23691338e+09  1.27460522e+09\n",
      " -6.03188815e+08  5.11304024e+08  1.20742959e+09  7.78436144e+07\n",
      "  7.46569385e+08  1.27470695e+09  4.37661568e+08  9.60957183e+06\n",
      "  6.60808213e+06  3.77077635e+08  2.05996990e+07 -1.68217407e+06\n",
      "  6.32283581e+08  1.11893110e+07  1.13539172e+09  1.32175491e+09\n",
      "  1.07663351e+09  1.02823964e+09  1.02824531e+09  1.28131528e+09\n",
      "  1.27470152e+09  1.27469477e+09  1.24039252e+09]\n",
      "Gradient Descent(58/99): loss=1.4649493408226877e+20, weights = [-3.14657843e-01 -4.76538187e+08  4.83792438e+08  6.97630105e+07\n",
      " -1.55070502e+09 -1.85270351e+09 -1.79779599e+09 -1.85257932e+09\n",
      "  8.76706851e+08 -7.43156586e+08 -1.75494268e+09 -1.13142068e+08\n",
      " -1.08510383e+09 -1.85272719e+09 -6.36120705e+08 -1.39670651e+07\n",
      " -9.60453963e+06 -5.48064780e+08 -2.99407032e+07  2.44496168e+06\n",
      " -9.18994737e+08 -1.62631424e+07 -1.65023899e+09 -1.92110921e+09\n",
      " -1.56483667e+09 -1.49449843e+09 -1.49450666e+09 -1.86233209e+09\n",
      " -1.85271929e+09 -1.85270948e+09 -1.80285276e+09]\n",
      "Gradient Descent(59/99): loss=3.0947448406277344e+20, weights = [-3.14668659e-01  6.92626061e+08 -7.03169778e+08 -1.01397287e+08\n",
      "  2.25387753e+09  2.69281826e+09  2.61301274e+09  2.69263777e+09\n",
      " -1.27425258e+09  1.08014349e+09  2.55072745e+09  1.64446727e+08\n",
      "  1.57714788e+09  2.69285268e+09  9.24571820e+08  2.03004787e+07\n",
      "  1.39597510e+07  7.96586634e+08  4.35174176e+07 -3.55363792e+06\n",
      "  1.33571605e+09  2.36377200e+07  2.39854551e+09  2.79224276e+09\n",
      "  2.27441722e+09  2.17218386e+09  2.17219583e+09  2.70681296e+09\n",
      "  2.69284120e+09  2.69282695e+09  2.62036252e+09]\n",
      "Gradient Descent(60/99): loss=6.53773162095379e+20, weights = [-3.14653711e-01 -1.00669972e+09  1.02202452e+09  1.47376234e+08\n",
      " -3.27590602e+09 -3.91388594e+09 -3.79789232e+09 -3.91362359e+09\n",
      "  1.85206678e+09 -1.56993825e+09 -3.70736356e+09 -2.39015659e+08\n",
      " -2.29231100e+09 -3.91393596e+09 -1.34382208e+09 -2.95058003e+07\n",
      " -2.02898480e+07 -1.15780157e+09 -6.32505398e+07  5.16504723e+06\n",
      " -1.94140107e+09 -3.43563252e+07 -3.48617419e+09 -4.05839482e+09\n",
      " -3.30575950e+09 -3.15716807e+09 -3.15718548e+09 -3.93422657e+09\n",
      " -3.91391928e+09 -3.91389856e+09 -3.80857489e+09]\n",
      "Gradient Descent(61/99): loss=1.38111336955811e+21, weights = [-3.14676370e-01  1.46319116e+09 -1.48546505e+09 -2.14204493e+08\n",
      "  4.76137683e+09  5.68865093e+09  5.52005961e+09  5.68826963e+09\n",
      " -2.69189280e+09  2.28183214e+09  5.38848027e+09  3.47398129e+08\n",
      "  3.33176728e+09  5.68872364e+09  1.95318281e+09  4.28853066e+07\n",
      "  2.94903491e+07  1.68281067e+09  9.19317139e+07 -7.50715559e+06\n",
      "  2.82173604e+09  4.99353186e+07  5.06699183e+09  5.89868786e+09\n",
      "  4.80476747e+09  4.58879676e+09  4.58882206e+09  5.71821510e+09\n",
      "  5.68869939e+09  5.68866928e+09  5.53558623e+09]\n",
      "Gradient Descent(62/99): loss=2.917639098947673e+21, weights = [-3.14644108e-01 -2.12668021e+09  2.15905427e+09  3.11336255e+08\n",
      " -6.92043947e+09 -8.26818919e+09 -8.02314957e+09 -8.26763499e+09\n",
      "  3.91254082e+09 -3.31653674e+09 -7.83190512e+09 -5.04927001e+08\n",
      " -4.84256857e+09 -8.26829487e+09 -2.83886025e+09 -6.23317958e+07\n",
      " -4.28628490e+07 -2.44588693e+09 -1.33618465e+08  1.09113010e+07\n",
      " -4.10126193e+09 -7.25786599e+07 -7.36463664e+09 -8.57346808e+09\n",
      " -6.98350575e+09 -6.66960238e+09 -6.66963915e+09 -8.31115933e+09\n",
      " -8.26825962e+09 -8.26821586e+09 -8.04571678e+09]\n",
      "Gradient Descent(63/99): loss=6.163590983433825e+21, weights = [-3.14691336e-01  3.09103064e+09 -3.13808482e+09 -4.52512747e+08\n",
      "  1.00585365e+10  1.20174279e+10  1.16612742e+10  1.20166224e+10\n",
      " -5.68669587e+09  4.82043169e+09  1.13833093e+10  7.33887881e+08\n",
      "  7.03844789e+09  1.20175815e+10  4.12615116e+09  9.05963623e+07\n",
      "  6.22991548e+07  3.55498274e+09  1.94208216e+08 -1.58590679e+07\n",
      "  5.96099320e+09  1.05489702e+08  1.07041564e+10  1.24611366e+10\n",
      "  1.01502004e+10  9.69395644e+09  9.69400988e+09  1.20798830e+10\n",
      "  1.20175303e+10  1.20174667e+10  1.16940746e+10]\n",
      "Gradient Descent(64/99): loss=1.3020751546950695e+22, weights = [-3.14623533e-01 -4.49266908e+09  4.56106014e+09  6.57706202e+08\n",
      " -1.46196143e+10 -1.74667718e+10 -1.69491190e+10 -1.74656010e+10\n",
      "  8.26534761e+09 -7.00627297e+09 -1.65451100e+10 -1.06667186e+09\n",
      " -1.02300562e+10 -1.74669950e+10 -5.99716854e+09 -1.31677593e+08\n",
      " -9.05489201e+07 -5.16700185e+09 -2.82272597e+08  2.30504166e+07\n",
      " -8.66402600e+09 -1.53324369e+08 -1.55579928e+10 -1.81116817e+10\n",
      " -1.47528435e+10 -1.40897142e+10 -1.40897919e+10 -1.75575474e+10\n",
      " -1.74669206e+10 -1.74668281e+10 -1.69967929e+10]\n",
      "Gradient Descent(65/99): loss=2.7506687465650683e+22, weights = [-3.14723827e-01  6.52988527e+09 -6.62928850e+09 -9.55945335e+08\n",
      "  2.12489285e+10  2.53871393e+10  2.46347551e+10  2.53854377e+10\n",
      " -1.20132978e+10  1.01832915e+10  2.40475468e+10  1.55035786e+09\n",
      "  1.48689103e+10  2.53874638e+10  8.71660518e+09  1.91387249e+08\n",
      "  1.31608638e+08  7.50999655e+09  4.10270073e+08 -3.35027070e+07\n",
      "  1.25927583e+10  2.22849830e+08  2.26128179e+10  2.63244858e+10\n",
      "  2.14425710e+10  2.04787434e+10  2.04788563e+10  2.55190774e+10\n",
      "  2.53873556e+10  2.53872212e+10  2.47040468e+10]\n",
      "Gradient Descent(66/99): loss=5.810861628107651e+22, weights = [-3.14580345e-01 -9.49088413e+09  9.63536209e+09  1.38942202e+09\n",
      " -3.08843280e+10 -3.68990247e+10 -3.58054693e+10 -3.68965514e+10\n",
      "  1.74607689e+10 -1.48009399e+10 -3.49519894e+10 -2.25337295e+09\n",
      " -2.16112686e+10 -3.68994963e+10 -1.26691797e+10 -2.78172454e+08\n",
      " -1.91287027e+08 -1.09154302e+10 -5.96308444e+08  4.86946243e+07\n",
      " -1.83029877e+10 -3.23901849e+08 -3.28666777e+10 -3.82614141e+10\n",
      " -3.11657783e+10 -2.97648998e+10 -2.97650639e+10 -3.70907906e+10\n",
      " -3.68993390e+10 -3.68991437e+10 -3.59061816e+10]\n",
      "Gradient Descent(67/99): loss=1.2275601307201996e+23, weights = [-3.14791220e-01  1.37945581e+10 -1.40045501e+10 -2.01946020e+09\n",
      "  4.48889324e+10  5.36310139e+10  5.20415821e+10  5.36274191e+10\n",
      " -2.53784144e+10  2.15124768e+10  5.08010887e+10  3.27517264e+09\n",
      "  3.14109724e+10  5.36316994e+10  1.84140626e+10  4.04310706e+08\n",
      "  2.78026784e+08  1.58650695e+10  8.66706552e+08 -7.07753686e+07\n",
      "  2.66025402e+10  4.70776253e+08  4.77701854e+10  5.56111835e+10\n",
      "  4.52980073e+10  4.32618957e+10  4.32621342e+10  5.39097367e+10\n",
      "  5.36314708e+10  5.36311869e+10  5.21879627e+10]\n",
      "Gradient Descent(68/99): loss=2.593253756456315e+23, weights = [-3.14486408e-01 -2.00497477e+10  2.03549613e+10  2.93519135e+09\n",
      " -6.52439727e+10 -7.79501810e+10 -7.56400158e+10 -7.79449561e+10\n",
      "  3.68863434e+10 -3.12673832e+10 -7.38370166e+10 -4.76031090e+09\n",
      " -4.56543855e+10 -7.79511773e+10 -2.67639824e+10 -5.87646782e+08\n",
      " -4.04098983e+08 -2.30591396e+10 -1.25971761e+09  1.02868702e+08\n",
      " -3.86655533e+10 -6.84251358e+08 -6.94317397e+10 -8.08282653e+10\n",
      " -6.58385440e+10 -6.28791506e+10 -6.28794972e+10 -7.83552916e+10\n",
      " -7.79508451e+10 -7.79504325e+10 -7.58527732e+10]\n",
      "Gradient Descent(69/99): loss=5.47831823230462e+23, weights = [-3.14935158e-01  2.91413745e+10 -2.95849883e+10 -4.26616393e+09\n",
      "  9.48290757e+10  1.13296958e+11  1.09939240e+11  1.13289364e+11\n",
      " -5.36125822e+10  4.54456852e+10  1.07318665e+11  6.91889020e+09\n",
      "  6.63565231e+10  1.13298406e+11  3.89002020e+10  8.54117230e+08\n",
      "  5.87339052e+08  3.35153855e+10  1.83094088e+09 -1.49514868e+08\n",
      "  5.61985809e+10  9.94527482e+08  1.00915800e+11  1.17480119e+11\n",
      "  9.56932574e+10  9.13919169e+10  9.13924207e+10  1.13885767e+11\n",
      "  1.13297923e+11  1.13297323e+11  1.10248473e+11]\n",
      "Gradient Descent(70/99): loss=1.1573094449273195e+24, weights = [-3.14288858e-01 -4.23556307e+10  4.30004026e+10  6.20067059e+09\n",
      " -1.37829645e+11 -1.64671852e+11 -1.59791566e+11 -1.64660814e+11\n",
      "  7.79233914e+10 -6.60531870e+10 -1.55982681e+11 -1.00562847e+10\n",
      " -9.64461160e+10 -1.64673956e+11 -5.65396320e+10 -1.24141962e+09\n",
      " -8.53669958e+08 -4.87130520e+10 -2.66118729e+09  2.17312897e+08\n",
      " -8.16820201e+10 -1.44549938e+09 -1.46676415e+11 -1.70751882e+11\n",
      " -1.39085693e+11 -1.32833895e+11 -1.32834627e+11 -1.65527659e+11\n",
      " -1.64673254e+11 -1.64672383e+11 -1.60241021e+11]\n",
      "Gradient Descent(71/99): loss=2.444847295325026e+24, weights = [-3.15231108e-01  6.15619366e+10 -6.24990825e+10 -9.01238593e+09\n",
      "  2.00328970e+11  2.39342867e+11  2.32249599e+11  2.39326824e+11\n",
      " -1.13258020e+11  9.60052310e+10  2.26713563e+11  1.46163414e+10\n",
      "  1.40179938e+11  2.39345926e+11  8.21777219e+10  1.80434559e+09\n",
      "  1.24076953e+09  7.08021523e+10  3.86791179e+09 -3.15854175e+08\n",
      "  1.18721012e+11  2.10096602e+09  2.13187339e+11  2.48179908e+11\n",
      "  2.02154577e+11  1.93067880e+11  1.93068944e+11  2.40586743e+11\n",
      "  2.39344906e+11  2.39343639e+11  2.32902862e+11]\n",
      "Gradient Descent(72/99): loss=5.164805595994671e+24, weights = [-3.13865808e-01 -8.94774078e+10  9.08395057e+10  1.30990832e+10\n",
      " -2.91168828e+11 -3.47873711e+11 -3.37563975e+11 -3.47850393e+11\n",
      "  1.64615258e+11 -1.39539132e+11 -3.29517605e+11 -2.12441716e+10\n",
      " -2.03745011e+11 -3.47878157e+11 -1.19441492e+11 -2.62253229e+09\n",
      " -1.80340072e+09 -1.02907631e+11 -5.62182965e+09  4.59079333e+08\n",
      " -1.72555462e+11 -3.05365627e+09 -3.09857868e+11 -3.60717938e+11\n",
      " -2.93822263e+11 -2.80615172e+11 -2.80616719e+11 -3.49681626e+11\n",
      " -3.47876674e+11 -3.47874833e+11 -3.38513463e+11]\n",
      "Gradient Descent(73/99): loss=1.0910790582064322e+25, weights = [-3.15836608e-01  1.30051245e+11 -1.32030991e+11 -1.90389074e+10\n",
      "  4.23200332e+11  5.05618237e+11  4.90633517e+11  5.05584347e+11\n",
      " -2.39260611e+11  2.02813630e+11  4.78938492e+11  3.08774140e+10\n",
      "  2.96133885e+11  5.05624700e+11  1.73602645e+11  3.81172855e+09\n",
      "  2.62115896e+09  1.49571449e+11  8.17106758e+09 -6.67250430e+08\n",
      "  2.50801328e+11  4.43834717e+09  4.50363981e+11  5.24286724e+11\n",
      "  4.27056976e+11  4.07861083e+11  4.07863332e+11  5.08245958e+11\n",
      "  5.05622545e+11  5.05619869e+11  4.92013553e+11]\n",
      "Gradient Descent(74/99): loss=2.3049338239949205e+25, weights = [-3.12945808e-01 -1.89023428e+11  1.91900896e+11  2.76721652e+10\n",
      " -6.15101973e+11 -7.34892560e+11 -7.13112967e+11 -7.34843302e+11\n",
      "  3.47754155e+11 -2.94780165e+11 -6.96114793e+11 -4.48788832e+10\n",
      " -4.30416810e+11 -7.34901953e+11 -2.52323360e+11 -5.54016992e+09\n",
      " -3.80973247e+09 -2.17395135e+11 -1.18762662e+10  9.69817425e+08\n",
      " -3.64528049e+11 -6.45093091e+09 -6.54583072e+11 -7.62026336e+11\n",
      " -6.20707425e+11 -5.92807090e+11 -5.92810357e+11 -7.38711829e+11\n",
      " -7.34898821e+11 -7.34894931e+11 -7.15118784e+11]\n",
      "Gradient Descent(75/99): loss=4.869234628817057e+25, weights = [-3.17136608e-01  2.74736749e+11 -2.78919016e+11 -4.02202034e+10\n",
      "  8.94022071e+11  1.06813211e+12  1.03647649e+12  1.06806052e+12\n",
      " -5.05444468e+11  4.28449242e+11  1.01177043e+12  6.52293666e+10\n",
      "  6.25590788e+11  1.06814576e+12  3.66740252e+11  8.05237896e+09\n",
      "  5.53726872e+09  3.15973705e+11  1.72615998e+10 -1.40958446e+09\n",
      "  5.29824543e+11  9.37612765e+09  9.51406009e+11  1.10756979e+12\n",
      "  9.02169335e+11  8.61617496e+11  8.61622246e+11  1.07368324e+12\n",
      "  1.06814121e+12  1.06813556e+12  1.03939185e+12]\n",
      "Gradient Descent(76/99): loss=1.028638897292847e+26, weights = [-3.11040608e-01 -3.99317070e+11  4.05395799e+11  5.84581925e+10\n",
      " -1.29941944e+12 -1.55248028e+12 -1.50647030e+12 -1.55237622e+12\n",
      "  7.34639995e+11 -6.22731019e+11 -1.47056120e+12 -9.48078465e+10\n",
      " -9.09267075e+11 -1.55250012e+12 -5.33039877e+11 -1.17037578e+10\n",
      " -8.04816221e+09 -4.59253064e+11 -2.50889314e+10  2.04876537e+09\n",
      " -7.70075298e+11 -1.36277649e+10 -1.38282433e+12 -1.60980111e+12\n",
      " -1.31126111e+12 -1.25232090e+12 -1.25232781e+12 -1.56054859e+12\n",
      " -1.55249350e+12 -1.55248529e+12 -1.51070764e+12]\n",
      "Gradient Descent(77/99): loss=2.1730273065130765e+26, weights = [-3.19894208e-01  5.80388764e+11 -5.89223914e+11 -8.49662603e+10\n",
      "  1.88864564e+12  2.25645778e+12  2.18958442e+12  2.25630653e+12\n",
      " -1.06776502e+12  9.05110534e+11  2.13739222e+12  1.37798789e+11\n",
      "  1.32157735e+12  2.25648662e+12  7.74748636e+11  1.70108669e+10\n",
      "  1.16976289e+10  6.67502940e+11  3.64655933e+10 -2.97778504e+09\n",
      "  1.11926858e+12  1.98073216e+10  2.00987076e+12  2.33977094e+12\n",
      "  1.90585696e+12  1.82019011e+12  1.82020014e+12  2.26818469e+12\n",
      "  2.25647700e+12  2.25646506e+12  2.19574320e+12]\n",
      "Gradient Descent(78/99): loss=4.5905785667631645e+26, weights = [-3.06951808e-01 -8.43568039e+11  8.56409518e+11  1.23494502e+11\n",
      " -2.74505848e+12 -3.27965630e+12 -3.18245899e+12 -3.27943647e+12\n",
      "  1.55194673e+12 -1.31553601e+12 -3.10660005e+12 -2.00284123e+11\n",
      " -1.92085112e+12 -3.27969821e+12 -1.12606106e+12 -2.47245028e+10\n",
      " -1.70019589e+10 -9.70184436e+11 -5.30010416e+10  4.32807188e+09\n",
      " -1.62680476e+12 -2.87890194e+10 -2.92125354e+12 -3.40074809e+12\n",
      " -2.77007433e+12 -2.64556155e+12 -2.64557614e+12 -3.29670081e+12\n",
      " -3.27968423e+12 -3.27966688e+12 -3.19141049e+12]\n",
      "Gradient Descent(79/99): loss=9.69772055531161e+26, weights = [-3.25701408e-01  1.22608686e+12 -1.24475135e+12 -1.79493508e+11\n",
      "  3.98981466e+12  4.76682769e+12  4.62555592e+12  4.76650817e+12\n",
      " -2.25568229e+12  1.91207032e+12  4.51529849e+12  2.91103645e+11\n",
      "  2.79186765e+12  4.76688861e+12  1.63667731e+12  3.59359133e+10\n",
      "  2.47115554e+10  1.41011789e+12  7.70345456e+10 -6.29065091e+09\n",
      "  2.36448495e+12  4.18434989e+10  4.24590597e+12  4.94282897e+12\n",
      "  4.02617403e+12  3.84520051e+12  3.84522171e+12  4.79160110e+12\n",
      "  4.76686829e+12  4.76684306e+12  4.63856651e+12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/99): loss=2.0486695217423394e+27, weights = [-2.98469408e-01 -1.78206015e+12  1.80918812e+12  2.60885454e+11\n",
      " -5.79900980e+12 -6.92836204e+12 -6.72303011e+12 -6.92789764e+12\n",
      "  3.27852915e+12 -2.77910516e+12 -6.56277607e+12 -4.23105590e+11\n",
      " -4.05784961e+12 -6.92845059e+12 -2.37883424e+12 -5.22311764e+10\n",
      " -3.59170949e+10 -2.04954069e+12 -1.11966124e+11  9.14316812e+09\n",
      " -3.43666875e+12 -6.08175770e+10 -6.17122659e+12 -7.18417170e+12\n",
      " -5.85185644e+12 -5.58881986e+12 -5.58885067e+12 -6.96436904e+12\n",
      " -6.92842106e+12 -6.92838439e+12 -6.74194040e+12]\n",
      "Gradient Descent(81/99): loss=4.327869405369918e+27, weights = [-3.38026208e-01  2.59014143e+12 -2.62957067e+12 -3.79184857e+11\n",
      "  8.42859067e+12  1.00700516e+13  9.77161116e+12  1.00693766e+13\n",
      " -4.76518944e+12  4.03929992e+12  9.53868937e+12  6.14964271e+11\n",
      "  5.89789542e+12  1.00701803e+13  3.45752479e+12  7.59155824e+10\n",
      "  5.22038247e+10  2.97891194e+12  1.62737547e+11 -1.32891690e+10\n",
      "  4.99503798e+12  8.83955157e+10  8.96959043e+12  1.04418590e+13\n",
      "  8.50540080e+12  8.12308938e+12  8.12313416e+12  1.01223861e+13\n",
      "  1.00701373e+13  1.00700840e+13  9.79909638e+12]\n",
      "Gradient Descent(82/99): loss=9.142740393778729e+27, weights = [-2.80771808e-01 -3.76464994e+12  3.82195850e+12  5.51127530e+11\n",
      " -1.22505640e+13 -1.46363509e+13 -1.42025817e+13 -1.46353699e+13\n",
      "  6.92598092e+12 -5.87093432e+12 -1.38640408e+13 -8.93821927e+11\n",
      " -8.57231631e+12 -1.46365380e+13 -5.02535125e+12 -1.10339763e+11\n",
      " -7.58758280e+10 -4.32970977e+12 -2.36531446e+11  1.93151883e+10\n",
      " -7.26005507e+12 -1.28478765e+11 -1.30368820e+13 -1.51767557e+13\n",
      " -1.23622040e+13 -1.18065321e+13 -1.18065972e+13 -1.47124167e+13\n",
      " -1.46364756e+13 -1.46363982e+13 -1.42425302e+13]\n",
      "Gradient Descent(83/99): loss=1.9314284715781017e+28, weights = [-3.63619808e-01  5.47174335e+12 -5.55503868e+12 -8.01038198e+11\n",
      "  1.78056242e+13  2.12732545e+13  2.06427911e+13  2.12718285e+13\n",
      " -1.00665907e+13  8.53312961e+12  2.01507376e+13  1.29912854e+12\n",
      "  1.24594625e+13  2.12735264e+13  7.30411400e+12  1.60373707e+11\n",
      "  1.10281982e+11  6.29303150e+12  3.43787441e+11 -2.80737266e+10\n",
      "  1.05521519e+13  1.86737902e+11  1.89485008e+13  2.20587076e+13\n",
      "  1.79678877e+13  1.71602446e+13  1.71603392e+13  2.13838125e+13\n",
      "  2.12734357e+13  2.12733231e+13  2.07008544e+13]\n",
      "Gradient Descent(84/99): loss=4.080194537035012e+28, weights = [-2.43619808e-01 -7.95292413e+12  8.07399001e+12  1.16427171e+12\n",
      " -2.58796456e+13 -3.09196846e+13 -3.00033355e+13 -3.09176121e+13\n",
      "  1.46313208e+13 -1.24025065e+13 -2.92881586e+13 -1.88822283e+12\n",
      " -1.81092485e+13 -3.09200798e+13 -1.06161895e+13 -2.33095715e+11\n",
      " -1.60289725e+11 -9.14662820e+12 -4.99679034e+11  4.08038542e+10\n",
      " -1.53370614e+13 -2.71414843e+11 -2.75407634e+13 -3.20613042e+13\n",
      " -2.61154880e+13 -2.49416163e+13 -2.49417538e+13 -3.10803756e+13\n",
      " -3.09199480e+13 -3.09197844e+13 -3.00877277e+13]\n",
      "Gradient Descent(85/99): loss=8.619520580251035e+28, weights = [-4.21687008e-01  1.15592048e+13 -1.17351685e+13 -1.69221471e+12\n",
      "  3.76148596e+13  4.49403216e+13  4.36084508e+13  4.49373093e+13\n",
      " -2.12659433e+13  1.80264656e+13  4.25689745e+13  2.74444395e+12\n",
      "  2.63209493e+13  4.49408960e+13  1.54301371e+13  3.38793766e+11\n",
      "  2.32973651e+11  1.32941981e+13  7.26260205e+11 -5.93065018e+10\n",
      "  2.22917045e+13  3.94488835e+11  4.00292170e+13  4.65996125e+13\n",
      "  3.79576455e+13  3.62514777e+13  3.62516775e+13  4.51738784e+13\n",
      "  4.49407044e+13  4.49404666e+13  4.37311110e+13]\n",
      "Gradient Descent(86/99): loss=1.8208968802590592e+29, weights = [-1.64624608e-01 -1.68007659e+13  1.70565208e+13  2.45955528e+12\n",
      " -5.46714466e+13 -6.53186644e+13 -6.33828523e+13 -6.53142862e+13\n",
      "  3.09090582e+13 -2.62006282e+13 -6.18720219e+13 -3.98892147e+12\n",
      " -3.82562740e+13 -6.53194993e+13 -2.24269856e+13 -4.92420960e+11\n",
      " -3.38616351e+11 -1.93224978e+13 -1.05558538e+12  8.61992383e+10\n",
      " -3.23999543e+13 -5.73371149e+11 -5.81806026e+13 -6.77303666e+13\n",
      " -5.51696700e+13 -5.26898345e+13 -5.26901249e+13 -6.56581284e+13\n",
      " -6.53192209e+13 -6.53188752e+13 -6.35611332e+13]\n",
      "Gradient Descent(87/99): loss=3.846693580770626e+29, weights = [-5.34775008e-01  2.44191308e+13 -2.47908587e+13 -3.57484905e+12\n",
      "  7.94624014e+13  9.49376367e+13  9.21240240e+13  9.49312732e+13\n",
      " -4.49248766e+13  3.80813928e+13  8.99281023e+13  5.79771158e+12\n",
      "  5.56037126e+13  9.49388502e+13  3.25965791e+13  7.15710932e+11\n",
      "  4.92163096e+11  2.80843507e+13  1.53424419e+12 -1.25286578e+11\n",
      "  4.70918246e+13  8.33368262e+11  8.45627963e+13  9.84429335e+13\n",
      "  8.01865459e+13  7.65822205e+13  7.65826427e+13  9.54310318e+13\n",
      "  9.49384455e+13  9.49379430e+13  9.23831469e+13]\n",
      "Gradient Descent(88/99): loss=8.126243536776714e+29, weights = [ 6.12739206e-03 -3.54920693e+13  3.60323585e+13  5.19587660e+12\n",
      " -1.15494900e+14 -1.37987434e+14 -1.33897978e+14 -1.37978184e+14\n",
      "  6.52962159e+13 -5.53495309e+13 -1.30706309e+14 -8.42670377e+12\n",
      " -8.08174067e+13 -1.37989197e+14 -4.73776096e+13 -1.04025251e+12\n",
      " -7.15336139e+11 -4.08192958e+13 -2.22995248e+12  1.82098207e+11\n",
      " -6.84457738e+13 -1.21126196e+12 -1.22908086e+14 -1.43082219e+14\n",
      " -1.16547410e+14 -1.11308691e+14 -1.11309305e+14 -1.38704560e+14\n",
      " -1.37988609e+14 -1.37987879e+14 -1.34274602e+14]\n",
      "Gradient Descent(89/99): loss=1.7166907795597136e+30, weights = [-7.86192608e-01  5.15860698e+13 -5.23713550e+13 -7.55196466e+12\n",
      "  1.67866458e+14  2.00558308e+14  1.94614476e+14  2.00544865e+14\n",
      " -9.49050089e+13  8.04479654e+13  1.89975532e+14  1.22478215e+13\n",
      "  1.17464337e+14  2.00560872e+14  6.88611491e+13  1.51195857e+12\n",
      "  1.03970776e+12  5.93289454e+13  3.24113208e+12 -2.64671263e+11\n",
      "  9.94827446e+13  1.76051284e+12  1.78641179e+14  2.07963342e+14\n",
      "  1.69396233e+14  1.61781998e+14  1.61782890e+14  2.01600619e+14\n",
      "  2.00560017e+14  2.00558955e+14  1.95161880e+14]\n",
      "Gradient Descent(90/99): loss=3.6265553933845026e+30, weights = [ 3.66728992e-01 -7.49779500e+13  7.61193254e+13  1.09764289e+13\n",
      " -2.43986079e+14 -2.91502161e+14 -2.82863077e+14 -2.91482622e+14\n",
      "  1.37940010e+14 -1.16927371e+14 -2.76120588e+14 -1.78016381e+13\n",
      " -1.70728943e+14 -2.91505886e+14 -1.00086473e+14 -2.19756137e+12\n",
      " -1.51116681e+12 -8.62318590e+13 -4.71083453e+12  3.84687354e+11\n",
      " -1.44593536e+14 -2.55882343e+12 -2.59646634e+14 -3.02265032e+14\n",
      " -2.46209535e+14 -2.35142600e+14 -2.35143896e+14 -2.93017110e+14\n",
      " -2.91504644e+14 -2.91503101e+14 -2.83658703e+14]\n",
      "Gradient Descent(91/99): loss=7.661195701569041e+30, weights = [-1.30218621e+00  1.08976958e+14 -1.10635894e+14 -1.59537282e+13\n",
      "  3.54622402e+14  4.23684814e+14  4.11128308e+14  4.23656415e+14\n",
      " -2.00489381e+14  1.69948488e+14  4.01328416e+14  2.58738519e+13\n",
      "  2.48146567e+14  4.23690230e+14  1.45471028e+14  3.19405311e+12\n",
      "  2.19641058e+12  1.25333991e+14  6.84697860e+12 -5.59125154e+11\n",
      "  2.10159971e+14  3.71913068e+12  3.77384290e+14  4.39328147e+14\n",
      "  3.57854093e+14  3.41768818e+14  3.41770702e+14  4.25886723e+14\n",
      "  4.23688424e+14  4.23686181e+14  4.12284715e+14]\n",
      "Gradient Descent(92/99): loss=1.6184481749488633e+31, weights = [ 1.14599299e+00 -1.58392934e+14  1.60804120e+14  2.31880010e+13\n",
      " -5.15427146e+14 -6.15806146e+14 -5.97555849e+14 -6.15764869e+14\n",
      "  2.91401978e+14 -2.47012213e+14 -5.83312162e+14 -3.76064388e+13\n",
      " -3.60669479e+14 -6.15814017e+14 -2.11435364e+14 -4.64240743e+12\n",
      " -3.19238049e+12 -1.82167119e+14 -9.95176451e+12  8.12662371e+11\n",
      " -3.05457730e+14 -5.40558323e+12 -5.48510490e+14 -6.38543001e+14\n",
      " -5.20124257e+14 -4.96745060e+14 -4.96747798e+14 -6.19006517e+14\n",
      " -6.15811392e+14 -6.15808132e+14 -5.99236632e+14]\n",
      "Gradient Descent(93/99): loss=3.419015251703878e+31, weights = [-2.37554301e+00  2.30216753e+14 -2.33721299e+14 -3.37026795e+13\n",
      "  7.49149353e+14  8.95045553e+14  8.68519597e+14  8.94985560e+14\n",
      " -4.23539204e+14  3.59020747e+14  8.47817061e+14  5.46592073e+13\n",
      "  5.24216290e+14  8.95056993e+14  3.07311454e+14  6.74752300e+12\n",
      "  4.63997637e+12  2.64771423e+14  1.44644262e+13 -1.18116691e+12\n",
      "  4.43968584e+14  7.85676348e+12  7.97234452e+14  9.28092513e+14\n",
      "  7.55976385e+14  7.21995810e+14  7.21999790e+14  8.99697145e+14\n",
      "  8.95053178e+14  8.95048441e+14  8.70962536e+14]\n",
      "Gradient Descent(94/99): loss=7.22276157638048e+31, weights = [ 2.74158979e+00 -3.34609328e+14  3.39703024e+14  4.89852751e+13\n",
      " -1.08885369e+15 -1.30090703e+15 -1.26235279e+15 -1.30081983e+15\n",
      "  6.15594508e+14 -5.21819936e+14 -1.23226261e+15 -7.94446123e+13\n",
      " -7.61923964e+14 -1.30092366e+15 -4.46662886e+14 -9.80721043e+12\n",
      " -6.74398955e+12 -3.84832933e+14 -2.10233699e+13  1.71677109e+12\n",
      " -6.45287661e+14 -1.14194398e+13 -1.15874315e+15 -1.34893924e+15\n",
      " -1.09877647e+15 -1.04938729e+15 -1.04939307e+15 -1.30766790e+15\n",
      " -1.30091811e+15 -1.30091122e+15 -1.26590348e+15]\n",
      "Gradient Descent(95/99): loss=1.5258277880813823e+32, weights = [-4.68322941e+00  4.86339075e+14 -4.93742524e+14 -7.11978159e+13\n",
      "  1.58259813e+15  1.89080778e+15  1.83477098e+15  1.89068104e+15\n",
      " -8.94737947e+14  7.58440975e+14  1.79103632e+15  1.15469044e+14\n",
      "  1.10742100e+15  1.89083195e+15  6.49203703e+14  1.42543236e+13\n",
      "  9.80207473e+12  5.59336745e+14  3.05564890e+13 -2.49524683e+12\n",
      "  9.37895563e+14  1.65976239e+13  1.68417920e+15  1.96062037e+15\n",
      "  1.59702042e+15  1.52523555e+15  1.52524395e+15  1.90063439e+15\n",
      "  1.89082389e+15  1.89081388e+15  1.83993176e+15]\n",
      "Gradient Descent(96/99): loss=3.2233521960557643e+32, weights = [ 6.01347459e+00 -7.06871196e+14  7.17631764e+14  1.03482710e+14\n",
      " -2.30023268e+15 -2.74820105e+15 -2.66675417e+15 -2.74801684e+15\n",
      "  1.30045994e+15 -1.10235863e+15 -2.60318788e+15 -1.67828878e+14\n",
      " -1.60958484e+15 -2.74823617e+15 -9.43587347e+14 -2.07179955e+13\n",
      " -1.42468591e+13 -8.12969910e+14 -4.44124337e+13  3.62672506e+12\n",
      " -1.36318752e+15 -2.41238734e+13 -2.44787603e+15 -2.84967040e+15\n",
      " -2.32119481e+15 -2.21685883e+15 -2.21687105e+15 -2.76248357e+15\n",
      " -2.74822446e+15 -2.74820991e+15 -2.67425512e+15]\n",
      "Gradient Descent(97/99): loss=6.809418114531891e+32, weights = [-9.79298941e+00  1.02740436e+15 -1.04304435e+15 -1.50407299e+14\n",
      "  3.34328108e+15  3.99438223e+15  3.87600300e+15  3.99411449e+15\n",
      " -1.89015795e+15  1.60222694e+15  3.78361234e+15  2.43931458e+14\n",
      "  2.33945661e+15  3.99443329e+15  1.37146026e+15  3.01126416e+13\n",
      "  2.07071462e+13  1.18161390e+15  6.45514041e+13 -5.27127596e+12\n",
      "  1.98132957e+15  3.50629265e+13  3.55787381e+15  4.14186320e+15\n",
      "  3.37374855e+15  3.22210107e+15  3.22211883e+15  4.01514121e+15\n",
      "  3.99441626e+15  3.99439512e+15  3.88690527e+15]\n",
      "Gradient Descent(98/99): loss=1.4385078712544533e+33, weights = [ 1.32560218e+01 -1.49328439e+15  1.51601638e+15  2.18610004e+14\n",
      " -4.85930334e+15 -5.80564855e+15 -5.63358984e+15 -5.80525940e+15\n",
      "  2.74725655e+15 -2.32876223e+15 -5.49930433e+15 -3.54543013e+14\n",
      " -3.40029123e+15 -5.80572275e+15 -1.99335362e+15 -4.37673222e+13\n",
      " -3.00968727e+13 -1.71742078e+15 -9.38224595e+13  7.66155412e+12\n",
      " -2.87977026e+15 -5.09623307e+13 -5.17120388e+15 -6.02000527e+15\n",
      " -4.90358640e+15 -4.68317385e+15 -4.68319966e+15 -5.83582076e+15\n",
      " -5.80569800e+15 -5.80566728e+15 -5.64943579e+15]\n",
      "Gradient Descent(99/99): loss=3.038886525773663e+33, weights = [-1.97249702e+01  2.17041932e+15 -2.20345921e+15 -3.17739460e+14\n",
      "  7.06277110e+15  8.43823978e+15  8.18816046e+15  8.43767418e+15\n",
      " -3.99300946e+15  3.38474745e+15  7.99298273e+15  5.15311758e+14\n",
      "  4.94216494e+15  8.43834763e+15  2.89724666e+15  6.36137644e+13\n",
      "  4.37444027e+13  2.49619111e+15  1.36366575e+14 -1.11357121e+13\n",
      "  4.18561196e+15  7.40713742e+13  7.51610400e+15  8.74979730e+15\n",
      "  7.12713445e+15  6.80677508e+15  6.80681260e+15  8.48209368e+15\n",
      "  8.43831166e+15  8.43826700e+15  8.21119181e+15]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.2\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best value of gamma  with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value of gamma = 0.15210526315789474 \n",
      " Loss = 0.8268371252196446 \n",
      " Weights = [-3.14660071e-01  3.97232605e-02 -2.82696560e-01 -2.46642368e-01\n",
      " -1.10669348e-01  2.47403887e-01  2.84252790e-01  1.10460377e-01\n",
      "  2.43046474e-01 -1.67976046e-02  3.16603657e-02 -1.46096111e-01\n",
      "  1.27518187e-01 -1.57790911e-01  2.07922276e-01 -7.64695236e-04\n",
      " -1.31130433e-03  2.69165231e-01 -9.78483228e-04  2.38747262e-03\n",
      "  1.73120204e-01  8.72268585e-04 -9.11681190e-03 -1.46764924e-01\n",
      " -1.76043186e-02  3.59461749e-01 -2.07502388e-01  2.27786113e-01\n",
      " -4.99030871e-01 -2.40182437e-04 -1.77637595e-01]\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import cross_validation\n",
    "from cost import compute_loss_rmse\n",
    "\n",
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.linspace(0.01, 0.16, 20)\n",
    "k_fold = 4\n",
    "seed = 6\n",
    "\n",
    "# prepare storage of the mean of the weights and rmse for train and test data\n",
    "ws = np.zeros((num_features, len(gammas)))\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    # prepare storage of weights and rmse for train and test data for each fold\n",
    "    ws_tmp = np.zeros((num_features, k_fold))\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross-validation\n",
    "    for i,k in enumerate(range(k_fold)):\n",
    "        tx_train, y_train, tx_test, y_test = cross_validation(y, tx, k, k_fold, seed)\n",
    "        w,_ = least_squares_GD(y_train, tx_train, w_initial, max_iters, gamma, printing=False)\n",
    "        # store weights and rmse for train and test data for each fold\n",
    "        ws_tmp[:, i] = w\n",
    "        rmse_tr.append(compute_loss_rmse(y_train, tx_train, w))\n",
    "        rmse_te.append(compute_loss_rmse(y_test, tx_test, w))\n",
    "    # store the mean of the weights and rmse for train and test data\n",
    "    ws[:, ind] = np.mean(ws_tmp, 1)\n",
    "    rmse_train.append(np.mean(rmse_tr))\n",
    "    rmse_test.append(np.mean(rmse_te))\n",
    "    \n",
    "loss = np.amin(rmse_test)\n",
    "weights = ws[:,np.argmin(rmse_test)]\n",
    "gamma_star = gammas[np.argmin(rmse_test)]\n",
    "\n",
    "plt.plot(gammas, rmse_train, color='b', label=\"Train error\")\n",
    "plt.plot(gammas, rmse_test, color='g', label=\"Test error\")\n",
    "plt.plot(gamma_star, loss, 'r*', markersize=15, label=\"Minimal loss\")\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title(\"Cross validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss, we = weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tx_test = build_model_data(tX_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv'\n",
    "y_pred = predict_labels(weights, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
