{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=31.158562417782754, weights = [ 0.24084039  0.48622211  0.86606501 -0.04788257 -0.50932991 -1.16372163\n",
      " -0.46792284 -0.82546922  0.99143524 -0.19301815 -1.1115627   0.08773264\n",
      " -0.3840451  -0.73944795 -0.1095499   0.73934211  0.30197745 -0.26341767\n",
      "  0.2312355   0.23039363  0.13396917  0.66641327 -0.48796928 -0.64085351\n",
      " -0.41338724 -0.33307157 -0.99023927 -0.83372347 -1.14126282 -0.83964126\n",
      " -0.76535492]\n",
      "Gradient Descent(1/99): loss=60.32831908428609, weights = [ 0.12973951  0.93712383  0.21280519 -0.22169698  1.11689625  0.84129952\n",
      "  1.48238341  1.17923689 -0.02107089  0.53641031  0.73213543  0.11135228\n",
      "  0.77981329  1.26554459  0.59436822  0.57665394  0.25670048  0.2359958\n",
      "  0.12931293  0.19267125  1.03561237  0.54778519  1.23286171  1.39757221\n",
      "  1.22187688  1.2274432   0.57051721  1.1805799   0.86366761  1.16534933\n",
      "  1.13772217]\n",
      "Gradient Descent(2/99): loss=124.62487852410464, weights = [ 0.04085881  0.14799145  0.86603845 -0.1694721  -1.29764678 -2.01289761\n",
      " -1.28309573 -1.67490142  1.27907989 -0.65837633 -1.99563382 -0.11264289\n",
      " -0.89697275 -1.58872632 -0.36885331  0.42801663  0.19653058 -0.64270614\n",
      " -0.00393141  0.16640854 -0.44507287  0.41406165 -1.34585312 -1.58704115\n",
      " -1.22208457 -1.10797322 -1.7647351  -1.68902193 -1.99062921 -1.68888038\n",
      " -1.66431264]\n",
      "Gradient Descent(3/99): loss=261.565999731644, weights = [-0.03024575  1.17544574 -0.29525024 -0.35728624  2.18851635  2.16729701\n",
      "  2.7767967   2.50490932 -0.73874132  0.98397356  1.95551441  0.11909909\n",
      "  1.54898344  2.59149417  1.08202682  0.36768224  0.1811721   0.58977682\n",
      "  0.00843808  0.13533873  1.57956438  0.36410809  2.35594857  2.73134497\n",
      "  2.28930481  2.24443438  1.58782622  2.51261616  2.18954569  2.49131083\n",
      "  2.39075494]\n",
      "Gradient Descent(4/99): loss=551.3806582479436, weights = [-0.0871294  -0.42520997  1.23105364 -0.14513584 -2.88909924 -3.89149995\n",
      " -3.09932286 -3.55356726  2.09694612 -1.47062024 -3.78440273 -0.26074773\n",
      " -2.0004436  -3.46740196 -0.98459725  0.25479782  0.11950909 -1.19181438\n",
      " -0.12315602  0.12167833 -1.46474374  0.24185447 -3.05461506 -3.5621228\n",
      " -2.83949572 -2.65475816 -3.31128933 -3.57780019 -3.86934545 -3.56751975\n",
      " -3.5115212 ]\n",
      "Gradient Descent(5/99): loss=1163.8971564839414, weights = [-1.32636323e-01  1.80421836e+00 -1.11752912e+00 -4.84647365e-01\n",
      "  4.48571480e+00  4.92378968e+00  5.45744603e+00  5.26105752e+00\n",
      " -2.09939928e+00  2.04800536e+00  4.56844928e+00  2.74699122e-01\n",
      "  3.16243938e+00  5.34798226e+00  2.05364236e+00  2.71340190e-01\n",
      "  1.40462505e-01  1.43328631e+00  9.85006865e-06  9.17440307e-02\n",
      "  2.87555664e+00  2.64188267e-01  4.78747721e+00  5.57078828e+00\n",
      "  4.59927063e+00  4.44892242e+00  3.79251086e+00  5.28324987e+00\n",
      "  4.94598506e+00  5.24778528e+00  5.06323243e+00]\n",
      "Gradient Descent(6/99): loss=2458.014063646091, weights = [-1.69041858e-01 -1.52456071e+00  2.18664286e+00 -5.02101033e-03\n",
      " -6.23650408e+00 -7.88384891e+00 -6.96819524e+00 -7.54578848e+00\n",
      "  3.94126452e+00 -3.10182730e+00 -7.55919083e+00 -5.07002405e-01\n",
      " -4.33855628e+00 -7.45983569e+00 -2.33425867e+00  1.37087516e-01\n",
      "  5.38042842e-02 -2.33566520e+00 -2.16726539e-01  9.32119247e-02\n",
      " -3.50491072e+00  1.07894123e-01 -6.62826878e+00 -7.71566708e+00\n",
      " -6.22249478e+00 -5.88695621e+00 -6.54336214e+00 -7.59096103e+00\n",
      " -7.86179122e+00 -7.55990746e+00 -7.40149514e+00]\n",
      "Gradient Descent(7/99): loss=5191.970189635738, weights = [-0.19816629  3.23266562 -2.71054518 -0.705831    9.34497189 10.73422315\n",
      " 11.10028137 11.0709756  -4.88502093  4.35744943 10.0811038   0.63183487\n",
      "  6.56625493 11.15846035  4.06604687  0.24843951  0.13371149  3.1919845\n",
      "  0.08063104  0.05564604  5.7064638   0.23636706  9.94862921 11.5851242\n",
      "  9.50026564  9.12867249  8.47239832 11.12388386 10.75641473 11.05821166\n",
      " 10.71477958]\n",
      "Gradient Descent(8/99): loss=10967.591627479793, weights = [ -0.22146583  -3.75625544   4.32354735   0.31499157 -13.30484089\n",
      " -16.32470538 -15.15473756 -15.98619481   7.90622019  -6.50251172\n",
      " -15.54541121  -1.01834379  -9.28126608 -15.90082676  -5.21819371\n",
      "   0.02169211  -0.02019527  -4.79323098  -0.35617649   0.08042459\n",
      "  -7.73617872  -0.02901532 -14.15920601 -16.47676004 -13.35581527\n",
      " -12.70034499 -13.35670093 -16.07564223 -16.30276622 -16.00081744\n",
      " -15.61644309]\n",
      "Gradient Descent(9/99): loss=23168.792485455222, weights = [-2.40105463e-01  6.33308639e+00 -5.97523443e+00 -1.16308277e+00\n",
      "  1.96124681e+01  2.30051029e+01  2.30112944e+01  2.33409251e+01\n",
      " -1.07156052e+01  9.26945788e+00  2.17134670e+01  1.38566430e+00\n",
      "  1.37542378e+01  2.34294724e+01  8.29071775e+00  2.99975407e-01\n",
      "  1.72493398e-01  6.85941606e+00  2.82383699e-01  1.93378096e-02\n",
      "  1.17545477e+01  2.94023387e-01  2.08671014e+01  2.43018605e+01\n",
      "  1.98622369e+01  1.90243583e+01  1.83682077e+01  2.34586037e+01\n",
      "  2.30273572e+01  2.33291041e+01  2.26550174e+01]\n",
      "Gradient Descent(10/99): loss=48944.18965801178, weights = [ -0.25501717  -8.39479151   8.92567494   0.99278006 -28.23473064\n",
      " -34.15839486 -32.45639385 -33.81879034  16.32547477 -13.66238463\n",
      " -32.42965128  -2.10335539 -19.72511246 -33.73476694 -11.33212331\n",
      "  -0.14583121  -0.1330583  -10.03380576  -0.63696595   0.0870636\n",
      " -16.61594115  -0.22545564 -30.05477486 -34.97517047 -28.41977897\n",
      " -27.08758214 -27.74396262 -34.00192902 -34.13664587 -33.83459183\n",
      " -32.97031341]\n",
      "Gradient Descent(11/99): loss=103395.43274446868, weights = [-2.66946537e-01  1.29528032e+01 -1.27938935e+01 -2.13212866e+00\n",
      "  4.13052924e+01  4.89264206e+01  4.81676623e+01  4.92604092e+01\n",
      " -2.29980014e+01  1.96634088e+01  4.62747959e+01  2.97199409e+00\n",
      "  2.89370880e+01  4.93511002e+01  1.71980437e+01  4.68167168e-01\n",
      "  2.90072851e-01  1.45597676e+01  7.10972900e-01 -2.90515997e-02\n",
      "  2.45827799e+01  4.89762888e-01  4.39454559e+01  5.11746527e+01\n",
      "  4.17555000e+01  3.99332465e+01  3.92772549e+01  4.95147338e+01\n",
      "  4.89488602e+01  4.92504777e+01  4.78791149e+01]\n",
      "Gradient Descent(12/99): loss=218425.16492605233, weights = [ -0.27649003 -18.12919305  18.71800636   2.41873702 -59.7717035\n",
      " -71.83328217 -69.01166797 -71.49124424  34.13981619 -28.77599504\n",
      " -68.10940454  -4.40158302 -41.78977004 -71.41015592 -24.26197596\n",
      "  -0.45265452  -0.34220756 -21.14896029  -1.23497346   0.12488521\n",
      " -35.32957156  -0.581519   -63.62240642 -74.04620686 -60.24086064\n",
      " -57.47854838 -58.1350614  -71.87250595 -71.81188755 -71.50962879\n",
      " -69.63104386]\n",
      "Gradient Descent(13/99): loss=461428.5787450928, weights = [ -0.28412482  26.99688306 -27.13486031  -4.18671336  87.13509619\n",
      " 103.68545137 101.30672377 104.01568133 -48.92151316  41.62853981\n",
      "  98.15095175   6.31775593  61.0096794  104.11081168  36.00379111\n",
      "   0.8615786    0.56253197  30.78595649   1.6071674   -0.11129089\n",
      "  51.72230064   0.95022603  92.71084519 107.95074054  88.00661067\n",
      "  84.10523571  83.44951577 104.55846653 103.70832567 104.0096568\n",
      " 101.16532345]\n",
      "Gradient Descent(14/99): loss=974779.8302516069, weights = [  -0.29023266  -38.63811378   39.46255235    5.42286424 -126.39104603\n",
      " -151.4227724  -146.23966391 -151.07548463   71.79243026  -60.69951717\n",
      " -143.49318721   -9.2610732   -88.40339854 -151.00068161  -51.58554426\n",
      "   -1.06939813   -0.76423037  -44.66775088   -2.50997593    0.22155659\n",
      "  -74.82797059   -1.29628467 -134.52354766 -156.57880982 -127.4634049\n",
      " -121.67962333 -122.33646743 -151.87550232 -151.40208611 -151.09940415\n",
      " -147.07832399]\n",
      "Gradient Descent(15/99): loss=2059248.149970665, weights = [  -0.29511893   56.71649291  -57.37750233   -8.53564323  183.95530097\n",
      "  219.36520054  213.56075507  219.68759456 -103.6695572    88.03234955\n",
      "  207.73224801   13.38239203  128.76257122  219.79202198   75.72444898\n",
      "    1.7190518     1.15443301   65.02957975    3.48747851   -0.27095524\n",
      "  109.08561679    1.95280327  195.74009403  227.89731932  185.71320242\n",
      "  177.41987861  176.76469168  220.83953835  219.38903106  219.68975078\n",
      "  213.7332593 ]\n",
      "Gradient Descent(16/99): loss=4350216.618554008, weights = [-2.99027942e-01 -8.19164990e+01  8.33346095e+01  1.17605804e+01\n",
      " -2.67122596e+02 -3.19557818e+02 -3.09389284e+02 -3.19199340e+02\n",
      "  1.51347990e+02 -1.28138855e+02 -3.02751165e+02 -1.95292394e+01\n",
      " -1.86877142e+02 -3.19137893e+02 -1.09312707e+02 -2.34979495e+00\n",
      " -1.64229928e+00 -9.43836383e+01 -5.21678523e+00  4.37572343e-01\n",
      " -1.58242834e+02 -2.78245606e+00 -2.84293372e+02 -3.30925921e+02\n",
      " -2.69473295e+02 -2.57306370e+02 -2.57963946e+02 -3.20884218e+02\n",
      " -3.19538592e+02 -3.19235020e+02 -3.10688494e+02]\n",
      "Gradient Descent(17/99): loss=9189949.064813782, weights = [-3.02155153e-01  1.19543645e+02 -1.21221181e+02 -1.77312859e+01\n",
      "  3.88494224e+02  4.63741711e+02  4.50697219e+02  4.64047648e+02\n",
      " -2.19315071e+02  1.86060150e+02  4.39218637e+02  2.83051967e+01\n",
      "  2.71891287e+02  4.64171640e+02  1.59631349e+02  3.54991982e+00\n",
      "  2.41595961e+00  1.37340888e+02  7.44646975e+00 -5.98365588e-01\n",
      "  2.30290398e+02  4.08974951e+00  4.13402587e+02  4.81292663e+02\n",
      "  3.92120299e+02  3.74549044e+02  3.73894957e+02  4.66486224e+02\n",
      "  4.63767596e+02  4.64067022e+02  4.51535249e+02]\n",
      "Gradient Descent(18/99): loss=19414013.889398295, weights = [-3.04656923e-01 -1.73302907e+02  1.76056680e+02  2.51410570e+01\n",
      " -5.64418134e+02 -6.74747659e+02 -6.54050258e+02 -6.74365449e+02\n",
      "  3.19420622e+02 -2.70608847e+02 -6.39194385e+02 -4.12216071e+01\n",
      " -3.94906551e+02 -6.74332290e+02 -2.31265832e+02 -5.03768660e+00\n",
      " -3.48805140e+00 -1.99436780e+02 -1.09477684e+01  9.02192867e-01\n",
      " -3.34438671e+02 -5.90689321e+00 -6.00676014e+02 -6.99234099e+02\n",
      " -5.69473884e+02 -5.43822238e+02 -5.44481383e+02 -6.77919854e+02\n",
      " -6.74731486e+02 -6.74426036e+02 -6.56320253e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(19/99): loss=41012625.5208341, weights = [-3.06658338e-01  2.52304934e+02 -2.56054707e+02 -3.71650802e+01\n",
      "  8.20591812e+02  9.79993632e+02  9.51651496e+02  9.80264891e+02\n",
      " -4.63611469e+02  3.93143143e+02  9.28233893e+02  5.98301722e+01\n",
      "  5.74253667e+02  9.80430144e+02  3.36884921e+02  7.43265419e+00\n",
      "  5.08857447e+00  2.90075496e+02  1.57978009e+01 -1.28309261e+00\n",
      "  4.86356477e+02  8.61619148e+00  8.73230291e+02  1.01660176e+03\n",
      "  8.28159823e+02  7.90988838e+02  7.90337054e+02  9.85421274e+02\n",
      "  9.80023888e+02  9.80320580e+02  9.53897387e+02]\n",
      "Gradient Descent(20/99): loss=86640272.87595862, weights = [-3.08259470e-01 -3.66324785e+02  3.71969583e+02  5.34003325e+01\n",
      " -1.19245937e+03 -1.42509576e+03 -1.38215857e+03 -1.42466332e+03\n",
      "  6.74485157e+02 -5.71584461e+02 -1.34994525e+03 -8.70465189e+01\n",
      " -8.34375047e+02 -1.42468999e+03 -4.88896310e+02 -1.07026225e+01\n",
      " -7.38095285e+00 -4.21387538e+02 -2.30660352e+01  1.88952423e+00\n",
      " -7.06641662e+02 -1.24976975e+01 -1.26903329e+03 -1.47729050e+03\n",
      " -1.20323430e+03 -1.14909567e+03 -1.14975815e+03 -1.43216739e+03\n",
      " -1.42508600e+03 -1.42477658e+03 -1.38647765e+03]\n",
      "Gradient Descent(21/99): loss=183029903.77421427, weights = [-3.09540376e-01  5.32798414e+02 -5.40861781e+02 -7.82263981e+01\n",
      "  1.73341310e+03  2.07058968e+03  2.00992823e+03  2.07078776e+03\n",
      " -9.79688661e+02  8.30607222e+02  1.96128662e+03  1.26428892e+02\n",
      "  1.21300163e+03  2.07104011e+03  7.11337283e+02  1.56469293e+01\n",
      "  1.07397671e+01  6.12709889e+02  3.34295778e+01 -2.72473437e+00\n",
      "  1.02731640e+03  1.81861374e+01  1.84463793e+03  2.14746105e+03\n",
      "  1.74930385e+03  1.67072805e+03  1.67008112e+03  2.08168534e+03\n",
      "  2.07062920e+03  2.07092011e+03  2.01514982e+03]\n",
      "Gradient Descent(22/99): loss=386655588.7064421, weights = [-3.10565101e-01 -7.74059482e+02  7.85870790e+02  1.13092351e+02\n",
      " -2.51920988e+03 -3.01022628e+03 -2.92030948e+03 -3.00968766e+03\n",
      "  1.42457314e+03 -1.20740807e+03 -2.85143034e+03 -1.83851024e+02\n",
      " -1.76276465e+03 -3.00984078e+03 -1.03314764e+03 -2.26592709e+01\n",
      " -1.56004795e+01 -8.90284619e+02 -4.86760523e+01  3.97935845e+00\n",
      " -1.49291879e+03 -2.64147762e+01 -2.68094703e+03 -3.12095120e+03\n",
      " -2.54207143e+03 -2.42775316e+03 -2.42842267e+03 -3.02553575e+03\n",
      " -3.01023005e+03 -3.00991224e+03 -2.92895590e+03]\n",
      "Gradient Descent(23/99): loss=816820318.8482475, weights = [-3.11384881e-01  1.12537534e+03 -1.14249667e+03 -1.64975551e+02\n",
      "  3.66177602e+03  4.37450311e+03  4.24556383e+03  4.37454670e+03\n",
      " -2.06991174e+03  1.75475718e+03  4.14363376e+03  2.67122752e+02\n",
      "  2.56237334e+03  4.37498298e+03  1.50237839e+03  3.30094320e+01\n",
      "  2.26816602e+01  1.29426485e+03  7.06680907e+01 -5.76683472e+00\n",
      "  2.17011890e+03  3.84078485e+01  3.89677138e+03  4.53643599e+03\n",
      "  3.69524411e+03  3.52919966e+03  3.52856298e+03  4.39757247e+03\n",
      "  4.37456224e+03  4.37484096e+03  4.25707293e+03]\n",
      "Gradient Descent(24/99): loss=1725554868.3183553, weights = [-3.12040705e-01 -1.63538520e+03  1.66027386e+03  2.39187766e+02\n",
      " -5.32200262e+03 -6.35885710e+03 -6.16969697e+03 -6.35809409e+03\n",
      "  3.00915738e+03 -2.55060725e+03 -6.02336219e+03 -3.88350503e+02\n",
      " -3.72401307e+03 -6.35851442e+03 -2.18289240e+03 -4.79093131e+01\n",
      " -3.29614622e+01 -1.88086002e+03 -1.02786261e+02  8.39704036e+00\n",
      " -3.15394158e+03 -5.58110153e+01 -5.66364428e+03 -6.59322429e+03\n",
      " -5.37040211e+03 -5.12895258e+03 -5.12963698e+03 -6.39156940e+03\n",
      " -6.35888942e+03 -6.35855389e+03 -6.18748386e+03]\n",
      "Gradient Descent(25/99): loss=3645281018.640926, weights = [-3.12565364e-01  2.37723282e+03 -2.41344278e+03 -3.48240618e+02\n",
      "  7.73549779e+03  9.24158213e+03  8.96840180e+03  9.24129942e+03\n",
      " -4.37303460e+03  3.70704410e+03  8.75389668e+03  5.64345268e+02\n",
      "  5.41295698e+03  9.24212419e+03  3.17347509e+03  6.96960632e+01\n",
      "  4.79117142e+01  2.73405133e+03  1.49327779e+02 -1.21909677e+01\n",
      "  4.58432745e+03  8.11299392e+01  8.23196660e+03  9.58321398e+03\n",
      "  7.80609388e+03  7.45526970e+03  7.45465467e+03  9.28994607e+03\n",
      "  9.24168269e+03  9.24193565e+03  8.99319447e+03]\n",
      "Gradient Descent(26/99): loss=7700754087.62386, weights = [-3.12985091e-01 -3.45493793e+03  3.50749589e+03  5.05562977e+02\n",
      " -1.12429743e+04 -1.34329297e+04 -1.30341175e+04 -1.34316926e+04\n",
      "  6.35663637e+03 -5.38815619e+03 -1.27241573e+04 -8.20357918e+02\n",
      " -7.86720334e+03 -1.34326774e+04 -4.61175771e+03 -1.01243560e+02\n",
      " -6.96349386e+01 -3.97349291e+03 -2.17102556e+02  1.77315085e+01\n",
      " -6.66289036e+03 -1.17908800e+02 -1.19646644e+04 -1.39284903e+04\n",
      " -1.13453280e+04 -1.08353101e+04 -1.08360259e+04 -1.35024061e+04\n",
      " -1.34330223e+04 -1.34326493e+04 -1.30712130e+04]\n",
      "Gradient Descent(27/99): loss=16268049902.435757, weights = [-3.13320873e-01  5.02183470e+03 -5.09833013e+03 -7.35397345e+02\n",
      "  1.63413477e+04  1.95234173e+04  1.89455217e+04  1.95224453e+04\n",
      " -9.23844169e+03  7.83129819e+03  1.84931973e+04  1.19223927e+03\n",
      "  1.14348913e+04  1.95240908e+04  6.70371336e+03  1.47204047e+02\n",
      "  1.01212600e+02  5.77562405e+03  3.15492058e+02 -2.57604505e+01\n",
      "  9.68441344e+03  1.71383568e+02  1.73901906e+04  2.02446723e+04\n",
      "  1.64903732e+04  1.57491970e+04  1.57486277e+04  1.96252165e+04\n",
      "  1.95236054e+04  1.95238040e+04  1.89983772e+04]\n",
      "Gradient Descent(28/99): loss=34366692484.937485, weights = [-3.13589498e-01 -7.29877418e+03  7.40982101e+03  1.06828379e+03\n",
      " -2.37511830e+04 -2.83770981e+04 -2.75353925e+04 -2.83748594e+04\n",
      "  1.34282772e+04 -1.13825589e+04 -2.68797704e+04 -1.73298153e+03\n",
      " -1.66198043e+04 -2.83770369e+04 -9.74279916e+03 -2.13907689e+02\n",
      " -1.47107212e+02 -8.39425024e+03 -4.58604675e+02  3.74522213e+01\n",
      " -1.40756342e+04 -2.49090370e+02 -2.52757314e+04 -2.94244325e+04\n",
      " -2.39675207e+04 -2.28901447e+04 -2.28909270e+04 -2.85242399e+04\n",
      " -2.83773181e+04 -2.83768660e+04 -2.76132771e+04]\n",
      "Gradient Descent(29/99): loss=72600561188.6213, weights = [-3.13804399e-01  1.06086436e+04 -1.07702092e+04 -1.55327995e+03\n",
      "  3.45214399e+04  4.12440703e+04  4.00224537e+04  4.12416424e+04\n",
      " -1.95167437e+04  1.65438915e+04  3.90677283e+04  2.51868606e+03\n",
      "  2.41563891e+04  4.12450213e+04  1.41614385e+04  3.10947112e+02\n",
      "  2.13813345e+02  1.22010147e+04  6.66513194e+02 -5.44251775e+01\n",
      "  2.04584861e+04  3.62048134e+02  3.67371922e+04  4.27672936e+04\n",
      "  3.48361452e+04  3.32703402e+04  3.32698674e+04  4.14587529e+04\n",
      "  4.12444434e+04  4.12445270e+04  4.01345931e+04]\n",
      "Gradient Descent(30/99): loss=153370636037.43503, weights = [-3.13976319e-01 -1.54189665e+04  1.56536040e+04  2.25704335e+03\n",
      " -5.01751069e+04 -5.99470555e+04 -5.81697275e+04 -5.99427007e+04\n",
      "  2.83673093e+04 -2.40458998e+04 -5.67838866e+04 -3.66091948e+03\n",
      " -3.51099090e+04 -5.99473978e+04 -2.05822601e+04 -4.51908804e+02\n",
      " -3.10768479e+02 -1.77332317e+04 -9.68789289e+02  7.91137360e+01\n",
      " -2.97352513e+04 -5.26213863e+02 -5.33957110e+04 -6.21600227e+04\n",
      " -5.06322432e+04 -4.83563080e+04 -4.83572306e+04 -6.02582676e+04\n",
      " -5.99475444e+04 -5.99469252e+04 -5.83337795e+04]\n",
      "Gradient Descent(31/99): loss=323999589169.08966, weights = [-3.14113855e-01  2.24109412e+04 -2.27521909e+04 -3.28108189e+03\n",
      "  7.29273761e+04  8.71295332e+04  8.45480367e+04  8.71240296e+04\n",
      " -4.12299320e+04  3.49494752e+04  8.25319747e+04  5.32084460e+03\n",
      "  5.10308951e+04  8.71310707e+04  2.99160852e+04  6.56863183e+02\n",
      "  4.51686269e+02  2.57748129e+04  1.40805030e+03 -1.14979364e+02\n",
      "  4.32190069e+04  7.64833048e+02  7.76082594e+04  9.03469403e+04\n",
      "  7.35920802e+04  7.02842247e+04  7.02839560e+04  8.75826847e+04\n",
      "  8.71302972e+04  8.71301379e+04  8.47854157e+04]\n",
      "Gradient Descent(32/99): loss=684457836871.597, weights = [-3.14223884e-01 -3.25730783e+04  3.30688312e+04  4.76832719e+03\n",
      " -1.05996353e+05 -1.26639439e+05 -1.22885585e+05 -1.26630614e+05\n",
      "  5.99264167e+04 -5.07975514e+04 -1.19957138e+05 -7.73373600e+03\n",
      " -7.41707520e+04 -1.26640633e+05 -4.34809103e+04 -9.54688506e+02\n",
      " -6.56506615e+02 -3.74620925e+04 -2.04657207e+03  1.67125492e+02\n",
      " -6.28166109e+04 -1.11164395e+03 -1.12799917e+05 -1.31314832e+05\n",
      " -1.06962187e+05 -1.02154263e+05 -1.02155482e+05 -1.27297254e+05\n",
      " -1.26640496e+05 -1.26639524e+05 -1.23231668e+05]\n",
      "Gradient Descent(33/99): loss=1445935569413.9224, weights = [-3.14311908e-01  4.73436237e+04 -4.80644360e+04 -6.93111231e+03\n",
      "  1.54060951e+05  1.84063806e+05  1.78609525e+05  1.84051805e+05\n",
      " -8.70996252e+04  7.38317616e+04  1.74351360e+05  1.12404841e+04\n",
      "  1.07804011e+05  1.84066583e+05  6.31982019e+04  1.38762386e+03\n",
      "  9.54199784e+02  5.44498151e+04  2.97456365e+03 -2.42901134e+02\n",
      "  9.13012204e+04  1.61572761e+03  1.63949493e+05  1.90860226e+05\n",
      "  1.55465033e+05  1.48477049e+05  1.48477211e+05  1.85020730e+05\n",
      "  1.84065396e+05  1.84064724e+05  1.79111477e+05]\n",
      "Gradient Descent(34/99): loss=3054577737691.795, weights = [-3.14382325e-01 -6.88115895e+04  6.98589788e+04  1.00734796e+04\n",
      " -2.23920232e+05 -2.67528890e+05 -2.59599574e+05 -2.67510621e+05\n",
      "  1.26595879e+05 -1.07311137e+05 -2.53412333e+05 -1.63376660e+04\n",
      " -1.56687832e+05 -2.67531886e+05 -9.18549219e+04 -2.01682077e+03\n",
      " -1.38688724e+03 -7.91398518e+04 -4.32342055e+03  3.53053171e+02\n",
      " -1.32701870e+05 -2.34838025e+03 -2.38292892e+05 -2.77406222e+05\n",
      " -2.25960708e+05 -2.15803890e+05 -2.15805735e+05 -2.68918914e+05\n",
      " -2.67531147e+05 -2.67529429e+05 -2.60330201e+05]\n",
      "Gradient Descent(35/99): loss=6452877536849.498, weights = [-3.14438662e-01  1.00014595e+05 -1.01537222e+05 -1.46419015e+04\n",
      "  3.25457811e+05  3.88840062e+05  3.77316945e+05  3.88814336e+05\n",
      " -1.84000584e+05  1.55971633e+05  3.68322276e+05  2.37458859e+04\n",
      "  2.27738744e+05  3.88845456e+05  1.33507570e+05  2.93137850e+03\n",
      "  2.01577348e+03  1.15026517e+05  6.28386209e+03 -5.13138908e+02\n",
      "  1.92876189e+05  3.41326563e+03  3.46347676e+05  4.03197248e+05\n",
      "  3.28423844e+05  3.13661490e+05  3.13662563e+05  3.90861217e+05\n",
      "  3.88843397e+05  3.88841640e+05  3.78377813e+05]\n",
      "Gradient Descent(36/99): loss=13631877163173.555, weights = [-3.14483727e-01 -1.45366409e+05  1.47579165e+05  2.12807551e+04\n",
      " -4.73037592e+05 -5.65161641e+05 -5.48411552e+05 -5.65123423e+05\n",
      "  2.67436910e+05 -2.26697609e+05 -5.35340020e+05 -3.45136949e+04\n",
      " -3.31007381e+05 -5.65168441e+05 -1.94046319e+05 -4.26060093e+03\n",
      " -2.92983560e+03 -1.67185248e+05 -9.13332695e+03  7.45830727e+02\n",
      " -2.80336357e+05 -4.96101855e+03 -5.03400166e+05 -5.86028202e+05\n",
      " -4.77348282e+05 -4.55891780e+05 -4.55894949e+05 -5.68098475e+05\n",
      " -5.65166434e+05 -5.65163141e+05 -5.49954538e+05]\n",
      "Gradient Descent(37/99): loss=28797706748760.39, weights = [-3.14519785e-01  2.11283446e+05 -2.14499911e+05 -3.09311524e+04\n",
      "  6.87538285e+05  8.21435405e+05  7.97091707e+05  8.21380682e+05\n",
      " -3.88706463e+05  3.29494315e+05  7.78091044e+05  5.01638876e+04\n",
      "  4.81104089e+05  8.21446328e+05  2.82038001e+05  6.19260419e+03\n",
      "  4.25837671e+03  2.42996419e+05  1.32748414e+04 -1.08402318e+03\n",
      "  4.07456041e+05  7.21061331e+03  7.31668742e+05  8.51764915e+05\n",
      "  6.93803978e+05  6.62618060e+05  6.62621056e+05  8.25704777e+05\n",
      "  8.21442424e+05  8.21438379e+05  7.99333301e+05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(38/99): loss=60835929201883.6, weights = [-3.14548623e-01 -3.07090515e+05  3.11765155e+05  4.49564242e+04\n",
      " -9.99304705e+05 -1.19391882e+06 -1.15853461e+06 -1.19383846e+06\n",
      "  5.64967372e+05 -4.78904741e+05 -1.13091984e+06 -7.29110398e+04\n",
      " -6.99262111e+05 -1.19393366e+06 -4.09928388e+05 -9.00064448e+03\n",
      " -6.18935525e+03 -3.53183522e+05 -1.92943858e+04  1.57558410e+03\n",
      " -5.92218178e+05 -1.04802869e+04 -1.06344640e+06 -1.23800042e+06\n",
      " -1.00841129e+06 -9.63083900e+05 -9.63089865e+05 -1.20012334e+06\n",
      " -1.19392897e+06 -1.19392235e+06 -1.16179373e+06]\n",
      "Gradient Descent(39/99): loss=128517534890724.86, weights = [-3.14571705e-01  4.46341889e+05 -4.53136616e+05 -6.53426335e+04\n",
      "  1.45244302e+06  1.73530469e+06  1.68387717e+06  1.73518871e+06\n",
      " -8.21153133e+05  6.96065641e+05  1.64373857e+06  1.05972630e+05\n",
      "  1.01634519e+06  1.73532729e+06  5.95812530e+05  1.30820332e+04\n",
      "  8.99593647e+03  5.13336265e+05  2.80434708e+04 -2.29003135e+03\n",
      "  8.60761743e+05  1.52326125e+04  1.54566985e+06  1.79937617e+06\n",
      "  1.46567933e+06  1.39979819e+06  1.39980525e+06  1.74432348e+06\n",
      "  1.73531949e+06  1.73531061e+06  1.68861308e+06]\n",
      "Gradient Descent(40/99): loss=271496745279878.66, weights = [-3.14590155e-01 -6.48737026e+05  6.58612477e+05  9.49719105e+04\n",
      " -2.11105812e+06 -2.52218527e+06 -2.44743598e+06 -2.52201587e+06\n",
      "  1.19350847e+06 -1.01169909e+06 -2.38909820e+06 -1.54026458e+05\n",
      " -1.47721012e+06 -2.52221708e+06 -8.65985057e+05 -1.90141119e+04\n",
      " -1.30751779e+04 -7.46109881e+05 -4.07598976e+04  3.32846058e+03\n",
      " -1.25107690e+06 -2.21398894e+04 -2.24655912e+06 -2.61530925e+06\n",
      " -2.13029616e+06 -2.03454088e+06 -2.03455275e+06 -2.53529284e+06\n",
      " -2.52220673e+06 -2.52219308e+06 -2.45432047e+06]\n",
      "Gradient Descent(41/99): loss=573544168585559.9, weights = [-3.14604937e-01  9.42909172e+05 -9.57263059e+05 -1.38037813e+05\n",
      "  3.06832482e+06  3.66587810e+06  3.55723528e+06  3.66563272e+06\n",
      " -1.73470834e+06  1.47045741e+06  3.47244223e+06  2.23870104e+05\n",
      "  2.14705639e+06  3.66592538e+06  1.25866965e+06  2.76361388e+04\n",
      "  1.90041589e+04  1.08443644e+06  5.92425954e+04 -4.83775586e+03\n",
      "  1.81838222e+06  3.21793015e+04  3.26526907e+06  3.80123040e+06\n",
      "  3.09628674e+06  2.95711101e+06  2.95712665e+06  3.68493017e+06\n",
      "  3.66590935e+06  3.66589025e+06  3.56724050e+06]\n",
      "Gradient Descent(42/99): loss=1211627465292058.2, weights = [-3.14616731e-01 -1.37047445e+06  1.39133677e+06  2.00630968e+05\n",
      " -4.45966706e+06 -5.32818381e+06 -5.17027474e+06 -5.32782634e+06\n",
      "  2.52131842e+06 -2.13724146e+06 -5.04703380e+06 -3.25384954e+05\n",
      " -3.12064619e+06 -5.32825149e+06 -1.82941702e+06 -4.01678292e+04\n",
      " -2.76216632e+04 -1.57617738e+06 -8.61063695e+04  7.03145934e+03\n",
      " -2.64293366e+06 -4.67711128e+04 -4.74591650e+06 -5.52491116e+06\n",
      " -4.50030808e+06 -4.29802254e+06 -4.29804689e+06 -5.35587428e+06\n",
      " -5.32822918e+06 -5.32820068e+06 -5.18481793e+06]\n",
      "Gradient Descent(43/99): loss=2559595572683579.0, weights = [-3.14626213e-01  1.99192090e+06 -2.02224371e+06 -2.91608332e+05\n",
      "  6.48191854e+06  7.74426631e+06  7.51475461e+06  7.74374756e+06\n",
      " -3.66461826e+06  3.10638083e+06  7.33562784e+06  4.72931687e+05\n",
      "  4.53571418e+06  7.74436571e+06  2.65897314e+06  5.83820793e+04\n",
      "  4.01467977e+04  2.29090091e+06  1.25151583e+05 -1.02198922e+04\n",
      "  3.84138119e+06  6.79796373e+04  6.89796863e+06  8.03020125e+06\n",
      "  6.54098869e+06  6.24697619e+06  6.24700997e+06  7.78451394e+06\n",
      "  7.74433230e+06  7.74429162e+06  7.53589139e+06]\n",
      "Gradient Descent(44/99): loss=5407214414805454.0, weights = [-3.14633730e-01 -2.89516416e+06  2.93923654e+06  4.23838566e+05\n",
      " -9.42116657e+06 -1.12559311e+07 -1.09223448e+07 -1.12551763e+07\n",
      "  5.32635261e+06 -4.51497996e+06 -1.06619942e+07 -6.87384376e+05\n",
      " -6.59244905e+06 -1.12560746e+07 -3.86469293e+06 -8.48556241e+04\n",
      " -5.83515041e+04 -3.32971728e+06 -1.81902011e+05  1.48541438e+04\n",
      " -5.58326846e+06 -9.88052339e+04 -1.00258761e+07 -1.16715233e+07\n",
      " -9.50702195e+06 -9.07968835e+06 -9.07973906e+06 -1.13144284e+07\n",
      " -1.12560270e+07 -1.12559671e+07 -1.09530672e+07]\n",
      "Gradient Descent(45/99): loss=1.1422885724491692e+16, weights = [-3.14639843e-01  4.20798637e+06 -4.27204394e+06 -6.16030180e+05\n",
      "  1.36932270e+07  1.63599710e+07  1.58751205e+07  1.63588747e+07\n",
      " -7.74160482e+06  6.56231301e+06  1.54967112e+07  9.99080979e+05\n",
      "  9.58181794e+06  1.63601805e+07  5.61715187e+06  1.23333706e+05\n",
      "  8.48111902e+04  4.83958950e+06  2.64386089e+05 -2.15897999e+04\n",
      "  8.11502088e+06  1.43608809e+05  1.45721440e+07  1.69640158e+07\n",
      "  1.38180141e+07  1.31969048e+07  1.31969769e+07  1.64449948e+07\n",
      "  1.63601103e+07  1.63600241e+07  1.59197730e+07]\n",
      "Gradient Descent(46/99): loss=2.4131152986558924e+16, weights = [-3.14644587e-01 -6.11611212e+06  6.20921638e+06  8.95370614e+05\n",
      " -1.99024676e+07 -2.37784566e+07 -2.30737473e+07 -2.37768624e+07\n",
      "  1.12520627e+07 -9.53801642e+06 -2.25237488e+07 -1.45211788e+06\n",
      " -1.39267259e+07 -2.37787601e+07 -8.16426795e+06 -1.79259790e+05\n",
      " -1.23269119e+05 -7.03411743e+06 -3.84272879e+05  3.13797749e+04\n",
      " -1.17948047e+07 -2.08728713e+05 -2.11799327e+07 -2.46564067e+07\n",
      " -2.00838395e+07 -1.91810859e+07 -1.91811923e+07 -2.39020339e+07\n",
      " -2.37786591e+07 -2.37785330e+07 -2.31386487e+07]\n",
      "Gradient Descent(47/99): loss=5.097770900501757e+16, weights = [-3.14648595e-01  8.88948422e+06 -9.02480736e+06 -1.30138005e+06\n",
      "  2.89273098e+07  3.45608785e+07  3.35366181e+07  3.45585623e+07\n",
      " -1.63543485e+07  1.38630626e+07  3.27372191e+07  2.11058550e+06\n",
      "  2.02418477e+07  3.45613207e+07  1.18663839e+07  2.60545763e+05\n",
      "  1.79165920e+05  1.02237626e+07  5.58522722e+05 -4.56090352e+04\n",
      "  1.71431995e+07  3.03377466e+05  3.07840458e+07  3.58369391e+07\n",
      "  2.91909259e+07  2.78788156e+07  2.78789686e+07  3.47404933e+07\n",
      "  3.45611730e+07  3.45609903e+07  3.36309482e+07]\n",
      "Gradient Descent(48/99): loss=1.0769177986844342e+17, weights = [-3.14651498e-01 -1.29204512e+07  1.31171367e+07  1.89149473e+06\n",
      " -4.20444975e+07 -5.02326281e+07 -4.87439115e+07 -5.02292607e+07\n",
      "  2.37702845e+07 -2.01493159e+07 -4.75820241e+07 -3.06763794e+06\n",
      " -2.94205827e+07 -5.02332697e+07 -1.72472357e+07 -3.78691127e+05\n",
      " -2.60409323e+05 -1.48597623e+07 -8.11786775e+05  6.62906147e+04\n",
      " -2.49168419e+07 -4.40945014e+05 -4.47431769e+07 -5.20873217e+07\n",
      " -4.24276508e+07 -4.05205595e+07 -4.05207835e+07 -5.04936889e+07\n",
      " -5.02330560e+07 -5.02327898e+07 -4.88810170e+07]\n",
      "Gradient Descent(49/99): loss=2.2750177827904403e+17, weights = [-3.14654265e-01  1.87792742e+07 -1.90651478e+07 -2.74920007e+06\n",
      "  6.11097191e+07  7.30107847e+07  7.08470063e+07  7.30058913e+07\n",
      " -3.45490008e+07  2.92860922e+07  6.91582553e+07  4.45866866e+06\n",
      "  4.27614470e+07  7.30117183e+07  2.50680545e+07  5.50409922e+05\n",
      "  3.78492820e+05  2.15979730e+07  1.17989426e+06 -9.63503150e+04\n",
      "  3.62154695e+07  6.40893045e+05  6.50321238e+07  7.57064965e+07\n",
      "  6.16666149e+07  5.88947467e+07  5.88950707e+07  7.33902253e+07\n",
      "  7.30114067e+07  7.30110206e+07  7.10462817e+07]\n",
      "Gradient Descent(50/99): loss=4.80603618803168e+17, weights = [-3.14655827e-01 -2.72948004e+07  2.77103038e+07  3.99583370e+06\n",
      " -8.88201306e+07 -1.06117776e+08 -1.02972823e+08 -1.06110663e+08\n",
      "  5.02153643e+07 -4.25659711e+07 -1.00518304e+08 -6.48046747e+06\n",
      " -6.21517714e+07 -1.06119132e+08 -3.64352491e+07 -7.99995183e+05\n",
      " -5.50121690e+05 -3.13916473e+07 -1.71492137e+06  1.40040702e+05\n",
      " -5.26374981e+07 -9.31508191e+05 -9.45211632e+07 -1.10035867e+08\n",
      " -8.96295524e+07 -8.56007708e+07 -8.56012433e+07 -1.06669274e+08\n",
      " -1.06118680e+08 -1.06118118e+08 -1.03262462e+08]\n",
      "Gradient Descent(51/99): loss=1.0152880568844936e+18, weights = [-3.14658022e-01  3.96717212e+07 -4.02756366e+07 -5.80775875e+06\n",
      "  1.29095923e+08  1.54237245e+08  1.49666204e+08  1.54226907e+08\n",
      " -7.29856927e+07  6.18676566e+07  1.46098673e+08  9.41905743e+06\n",
      "  9.03347055e+07  1.54239216e+08  5.29569384e+07  1.16275574e+06\n",
      "  7.99576253e+05  4.56262979e+07  2.49255833e+06 -2.03542632e+05\n",
      "  7.65061522e+07  1.35390378e+06  1.37382109e+08  1.59932008e+08\n",
      "  1.30272381e+08  1.24416734e+08  1.24417420e+08  1.55038823e+08\n",
      "  1.54238559e+08  1.54237743e+08  1.50087179e+08]\n",
      "Gradient Descent(52/99): loss=2.1448232974593485e+18, weights = [-3.14658410e-01 -5.76609991e+07  5.85387616e+07  8.44130629e+06\n",
      " -1.87634912e+08 -2.24176653e+08 -2.17532856e+08 -2.24161626e+08\n",
      "  1.06081306e+08 -8.99217576e+07 -2.12347617e+08 -1.36901615e+07\n",
      " -1.31297287e+08 -2.24179517e+08 -7.69704430e+07 -1.69001132e+06\n",
      " -1.16214685e+06 -6.63156986e+07 -3.62281746e+06  2.95839742e+05\n",
      " -1.11198129e+08 -1.96783608e+06 -1.99678497e+08 -2.32453725e+08\n",
      " -1.89344838e+08 -1.80833928e+08 -1.80834926e+08 -2.25341708e+08\n",
      " -2.24178562e+08 -2.24177376e+08 -2.18144725e+08]\n",
      "Gradient Descent(53/99): loss=4.5309968398927017e+18, weights = [-3.14660694e-01  8.38075769e+07 -8.50833642e+07 -1.22690461e+07\n",
      "  2.72718607e+08  3.25830323e+08  3.16173876e+08  3.25808484e+08\n",
      " -1.54184237e+08  1.30697087e+08  3.08637372e+08  1.98980120e+07\n",
      "  1.90834492e+08  3.25834488e+08  1.11872955e+08  2.45635275e+06\n",
      "  1.68912632e+06  9.63867799e+07  5.26559644e+06 -4.29989281e+05\n",
      "  1.61621302e+08  2.86015810e+06  2.90223396e+08  3.37860663e+08\n",
      "  2.75203904e+08  2.62833694e+08  2.62835142e+08  3.27523678e+08\n",
      "  3.25833099e+08  3.25831375e+08  3.17063198e+08]\n",
      "Gradient Descent(54/99): loss=9.571852556542126e+18, weights = [-3.14659639e-01 -1.21810410e+08  1.23664707e+08  1.78324866e+07\n",
      " -3.96383793e+08 -4.73579201e+08 -4.59544003e+08 -4.73547458e+08\n",
      "  2.24099608e+08 -1.89962129e+08 -4.48590046e+08 -2.89208339e+07\n",
      " -2.77369047e+08 -4.73585254e+08 -1.62602129e+08 -3.57019432e+06\n",
      " -2.45506644e+06 -1.40093695e+08 -7.65329921e+06  6.24969401e+05\n",
      " -2.34909035e+08 -4.15710659e+06 -4.21826189e+08 -4.91064739e+08\n",
      " -3.99996057e+08 -3.82016532e+08 -3.82018639e+08 -4.76040413e+08\n",
      " -4.73583235e+08 -4.73580729e+08 -4.60836591e+08]\n",
      "Gradient Descent(55/99): loss=2.022079568838404e+19, weights = [-3.14663042e-01  1.77045758e+08 -1.79740893e+08 -2.59186893e+07\n",
      "  5.76125383e+08  6.88325313e+08  6.67925807e+08  6.88279175e+08\n",
      " -3.25718343e+08  2.76101107e+08  6.52004740e+08  4.20350850e+07\n",
      "  4.03142992e+08  6.88334111e+08  2.36334622e+08  5.18911117e+06\n",
      "  3.56832473e+06  2.03619662e+08  1.11237140e+07 -9.08363911e+05\n",
      "  3.41429343e+08  6.04216083e+06  6.13104720e+08  7.13739728e+08\n",
      "  5.81375640e+08  5.55243239e+08  5.55246299e+08  6.91902570e+08\n",
      "  6.88331176e+08  6.88327533e+08  6.69804523e+08]\n",
      "Gradient Descent(56/99): loss=4.2716974154799735e+19, weights = [-3.14659601e-01 -2.57327763e+08  2.61245016e+08  3.76716073e+07\n",
      " -8.37371413e+08 -1.00044879e+09 -9.70799053e+08 -1.00038173e+09\n",
      "  4.73416443e+08 -4.01300101e+08 -9.47658525e+08 -6.10960386e+07\n",
      " -5.85949563e+08 -1.00046157e+09 -3.43501366e+08 -7.54213142e+06\n",
      " -5.18639381e+06 -2.95951695e+08 -1.61678002e+07  1.32026465e+06\n",
      " -4.96251648e+08 -8.78199936e+06 -8.91119157e+08 -1.03738745e+09\n",
      " -8.45002417e+08 -8.07020189e+08 -8.07024638e+08 -1.00564816e+09\n",
      " -1.00045731e+09 -1.00045201e+09 -9.73529680e+08]\n",
      "Gradient Descent(57/99): loss=9.024075555988452e+19, weights = [-3.14665722e-01  3.74013920e+08 -3.79707464e+08 -5.47539273e+07\n",
      "  1.21708035e+09  1.45410572e+09  1.41101121e+09  1.45400825e+09\n",
      " -6.88088751e+08  5.83271007e+08  1.37737753e+09  8.88002465e+07\n",
      "  8.51650401e+08  1.45412430e+09  4.99263239e+08  1.09621368e+07\n",
      "  7.53818186e+06  4.30152006e+08  2.34991446e+07 -1.91894317e+06\n",
      "  7.21278659e+08  1.27642271e+07  1.29520019e+09  1.50779435e+09\n",
      "  1.22817166e+09  1.17296626e+09  1.17297273e+09  1.46166277e+09\n",
      "  1.45411810e+09  1.45411041e+09  1.41498005e+09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(58/99): loss=1.9063602057833818e+20, weights = [-3.14657609e-01 -5.43611814e+08  5.51887115e+08  7.95822827e+07\n",
      " -1.76896961e+09 -2.11347494e+09 -2.05083908e+09 -2.11333327e+09\n",
      "  1.00010495e+09 -8.47757244e+08 -2.00195409e+09 -1.29067023e+08\n",
      " -1.23783419e+09 -2.11350195e+09 -7.25655866e+08 -1.59329552e+07\n",
      " -1.09563963e+07 -6.25205907e+08 -3.41549123e+07  2.78909453e+06\n",
      " -1.04834494e+09 -1.85522096e+07 -1.88251316e+09 -2.19150887e+09\n",
      " -1.78509031e+09 -1.70485183e+09 -1.70486123e+09 -2.12445876e+09\n",
      " -2.11349294e+09 -2.11348176e+09 -2.05660760e+09]\n",
      "Gradient Descent(59/99): loss=4.027237152045728e+20, weights = [-3.14669829e-01  7.90114454e+08 -8.02142220e+08 -1.15669142e+08\n",
      "  2.57111494e+09  3.07183739e+09  2.98079909e+09  3.07163149e+09\n",
      " -1.45360597e+09  1.23217567e+09  2.90974704e+09  1.87592907e+08\n",
      "  1.79913435e+09  3.07187665e+09  1.05470701e+09  2.31578084e+07\n",
      "  1.59246117e+07  9.08707669e+08  4.96425743e+07 -4.05381901e+06\n",
      "  1.52372055e+09  2.69647726e+07  2.73614521e+09  3.18525607e+09\n",
      "  2.59454563e+09  2.47792274e+09  2.47793640e+09  3.08780187e+09\n",
      "  3.07186356e+09  3.07184730e+09  2.98918337e+09]\n",
      "Gradient Descent(60/99): loss=8.507646681678703e+20, weights = [-3.14652658e-01 -1.14839456e+09  1.16587636e+09  1.68119711e+08\n",
      " -3.73699582e+09 -4.46477259e+09 -4.33245266e+09 -4.46447332e+09\n",
      "  2.11274859e+09 -1.79090995e+09 -4.22918181e+09 -2.72657553e+08\n",
      " -2.61495799e+09 -4.46482965e+09 -1.53296751e+09 -3.36587960e+07\n",
      " -2.31456815e+07 -1.32076428e+09 -7.21531698e+07  5.89203713e+06\n",
      " -2.21465685e+09 -3.91920412e+07 -3.97685963e+09 -4.62962136e+09\n",
      " -3.77105123e+09 -3.60154530e+09 -3.60156515e+09 -4.48797621e+09\n",
      " -4.46481062e+09 -4.46478699e+09 -4.34463882e+09]\n",
      "Gradient Descent(61/99): loss=1.7972632186190682e+21, weights = [-3.14678348e-01  1.66913801e+09 -1.69454699e+09 -2.44354170e+08\n",
      "  5.43154939e+09  6.48933902e+09  6.29701817e+09  6.48890405e+09\n",
      " -3.07078167e+09  2.60300421e+09  6.14691879e+09  3.96295055e+08\n",
      "  3.80071966e+09  6.48942196e+09  2.22809689e+09  4.89214925e+07\n",
      "  3.36411701e+07  1.91966937e+09  1.04871272e+08 -8.56380156e+06\n",
      "  3.21890059e+09  5.69638066e+07  5.78018026e+09  6.72893903e+09\n",
      "  5.48104734e+09  5.23467836e+09  5.23470721e+09  6.52306440e+09\n",
      "  6.48939430e+09  6.48935996e+09  6.31473018e+09]\n",
      "Gradient Descent(62/99): loss=3.7967668356011477e+21, weights = [-3.14640740e-01 -2.42601437e+09  2.46294513e+09  3.55157406e+08\n",
      " -7.89450407e+09 -9.43195205e+09 -9.15242264e+09 -9.43131983e+09\n",
      "  4.46323815e+09 -3.78334539e+09 -8.93426019e+09 -5.75996407e+08\n",
      " -5.52416901e+09 -9.43207260e+09 -3.23843506e+09 -7.11051109e+07\n",
      " -4.88958740e+07 -2.79015002e+09 -1.52425510e+08  1.24470867e+07\n",
      " -4.67852209e+09 -8.27942400e+07 -8.40122281e+09 -9.78019950e+09\n",
      " -7.96644705e+09 -7.60836120e+09 -7.60840314e+09 -9.48097031e+09\n",
      " -9.43203239e+09 -9.43198247e+09 -9.17816623e+09]\n",
      "Gradient Descent(63/99): loss=8.020771946246599e+21, weights = [-3.14694462e-01  3.52609890e+09 -3.57977604e+09 -5.16204749e+08\n",
      "  1.14742940e+10  1.37089030e+10  1.33026201e+10  1.37079841e+10\n",
      " -6.48710879e+09  5.49891633e+09  1.29855311e+10  8.37183951e+08\n",
      "  8.02912239e+09  1.37090782e+10  4.70691453e+09  1.03347967e+08\n",
      "  7.10678756e+07  4.05535311e+09  2.21543380e+08 -1.80912609e+07\n",
      "  6.80001398e+09  1.20337572e+08  1.22107861e+10  1.42150645e+10\n",
      "  1.15788598e+10  1.10583987e+10  1.10584596e+10  1.37801488e+10\n",
      "  1.37090198e+10  1.37089473e+10  1.33400372e+10]\n",
      "Gradient Descent(64/99): loss=1.6944096226944062e+22, weights = [-3.14615744e-01 -5.12502053e+09  5.20303776e+09  7.50279562e+08\n",
      " -1.66773520e+10 -1.99252521e+10 -1.93347387e+10 -1.99239165e+10\n",
      "  9.42871050e+09 -7.99241879e+09 -1.88738647e+10 -1.21680788e+09\n",
      " -1.16699555e+10 -1.99255068e+10 -6.84128106e+09 -1.50211457e+08\n",
      " -1.03293847e+08 -5.89426688e+09 -3.22002984e+08  2.62948052e+07\n",
      " -9.88350362e+09 -1.74905056e+08 -1.77478089e+10 -2.06609342e+10\n",
      " -1.68293334e+10 -1.60728674e+10 -1.60729560e+10 -2.00288045e+10\n",
      " -1.99254218e+10 -1.99253164e+10 -1.93891227e+10]\n",
      "Gradient Descent(65/99): loss=3.5794858508886368e+22, weights = [-3.14730252e-01  7.44897865e+09 -7.56237306e+09 -1.09049640e+09\n",
      "  2.42397544e+10  2.89604260e+10  2.81021423e+10  2.89584848e+10\n",
      " -1.37041916e+10  1.16166085e+10  2.74322833e+10  1.76857359e+09\n",
      "  1.69617368e+10  2.89607961e+10  9.94348341e+09  2.18325357e+08\n",
      "  1.50132797e+08  8.56704240e+09  4.68016341e+08 -3.82182747e+07\n",
      "  1.43652122e+10  2.54216354e+08  2.57956137e+10  3.00297057e+10\n",
      "  2.44606523e+10  2.33611643e+10  2.33612931e+10  2.91109346e+10\n",
      "  2.89606727e+10  2.89605194e+10  2.81811869e+10]\n",
      "Gradient Descent(66/99): loss=7.561760028450127e+22, weights = [-3.14562490e-01 -1.08267436e+10  1.09915571e+10  1.58498573e+09\n",
      " -3.52313541e+10 -4.20926304e+10 -4.08451550e+10 -4.20898090e+10\n",
      "  1.99184043e+10 -1.68841994e+10 -3.98715462e+10 -2.57053935e+09\n",
      " -2.46530944e+10 -4.20931684e+10 -1.44523900e+10 -3.17325739e+08\n",
      " -2.18211028e+08 -1.24517971e+10 -6.80239956e+08  5.55484823e+07\n",
      " -2.08791669e+10 -3.69491631e+08 -3.74927231e+10 -4.36467786e+10\n",
      " -3.55524190e+10 -3.39543643e+10 -3.39545515e+10 -4.23113876e+10\n",
      " -4.20929889e+10 -4.20927662e+10 -4.09600427e+10]\n",
      "Gradient Descent(67/99): loss=1.597442121853083e+23, weights = [-3.14803086e-01  1.57361675e+10 -1.59757163e+10 -2.30370295e+09\n",
      "  5.12071323e+10  6.11796779e+10  5.93665306e+10  6.11755771e+10\n",
      " -2.89504729e+10  2.45403975e+10  5.79514355e+10  3.73615922e+09\n",
      "  3.58321246e+10  6.11804599e+10  2.10058758e+10  4.61218183e+08\n",
      "  3.17159566e+08  1.80981072e+10  9.88697095e+08 -8.07371320e+07\n",
      "  3.03468967e+10  5.37038877e+08  5.44939269e+10  6.34385600e+10\n",
      "  5.16737853e+10  4.93510872e+10  4.93513592e+10  6.14976314e+10\n",
      "  6.11801991e+10  6.11798753e+10  5.95335145e+10]\n",
      "Gradient Descent(68/99): loss=3.37463940017879e+23, weights = [-3.14453436e-01 -2.28717866e+10  2.32199596e+10  3.34832494e+09\n",
      " -7.44271820e+10 -8.89218126e+10 -8.62864874e+10 -8.89158523e+10\n",
      "  4.20781641e+10 -3.56683249e+10 -8.42297126e+10 -5.43033343e+09\n",
      " -5.20803243e+10 -8.89229491e+10 -3.05310623e+10 -6.70359149e+08\n",
      " -4.60976659e+08 -2.63047560e+10 -1.43702518e+09  1.17347661e+08\n",
      " -4.41078011e+10 -7.80560997e+08 -7.92043849e+10 -9.22049925e+10\n",
      " -7.51054403e+10 -7.17295068e+10 -7.17299022e+10 -8.93839431e+10\n",
      " -8.89225700e+10 -8.89220994e+10 -8.65291907e+10]\n",
      "Gradient Descent(69/99): loss=7.12901639780756e+23, weights = [-3.14960250e-01  3.32430767e+10 -3.37491298e+10 -4.86663436e+09\n",
      "  1.08176443e+11  1.29243713e+11  1.25413391e+11  1.29235050e+11\n",
      " -6.11586519e+10  5.18422491e+10  1.22423965e+11  7.89273674e+09\n",
      "  7.56963259e+10  1.29245365e+11  4.43754771e+10  9.74335803e+08\n",
      "  6.70008105e+08  3.82327378e+10  2.08864918e+09 -1.70559361e+08\n",
      "  6.41086347e+10  1.13450906e+09  1.15119885e+11  1.34015662e+11\n",
      "  1.09162260e+11  1.04255498e+11  1.04256072e+11  1.29915399e+11\n",
      "  1.29244814e+11  1.29244130e+11  1.25766149e+11]\n",
      "Gradient Descent(70/99): loss=1.5060238672468793e+24, weights = [-3.14225917e-01 -4.83172637e+10  4.90527884e+10  7.07342640e+09\n",
      " -1.57229422e+11 -1.87849718e+11 -1.82282523e+11 -1.87837127e+11\n",
      "  8.88912522e+10 -7.53502947e+10 -1.77937531e+11 -1.14717253e+10\n",
      " -1.10021084e+11 -1.87852119e+11 -6.44976893e+10 -1.41615171e+09\n",
      " -9.73825577e+08 -5.55695037e+10 -3.03575430e+09  2.47900087e+08\n",
      " -9.31789148e+10 -1.64895608e+09 -1.67321391e+11 -1.94785524e+11\n",
      " -1.58662260e+11 -1.51530510e+11 -1.51531346e+11 -1.88825981e+11\n",
      " -1.87851318e+11 -1.87850324e+11 -1.82795240e+11]\n",
      "Gradient Descent(71/99): loss=3.181515881229777e+24, weights = [-3.15305594e-01  7.02268926e+10 -7.12959435e+10 -1.02808958e+10\n",
      "  2.28525642e+11  2.73030817e+11  2.64939158e+11  2.73012516e+11\n",
      " -1.29199295e+11  1.09518144e+11  2.58623914e+11  1.66736184e+10\n",
      "  1.59910522e+11  2.73034306e+11  9.37443876e+10  2.05831056e+09\n",
      "  1.41541012e+09  8.07676857e+10  4.41232750e+09 -3.60311232e+08\n",
      "  1.35431213e+11  2.39668086e+09  2.43193849e+11  2.83111687e+11\n",
      "  2.30608206e+11  2.20242540e+11  2.20243754e+11  2.74449770e+11\n",
      "  2.73033142e+11  2.73031697e+11  2.65684368e+11]\n",
      "Gradient Descent(72/99): loss=6.721037775464408e+24, weights = [-3.13762114e-01 -1.02071518e+11  1.03625333e+11  1.49428033e+10\n",
      " -3.32151377e+11 -3.96837576e+11 -3.85076727e+11 -3.96810977e+11\n",
      "  1.87785158e+11 -1.59179522e+11 -3.75897815e+11 -2.42343278e+10\n",
      " -2.32422497e+11 -3.96842648e+11 -1.36253101e+11 -2.99165854e+09\n",
      " -2.05723269e+09 -1.17392070e+11 -6.41311253e+09  5.23695595e+08\n",
      " -1.96842961e+11 -3.48346401e+09 -3.53470934e+11 -4.11489652e+11\n",
      " -3.35178288e+11 -3.20112274e+11 -3.20114039e+11 -3.98899959e+11\n",
      " -3.96840957e+11 -3.96838856e+11 -3.86159856e+11]\n",
      "Gradient Descent(73/99): loss=1.4198372871789136e+25, weights = [-3.16012317e-01  1.48356197e+11 -1.50614596e+11 -2.17186687e+10\n",
      "  4.82766558e+11  5.76784936e+11  5.59691088e+11  5.76746274e+11\n",
      " -2.72936982e+11  2.31360022e+11  5.46349967e+11  3.52234669e+10\n",
      "  3.37815274e+11  5.76792308e+11  1.98037537e+11  4.34823636e+09\n",
      "  2.99009191e+09  1.70623906e+11  9.32116039e+09 -7.61167157e+08\n",
      "  2.86102076e+11  5.06305271e+09  5.13753541e+11  5.98081046e+11\n",
      "  4.87166032e+11  4.65268282e+11  4.65270847e+11  5.79782512e+11\n",
      "  5.76789849e+11  5.76786796e+11  5.61265366e+11]\n",
      "Gradient Descent(74/99): loss=2.999444415299816e+25, weights = [-3.12715644e-01 -2.15628824e+11  2.18911301e+11  3.15670734e+10\n",
      " -7.01678708e+11 -8.38330042e+11 -8.13484931e+11 -8.38273850e+11\n",
      "  3.96701192e+11 -3.36271017e+11 -7.94094232e+11 -5.11956686e+10\n",
      " -4.90998769e+11 -8.38340757e+11 -2.87838337e+11 -6.31995904e+09\n",
      " -4.34595934e+09 -2.47993901e+11 -1.35478725e+10  1.10632101e+09\n",
      " -4.15836044e+11 -7.35891132e+09 -7.46716845e+11 -8.69282946e+11\n",
      " -7.08073138e+11 -6.76245779e+11 -6.76249507e+11 -8.42686880e+11\n",
      " -8.38337183e+11 -8.38332746e+11 -8.15773070e+11]\n",
      "Gradient Descent(75/99): loss=6.336406912054773e+25, weights = [-3.17565054e-01  3.13406453e+11 -3.18177381e+11 -4.58812711e+10\n",
      "  1.01985732e+12  1.21847367e+12  1.18236246e+12  1.21839200e+12\n",
      " -5.76586708e+11  4.88754262e+11  1.15417898e+12  7.44105201e+10\n",
      "  7.13643843e+11  1.21848925e+12  4.18359618e+11  9.18576613e+09\n",
      "  6.31664949e+09  3.60447585e+11  1.96912017e+10 -1.60798607e+09\n",
      "  6.04398323e+11  1.06958349e+10  1.08531816e+12  1.26346228e+12\n",
      "  1.02915133e+12  9.82891746e+11  9.82897164e+11  1.22480613e+12\n",
      "  1.21848405e+12  1.21847760e+12  1.18568817e+12]\n",
      "Gradient Descent(76/99): loss=1.3385829839131003e+26, weights = [-3.10548516e-01 -4.55521683e+11  4.62456005e+11  6.66862907e+10\n",
      " -1.48231512e+12 -1.77099474e+12 -1.71850877e+12 -1.77087603e+12\n",
      "  8.38041927e+11 -7.10381557e+11 -1.67754540e+12 -1.08152225e+11\n",
      " -1.03724809e+12 -1.77101737e+12 -6.08066221e+11 -1.33510833e+10\n",
      " -9.18095586e+09 -5.23893778e+11 -2.86202447e+10  2.33713286e+09\n",
      " -8.78464814e+11 -1.55458979e+10 -1.57745940e+12 -1.83638358e+12\n",
      " -1.49582353e+12 -1.42858738e+12 -1.42859525e+12 -1.78019867e+12\n",
      " -1.77100982e+12 -1.77100045e+12 -1.72334253e+12]\n",
      "Gradient Descent(77/99): loss=2.827792516627807e+26, weights = [-3.20625266e-01  6.62079552e+11 -6.72158265e+11 -9.69254179e+10\n",
      "  2.15447600e+12  2.57405837e+12  2.49777246e+12  2.57388583e+12\n",
      " -1.21805491e+12  1.03250651e+12  2.43823411e+12  1.57194223e+11\n",
      "  1.50759179e+12  2.57409127e+12  8.83795933e+11  1.94051777e+10\n",
      "  1.33440918e+10  7.61455208e+11  4.15981927e+10 -3.39691377e+09\n",
      "  1.27680770e+12  2.25952385e+10  2.29276377e+12  2.66909801e+12\n",
      "  2.17410983e+12  2.07638522e+12  2.07639667e+12  2.58743586e+12\n",
      "  2.57408030e+12  2.57406667e+12  2.50479810e+12]\n",
      "Gradient Descent(78/99): loss=5.973787664415141e+26, weights = [-3.05831013e-01 -9.62301794e+11  9.76950735e+11  1.40876581e+11\n",
      " -3.13143052e+12 -3.74127396e+12 -3.63039595e+12 -3.74102319e+12\n",
      "  1.77038609e+12 -1.50070012e+12 -3.54385973e+12 -2.28474482e+11\n",
      " -2.19121446e+12 -3.74132178e+12 -1.28455623e+12 -2.82045221e+10\n",
      " -1.93950159e+10 -1.10673968e+12 -6.04610358e+10  4.93725597e+09\n",
      " -1.85578052e+12 -3.28411269e+10 -3.33242535e+12 -3.87940964e+12\n",
      " -3.15996739e+12 -3.01792921e+12 -3.01794585e+12 -3.76071753e+12\n",
      " -3.74130583e+12 -3.74128603e+12 -3.64060739e+12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(79/99): loss=1.2619786936162457e+27, weights = [-3.27096657e-01  1.39866084e+12 -1.41995239e+12 -2.04757549e+11\n",
      "  4.55138843e+12  5.43776746e+12  5.27661144e+12  5.43740298e+12\n",
      " -2.57317373e+12  2.18119774e+12  5.15083507e+12  3.32077019e+11\n",
      "  3.18482816e+12  5.43783696e+12  1.86704265e+12  4.09939593e+10\n",
      "  2.81897524e+10  1.60859458e+12  8.78772997e+10 -7.17607161e+09\n",
      "  2.69729056e+12  4.77330485e+10  4.84352505e+12  5.63854125e+12\n",
      "  4.59286544e+12  4.38641956e+12  4.38644374e+12  5.46602778e+12\n",
      "  5.43781378e+12  5.43778500e+12  5.29145329e+12]\n",
      "Gradient Descent(80/99): loss=2.6659638952823264e+27, weights = [-2.96876532e-01 -2.03288839e+12  2.06383467e+12  2.97605562e+11\n",
      " -6.61523111e+12 -7.90354176e+12 -7.66930898e+12 -7.90301199e+12\n",
      "  3.73998817e+12 -3.17026933e+12 -7.48649889e+12 -4.82658481e+11\n",
      " -4.62899941e+12 -7.90364277e+12 -2.71365954e+12 -5.95828107e+10\n",
      " -4.09724922e+10 -2.33801731e+12 -1.27725563e+11  1.04300859e+10\n",
      " -3.92038621e+12 -6.93777629e+10 -7.03983809e+12 -8.19535710e+12\n",
      " -6.67551601e+12 -6.37545655e+12 -6.37549169e+12 -7.94461681e+12\n",
      " -7.90360908e+12 -7.90356725e+12 -7.69088092e+12]\n",
      "Gradient Descent(81/99): loss=5.631920354045552e+27, weights = [-3.42370657e-01  2.95470860e+12 -2.99968758e+12 -4.32555824e+11\n",
      "  9.61493033e+12  1.14874299e+13  1.11469834e+13  1.14866599e+13\n",
      " -5.43589863e+12  4.60783883e+12  1.08812775e+13  7.01521624e+11\n",
      "  6.72803506e+12  1.14875767e+13  3.94417777e+12  8.66008404e+10\n",
      "  5.95516092e+10  3.39819928e+12  1.85643157e+11 -1.51596441e+10\n",
      "  5.69809877e+12  1.00837347e+11  1.02320768e+13  1.19115699e+13\n",
      "  9.70255163e+12  9.26642917e+12  9.26648025e+12  1.15471305e+13\n",
      "  1.14875277e+13  1.14874669e+13  1.11783372e+13]\n",
      "Gradient Descent(82/99): loss=1.189758305821032e+28, weights = [-2.76761807e-01 -4.29453135e+12  4.35990620e+12  6.28699745e+11\n",
      " -1.39748534e+13 -1.66964443e+13 -1.62016213e+13 -1.66953252e+13\n",
      "  7.90082549e+12 -6.69727914e+12 -1.58154300e+13 -1.01962901e+12\n",
      " -9.77888562e+12 -1.66966577e+13 -5.73267869e+12 -1.25870288e+11\n",
      " -8.65554905e+10 -4.93912439e+12 -2.69823682e+11  2.20338366e+10\n",
      " -8.28192120e+12 -1.46562388e+11 -1.48718471e+13 -1.73129121e+13\n",
      " -1.41022069e+13 -1.34683232e+13 -1.34683975e+13 -1.67832165e+13\n",
      " -1.66965865e+13 -1.66964982e+13 -1.62471926e+13]\n",
      "Gradient Descent(83/99): loss=2.5133963857520592e+28, weights = [-3.73923482e-01  6.24190131e+12 -6.33692061e+12 -9.13785799e+11\n",
      "  2.03117986e+13  2.42675042e+13  2.35483020e+13  2.42658776e+13\n",
      " -1.14834819e+13  9.73418332e+12  2.29869910e+13  1.48198328e+12\n",
      "  1.42131548e+13  2.42678144e+13  8.33218150e+12  1.82946602e+11\n",
      "  1.25804374e+11  7.17878727e+12  3.92176156e+11 -3.20251554e+10\n",
      "  1.20373868e+13  2.13021605e+11  2.16155372e+13  2.51635114e+13\n",
      "  2.04969010e+13  1.95755806e+13  1.95756885e+13  2.43936235e+13\n",
      "  2.42677109e+13  2.42675825e+13  2.36145377e+13]\n",
      "Gradient Descent(84/99): loss=5.309617391199621e+28, weights = [-2.34217782e-01 -9.07231285e+12  9.21041898e+12  1.32814510e+12\n",
      " -2.95222534e+13 -3.52716872e+13 -3.42263602e+13 -3.52693229e+13\n",
      "  1.66907061e+13 -1.41481821e+13 -3.34105209e+13 -2.15399368e+12\n",
      " -2.06581586e+13 -3.52721380e+13 -1.21104378e+13 -2.65904366e+11\n",
      " -1.82850799e+11 -1.04340330e+13 -5.70009778e+11  4.65470719e+10\n",
      " -1.74957810e+13 -3.09616983e+11 -3.14171766e+13 -3.65739919e+13\n",
      " -2.97912910e+13 -2.84521947e+13 -2.84523515e+13 -3.54549957e+13\n",
      " -3.52719876e+13 -3.52718009e+13 -3.43226308e+13]\n",
      "Gradient Descent(85/99): loss=1.1216709390028555e+29, weights = [-4.33640832e-01  1.31861842e+13 -1.33869150e+13 -1.93039705e+12\n",
      "  4.29092203e+13  5.12657546e+13  4.97464206e+13  5.12623183e+13\n",
      " -2.42591640e+13  2.05637238e+13  4.85606361e+13  3.13072949e+12\n",
      "  3.00256714e+13  5.12664098e+13  1.76019573e+13  3.86479612e+11\n",
      "  2.65765121e+11  1.51653810e+13  8.28482666e+11 -6.76540011e+10\n",
      "  2.54293029e+13  4.50013865e+11  4.56634030e+13  5.31585938e+13\n",
      "  4.33002540e+13  4.13539399e+13  4.13541679e+13  5.15321850e+13\n",
      "  5.12661913e+13  5.12659200e+13  4.98863454e+13]\n",
      "Gradient Descent(86/99): loss=2.3695599940757663e+29, weights = [-1.40264832e-01 -1.91655046e+13  1.94572575e+13  2.80574221e+12\n",
      " -6.23665534e+13 -7.45123868e+13 -7.23041055e+13 -7.45073923e+13\n",
      "  3.52595651e+13 -2.98884148e+13 -7.05806229e+13 -4.55036951e+12\n",
      " -4.36409151e+13 -7.45133391e+13 -2.55836251e+13 -5.61730117e+11\n",
      " -3.86277226e+11 -2.20421750e+13 -1.20416097e+12  9.83319398e+10\n",
      " -3.69603075e+13 -6.54074194e+11 -6.63696296e+13 -7.72635404e+13\n",
      " -6.29349026e+13 -6.01060257e+13 -6.01063571e+13 -7.48996309e+13\n",
      " -7.45130215e+13 -7.45126271e+13 -7.25074798e+13]\n",
      "Gradient Descent(87/99): loss=5.005759149395351e+29, weights = [-5.62168332e-01  2.78561684e+13 -2.82802177e+13 -4.07801563e+12\n",
      "  9.06468808e+13  1.08300284e+14  1.05090650e+14  1.08293025e+14\n",
      " -5.12481358e+13  4.34414190e+13  1.02585648e+14  6.61375016e+12\n",
      "  6.34300376e+13  1.08301668e+14  3.71846076e+13  8.16448564e+11\n",
      "  5.61435958e+11  3.20372748e+13  1.75019189e+12 -1.42920895e+11\n",
      "  5.37200856e+13  9.50666379e+11  9.64651656e+13  1.12298958e+14\n",
      "  9.14729500e+13  8.73613091e+13  8.73617907e+13  1.08863125e+14\n",
      "  1.08301206e+14  1.08300633e+14  1.05386245e+14]\n",
      "Gradient Descent(88/99): loss=1.0574800690593521e+30, weights = [ 5.42880683e-02 -4.04876435e+13  4.11039793e+13  5.92720580e+12\n",
      " -1.31751020e+14 -1.57409419e+14 -1.52744365e+14 -1.57398868e+14\n",
      "  7.44867786e+13 -6.31400793e+13 -1.49103462e+14 -9.61277784e+12\n",
      " -9.21926055e+13 -1.57411431e+14 -5.40460954e+13 -1.18666996e+12\n",
      " -8.16021018e+11 -4.65646868e+13 -2.54382240e+12  2.07728865e+11\n",
      " -7.80796426e+13 -1.38174931e+12 -1.40207626e+14 -1.63221305e+14\n",
      " -1.32951673e+14 -1.26975594e+14 -1.26976294e+14 -1.58227483e+14\n",
      " -1.57410760e+14 -1.57409927e+14 -1.53173999e+14]\n",
      "Gradient Descent(89/99): loss=2.2339550567327145e+30, weights = [-8.39397132e-01  5.88469042e+13 -5.97427196e+13 -8.61491759e+12\n",
      "  1.91493971e+14  2.28787259e+14  2.22006822e+14  2.28771924e+14\n",
      " -1.08263064e+14  9.17711647e+13  2.16714938e+14  1.39717249e+13\n",
      "  1.33997659e+14  2.28790184e+14  7.85534827e+13  1.72476953e+12\n",
      "  1.18604855e+12  6.76796037e+13  3.69732739e+12 -3.01924231e+11\n",
      "  1.13485124e+14  2.00830827e+12  2.03785254e+14  2.37234565e+14\n",
      "  1.93239064e+14  1.84553113e+14  1.84554130e+14  2.29976277e+14\n",
      "  2.28789208e+14  2.28787998e+14  2.22631274e+14]\n",
      "Gradient Descent(90/99): loss=4.7192900760207983e+30, weights = [ 4.66069668e-01 -8.55312346e+13  8.68332606e+13  1.25213815e+13\n",
      " -2.78327569e+14 -3.32531626e+14 -3.22676576e+14 -3.32509337e+14\n",
      "  1.57355321e+14 -1.33385114e+14 -3.14985070e+14 -2.03072515e+13\n",
      " -1.94759356e+14 -3.32535877e+14 -1.14173829e+14 -2.50687218e+12\n",
      " -1.72386632e+12 -9.83691519e+13 -5.37389317e+12  4.38832808e+11\n",
      " -1.64945343e+14 -2.91898253e+12 -2.96192376e+14 -3.44809392e+14\n",
      " -2.80863980e+14 -2.68239354e+14 -2.68240833e+14 -3.34259808e+14\n",
      " -3.32534459e+14 -3.32532699e+14 -3.23584188e+14]\n",
      "Gradient Descent(91/99): loss=9.969627076653142e+30, weights = [-1.41139513e+00  1.24315666e+14 -1.26208100e+14 -1.81992449e+13\n",
      "  4.04536159e+14  4.83319232e+14  4.68995375e+14  4.83286836e+14\n",
      " -2.28708631e+14  1.93869051e+14  4.57816129e+14  2.95156442e+13\n",
      "  2.83073654e+14  4.83325410e+14  1.65946343e+14  3.64362196e+12\n",
      "  2.50555942e+12  1.42974981e+14  7.81070345e+12 -6.37823049e+11\n",
      "  2.39740374e+14  4.24260517e+12  4.30501824e+14  5.01164392e+14\n",
      "  4.08222715e+14  3.89873409e+14  3.89875558e+14  4.85831064e+14\n",
      "  4.83323349e+14  4.83320792e+14  4.70314547e+14]\n",
      "Gradient Descent(92/99): loss=2.106110504894884e+31, weights = [ 1.36925287e+00 -1.80687031e+14  1.83437596e+14  2.64517548e+13\n",
      " -5.87974466e+14 -7.02481995e+14 -6.81662935e+14 -7.02434909e+14\n",
      "  3.32417343e+14 -2.81779637e+14 -6.65414423e+14 -4.28996143e+13\n",
      " -4.11434372e+14 -7.02490974e+14 -2.41195280e+14 -5.29583483e+12\n",
      " -3.64171392e+12 -2.07807476e+14 -1.13524937e+13  9.27046096e+11\n",
      " -3.48451468e+14 -6.16642903e+12 -6.25714352e+14 -7.28419104e+14\n",
      " -5.93332704e+14 -5.66662842e+14 -5.66665966e+14 -7.06132825e+14\n",
      " -7.02487979e+14 -7.02484261e+14 -6.83580291e+14]\n",
      "Gradient Descent(93/99): loss=4.449215025520949e+31, weights = [-2.60848953e+00  2.62620184e+14 -2.66618003e+14 -3.84463937e+13\n",
      "  8.54593503e+14  1.02102486e+15  9.90765330e+14  1.02095643e+15\n",
      " -4.83153127e+14  4.09553579e+14  9.67148873e+14  6.23525914e+13\n",
      "  5.98000699e+14  1.02103791e+15  3.50566107e+14  7.69724929e+12\n",
      "  5.29306158e+12  3.02038489e+14  1.65003209e+13 -1.34741833e+12\n",
      "  5.06457980e+14  8.96261741e+12  9.09446670e+14  1.05872324e+15\n",
      "  8.62381453e+14  8.23618050e+14  8.23622590e+14  1.02633117e+15\n",
      "  1.02103356e+15  1.02102816e+15  9.93552117e+14]\n",
      "Gradient Descent(94/99): loss=9.399086276486244e+31, weights = [ 3.30157767e+00 -3.81706207e+14  3.87516850e+14  5.58800427e+13\n",
      " -1.24211186e+15 -1.48401209e+15 -1.44003126e+15 -1.48391262e+15\n",
      "  7.02240568e+14 -5.95267053e+14 -1.40570584e+15 -9.06265878e+13\n",
      " -8.69166165e+14 -1.48403106e+15 -5.09531510e+14 -1.11875934e+13\n",
      " -7.69321850e+12 -4.38998877e+14 -2.39824479e+13  1.95840979e+12\n",
      " -7.36113087e+14 -1.30267470e+13 -1.32183838e+15 -1.53880493e+15\n",
      " -1.25343128e+15 -1.19709048e+15 -1.19709708e+15 -1.49172456e+15\n",
      " -1.48402473e+15 -1.48401688e+15 -1.44408173e+15]\n",
      "Gradient Descent(95/99): loss=1.985582228013116e+32, weights = [-4.99997113e+00  5.54792195e+14 -5.63237694e+14 -8.12190397e+13\n",
      "  1.80535174e+15  2.15694246e+15  2.09301838e+15  2.15679788e+15\n",
      " -1.02067396e+15  8.65192939e+14  2.04312798e+15  1.31721525e+14\n",
      "  1.26329254e+15  2.15697002e+15  7.40580320e+14  1.62606459e+13\n",
      "  1.11817348e+13  6.38064422e+14  3.48573712e+13 -2.84645741e+12\n",
      "  1.06990609e+15  1.89337701e+13  1.92123052e+15  2.23658130e+15\n",
      "  1.82180399e+15  1.73991526e+15  1.73992485e+15  2.16815218e+15\n",
      "  2.15696083e+15  2.15694941e+15  2.09890554e+15]\n",
      "Gradient Descent(96/99): loss=4.194595802428796e+32, weights = [ 7.35695687e+00 -8.06364618e+14  8.18639756e+14  1.18048092e+14\n",
      " -2.62399467e+15 -3.13501541e+15 -3.04210474e+15 -3.13480527e+15\n",
      "  1.48350207e+15 -1.25751764e+15 -2.96959136e+15 -1.91451101e+14\n",
      " -1.83613688e+15 -3.13505547e+15 -1.07639900e+15 -2.36340915e+13\n",
      " -1.62521308e+13 -9.27396922e+14 -5.06635656e+13  4.13719329e+12\n",
      " -1.55505867e+15 -2.75193530e+13 -2.79241909e+15 -3.25076675e+15\n",
      " -2.64790725e+15 -2.52888580e+15 -2.52889974e+15 -3.15130822e+15\n",
      " -3.13504211e+15 -3.13502552e+15 -3.05066145e+15]\n",
      "Gradient Descent(97/99): loss=8.861196326963214e+32, weights = [-1.04427999e+01  1.17201342e+15 -1.18985476e+15 -1.71577403e+14\n",
      "  3.81385405e+15  4.55659889e+15  4.42155756e+15  4.55629346e+15\n",
      " -2.15620116e+15  1.82774333e+15  4.31616275e+15  2.78265260e+14\n",
      "  2.66873944e+15  4.55665713e+15  1.56449582e+15  3.43510514e+13\n",
      "  2.36217152e+13  1.34792823e+15  7.36371331e+13 -6.01321776e+12\n",
      "  2.26020536e+15  3.99980980e+13  4.05865110e+15  4.72483808e+15\n",
      "  3.84860987e+15  3.67561773e+15  3.67563799e+15  4.58027973e+15\n",
      "  4.55663770e+15  4.55661359e+15  4.43399435e+15]\n",
      "Gradient Descent(98/99): loss=1.8719515310514726e+33, weights = [ 1.49409569e+01 -1.70346692e+15  1.72939848e+15  2.49379765e+14\n",
      " -5.54325924e+15 -6.62280427e+15 -6.42652799e+15 -6.62236035e+15\n",
      "  3.13393797e+15 -2.65653980e+15 -6.27334154e+15 -4.04445595e+14\n",
      " -3.87888848e+15 -6.62288892e+15 -2.27392182e+15 -4.99276534e+13\n",
      " -3.43330629e+13 -1.95915091e+15 -1.07028143e+14  8.73993197e+12\n",
      " -3.28510323e+15 -5.81353726e+13 -5.89906035e+15 -6.86733210e+15\n",
      " -5.59377521e+15 -5.34233919e+15 -5.34236864e+15 -6.65722328e+15\n",
      " -6.62286069e+15 -6.62282564e+15 -6.44460428e+15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(99/99): loss=3.9545479022320705e+33, weights = [-2.30173023e+01  2.47590983e+15 -2.51360015e+15 -3.62461874e+14\n",
      "  8.05686914e+15  9.62593757e+15  9.34065913e+15  9.62529236e+15\n",
      " -4.55503289e+15  3.86115688e+15  9.11800976e+15  5.87842837e+14\n",
      "  5.63778376e+15  9.62606060e+15  3.30503946e+15  7.25675189e+13\n",
      "  4.99015080e+13  2.84753461e+15  1.55560422e+14 -1.27030841e+13\n",
      "  4.77474455e+15  8.44970565e+13  8.57400948e+15  9.98134738e+15\n",
      "  8.13029175e+15  7.76484121e+15  7.76488401e+15  9.67596399e+15\n",
      "  9.62601957e+15  9.62596862e+15  9.36693218e+15]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.2\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best value of gamma  with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-959b1ac15305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_loss_rmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from cross_validation import cross_validation\n",
    "from cost import compute_loss_rmse\n",
    "\n",
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.linspace(0.01, 0.16, 50)\n",
    "k_fold = 4\n",
    "seed = 6\n",
    "\n",
    "# prepare storage of the mean of the weights and rmse for train and test data\n",
    "ws = np.zeros((num_features, len(gammas)))\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    # prepare storage of weights and rmse for train and test data for each fold\n",
    "    ws_tmp = np.zeros((num_features, k_fold))\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross-validation\n",
    "    for i,k in enumerate(range(k_fold)):\n",
    "        tx_train, y_train, tx_test, y_test = cross_validation(y, tx, k, k_fold, seed)\n",
    "        w,_ = least_squares_GD(y_train, tx_train, w_initial, max_iters, gamma, printing=False)\n",
    "        # store weights and rmse for train and test data for each fold\n",
    "        ws_tmp[:, i] = w\n",
    "        rmse_tr.append(compute_loss_rmse(y_train, tx_train, w))\n",
    "        rmse_te.append(compute_loss_rmse(y_test, tx_test, w))\n",
    "    # store the mean of the weights and rmse for train and test data\n",
    "    ws[:, ind] = np.mean(ws_tmp, 1)\n",
    "    rmse_train.append(np.mean(rmse_tr))\n",
    "    rmse_test.append(np.mean(rmse_te))\n",
    "    \n",
    "loss = np.amin(rmse_test)\n",
    "weights = ws[np.argmin(rmse_test)]\n",
    "gamma_star = gammas[np.argmin(rmse_test)]\n",
    "\n",
    "plt.plot(gammas, rmse_train, color='b', label=\"Train error\")\n",
    "plt.plot(gammas, rmse_test, color='g', label=\"Test error\")\n",
    "plt.plot(gamma_star, loss, 'r*', markersize=15, label=\"Minimal loss\")\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title(\"Cross validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss, we = weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tx_test = build_model_data(tX_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
