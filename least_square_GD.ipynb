{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=40.00156134220384, weights = [ 0.43997152  0.03609442  0.97366353  0.16176133 -0.89362071 -0.64455298\n",
      " -0.84542194 -0.63020187  1.42441553  0.20353934 -0.81774745 -0.03698415\n",
      " -0.54963238 -1.121963    0.05639842  0.26271328  0.31728037  0.04030652\n",
      "  0.66963726  0.65691543  0.11135206  0.44979942 -1.14245422 -0.8465858\n",
      " -0.39954331 -0.56205652 -0.7963254  -1.29428443 -0.73556826 -1.02057853\n",
      " -0.82819413]\n",
      "Gradient Descent(1/99): loss=75.33485490432086, weights = [ 0.28904442  0.61011732  0.17026896 -0.13553745  0.9677504   1.56349505\n",
      "  1.3132997   1.5773667   0.22812452  0.96375492  1.23466304 -0.02452566\n",
      "  0.80909416  1.08602081  0.813515    0.14743443  0.2899907   0.54755504\n",
      "  0.53662585  0.53505434  1.12651676  0.3750701   0.79638827  1.41436348\n",
      "  1.44330003  1.19441188  0.96040874  0.92466638  1.47234326  1.18739873\n",
      "  1.30733694]\n",
      "Gradient Descent(2/99): loss=154.8942751177947, weights = [ 0.16830273 -0.21423867  0.85698422 -0.14519276 -1.69023165 -1.63218156\n",
      " -1.77403729 -1.61835573  1.63276318 -0.41143615 -1.80754037 -0.27228921\n",
      " -1.02415963 -2.10976667 -0.28556411  0.0367818   0.23767358 -0.45910783\n",
      "  0.36494517  0.44255143 -0.51483194  0.27176604 -2.05927202 -1.92219389\n",
      " -1.2749466  -1.40546414 -1.63928346 -2.28777489 -1.72347631 -2.00834558\n",
      " -1.80601197]\n",
      "Gradient Descent(3/99): loss=324.89151351254424, weights = [ 0.07170939  0.97547035 -0.4591726  -0.39976995  2.21885596  3.01892842\n",
      "  2.751026    3.03222654 -0.64627871  1.38267549  2.59462419 -0.00591814\n",
      "  1.72279569  2.541346    1.30956575  0.01617196  0.22936202  0.8985893\n",
      "  0.35395262  0.35770767  1.74959911  0.25260914  2.08371585  2.88223752\n",
      "  2.6376662   2.32778066  2.09413618  2.38753838  2.9275822   2.64273259\n",
      "  2.72030246]\n",
      "Gradient Descent(4/99): loss=684.866690270696, weights = [-0.00556529 -0.77148076  1.23164607 -0.19395426 -3.42315987 -3.73854078\n",
      " -3.79576295 -3.7249772   2.4933091  -1.38388142 -3.80449975 -0.41959131\n",
      " -2.22119838 -4.21625814 -1.0121262  -0.06952145  0.16531217 -1.09751378\n",
      "  0.18223155  0.30249964 -1.63662525  0.14558566 -3.93058173 -4.13960758\n",
      " -3.08261111 -3.13836069 -3.37191445 -4.40494138 -3.83001979 -4.11479985\n",
      " -3.85354462]\n",
      "Gradient Descent(5/99): loss=1445.7487045741464, weights = [-0.06738503  1.74489293 -1.3894688  -0.59337697  4.8097314   6.0839873\n",
      "  5.74501272  6.09672434 -2.1989543   2.51168435  5.50360986  0.18647657\n",
      "  3.54005767  5.60635224  2.35913899 -0.01614178  0.19052626  1.82053938\n",
      "  0.29500254  0.23721159  3.20694821  0.19393337  4.8253487   6.03350766\n",
      "  5.20362761  4.77296816  4.53955325  5.46876802  5.99252727  5.70772118\n",
      "  5.70677985]\n",
      "Gradient Descent(6/99): loss=3053.366991704767, weights = [-0.11684083 -1.93807075  2.29399412 -0.07333986 -7.13060922 -8.19269067\n",
      " -8.10003154 -8.17915104  4.52241001 -3.25055892 -8.01531327 -0.67668421\n",
      " -4.81658825 -8.67054778 -2.54384595 -0.13504504  0.09392597 -2.38690535\n",
      "  0.02995779  0.21328443 -3.89920856  0.03858378 -7.88348914 -8.7801589\n",
      " -6.86266003 -6.75291006 -6.98631301 -8.88197164 -8.28432975 -8.56904026\n",
      " -8.18384774]\n",
      "Gradient Descent(7/99): loss=6449.643536233651, weights = [-0.15640546  3.38783706 -3.16089962 -0.86452831 10.24418294 12.55719394\n",
      " 12.04276641 12.56919868 -5.32389044  5.04401653 11.64390613  0.59937567\n",
      "  7.33929188 12.07956511  4.57946758  0.01630332  0.18177324  3.76769829\n",
      "  0.33920003  0.15088558  6.37214224  0.19694893 10.60675964 12.72775322\n",
      " 10.65712226  9.97782694  9.74457641 11.97587808 12.46567934 12.18087452\n",
      " 12.00928018]\n",
      "Gradient Descent(8/99): loss=13624.45675175737, weights = [-1.88057169e-01 -4.38102913e+00  4.68402109e+00  2.66275928e-01\n",
      " -1.49938722e+01 -1.76026879e+01 -1.72159495e+01 -1.75887976e+01\n",
      "  8.92570283e+00 -7.07678941e+00 -1.69207997e+01 -1.23436244e+00\n",
      " -1.03233108e+01 -1.80807371e+01 -5.77702538e+00 -2.12126630e-01\n",
      "  8.29438599e-03 -5.13863317e+00 -1.68409595e-01  1.61968409e-01\n",
      " -8.60647590e+00 -8.66059979e-02 -1.62493578e+01 -1.85523956e+01\n",
      " -1.48212133e+01 -1.43564762e+01 -1.45898127e+01 -1.83406297e+01\n",
      " -1.76945079e+01 -1.79791411e+01 -1.73378981e+01]\n",
      "Gradient Descent(9/99): loss=28781.4997089824, weights = [ -0.21337853   6.88251057  -6.78871832  -1.38587684  21.70014894\n",
      "  26.23223113  25.32675853  26.24305365 -11.83545066  10.48767033\n",
      "  24.60425448   1.44949145  15.35101993  25.75470892   9.27224923\n",
      "   0.1198135    0.22072576   7.84267069   0.52401337   0.08052755\n",
      "  13.12082159   0.28331541  22.80294718  26.89541401  22.19935017\n",
      "  20.99911979  20.76601704  25.72220673  26.14073897  25.85588318\n",
      "  25.31840445]\n",
      "Gradient Descent(10/99): loss=60801.198660656846, weights = [ -0.23363563  -9.51647056   9.82583125   1.01385688 -31.62412806\n",
      " -37.48095702 -36.4916603  -37.46598817  18.29858182 -15.08396224\n",
      " -35.74443739  -2.43568972 -21.9646858  -37.95932551 -12.60501166\n",
      "  -0.35747715  -0.12233117 -10.99236958  -0.5185092    0.14519401\n",
      " -18.49684068  -0.28769743 -33.94017312 -39.17489906 -31.6168133\n",
      " -30.39874433 -30.63209903 -38.32200628 -37.57303526 -37.85754658\n",
      " -36.67989265]\n",
      "Gradient Descent(11/99): loss=128443.69945689455, weights = [-2.49841302e-01  1.42912701e+01 -1.43752889e+01 -2.47097895e+00\n",
      "  4.58867399e+01  5.51219477e+01  5.33730578e+01  5.51305897e+01\n",
      " -2.55344718e+01  2.20488628e+01  5.19741600e+01  3.22400825e+00\n",
      "  3.22715236e+01  5.46447320e+01  1.91882304e+01  3.44541741e-01\n",
      "  3.46762515e-01  1.64123231e+01  9.67410208e-01  6.98851648e-03\n",
      "  2.74245755e+01  5.15921548e-01  4.85502317e+01  5.68431038e+01\n",
      "  4.65959669e+01  4.42979110e+01  4.40649935e+01  5.47622464e+01\n",
      "  5.50306167e+01  5.47456209e+01  5.34316563e+01]\n",
      "Gradient Descent(12/99): loss=271340.3183675226, weights = [ -0.26280584 -20.33878525  20.75368249   2.60024649 -66.76692506\n",
      " -79.47335586 -77.22734807 -79.45580808  38.1454349  -31.94969206\n",
      " -75.5174545   -4.99196832 -46.55945406 -79.95232173 -27.02661463\n",
      "  -0.66578831  -0.36038703 -23.39390758  -1.21659491   0.17140464\n",
      " -39.34938741  -0.67285578 -71.32947017 -82.72517557 -67.08760526\n",
      " -64.27604664 -64.50954187 -80.53247594 -79.56587342 -79.85015253\n",
      " -77.54197699]\n",
      "Gradient Descent(13/99): loss=573213.2783718647, weights = [-2.73177474e-01  2.99687756e+01 -3.03455529e+01 -4.76229614e+00\n",
      "  9.69733635e+01  1.16153326e+02  1.12607405e+02  1.16157648e+02\n",
      " -5.44355687e+01  4.65123621e+01  1.09787819e+02  6.95741823e+00\n",
      "  6.80160698e+01  1.15676831e+02  4.01391716e+01  8.13495322e-01\n",
      "  6.45716593e-01  3.44845515e+01  1.93728971e+00 -9.76726552e-02\n",
      "  5.76776754e+01  1.03862808e+00  1.02925720e+02  1.20121465e+02\n",
      "  9.81424631e+01  9.35267885e+01  9.32941801e+01  1.16110958e+02\n",
      "  1.16062436e+02  1.15777125e+02  1.12820914e+02]\n",
      "Gradient Descent(14/99): loss=1210928.0265160538, weights = [  -0.28147478  -43.17538896   43.88837089    5.94841335 -141.01311094\n",
      " -168.18228463 -163.29596004 -168.15901313   80.10474158  -67.5465233\n",
      " -159.54358487  -10.40453811  -98.51601473 -168.66244153  -57.488957\n",
      "   -1.32556668   -0.83516126  -49.61998612   -2.66424489    0.26852533\n",
      "  -83.36933424   -1.46179866 -150.33105059 -174.71504739 -142.01449513\n",
      " -135.83546962 -136.06932843 -169.70228988 -168.27563443 -168.5594383\n",
      " -163.86426663]\n",
      "Gradient Descent(15/99): loss=2558117.5725480565, weights = [  -0.28811262   63.11301764  -64.03980037   -9.60866084  204.89077537\n",
      "  245.08493843  237.72887573  245.08040217 -115.46246686   98.21837702\n",
      "  231.91742879   14.834576    143.52858186  244.61003609   84.40280292\n",
      "    1.79427851    1.30139301   72.63869355    4.0086903    -0.28433506\n",
      "  121.61593061    2.16234297  217.78035088  253.80851645  207.04045782\n",
      "  197.52961815  197.29760847  245.7127628   244.99507221  244.70908292\n",
      "  238.28284936]\n",
      "Gradient Descent(16/99): loss=5404091.688523006, weights = [-2.93422898e-01 -9.13941706e+01  9.27994337e+01  1.30137560e+01\n",
      " -2.97863376e+02 -3.55581064e+02 -3.45130307e+02 -3.55545444e+02\n",
      "  1.68768475e+02 -1.42724667e+02 -3.37053249e+02 -2.18463010e+01\n",
      " -2.08274047e+02 -3.56063671e+02 -1.21836770e+02 -2.72987553e+00\n",
      " -1.81748174e+00 -1.05044052e+02 -5.70341754e+00  5.02140727e-01\n",
      " -1.76338243e+02 -3.11311770e+00 -3.17238461e+02 -3.69037463e+02\n",
      " -3.00296080e+02 -2.87002571e+02 -2.87237242e+02 -3.58074870e+02\n",
      " -3.55676083e+02 -3.55958892e+02 -3.46221427e+02]\n",
      "Gradient Descent(17/99): loss=11416288.474434879, weights = [-2.97671119e-01  1.33154204e+02 -1.35185574e+02 -1.98557026e+01\n",
      "  4.32866960e+02  5.17457666e+02  5.02039753e+02  5.17434666e+02\n",
      " -2.44362990e+02  2.07465694e+02  4.89918232e+02  3.14695638e+01\n",
      "  3.03052619e+02  5.16986192e+02  1.77916069e+02  3.85561135e+00\n",
      "  2.70417236e+00  1.53222308e+02  8.40090909e+00 -6.55137299e-01\n",
      "  2.56708695e+02  4.54829233e+00  4.60399953e+02  5.36233740e+02\n",
      "  4.37092980e+02  4.17241396e+02  4.17010615e+02  5.19501134e+02\n",
      "  5.17370049e+02  5.17082620e+02  5.03325929e+02]\n",
      "Gradient Descent(18/99): loss=24117215.715174243, weights = [-3.01069695e-01 -1.93235722e+02  1.96156024e+02  2.79297497e+01\n",
      " -6.29215295e+02 -7.51464784e+02 -7.29271408e+02 -7.51402830e+02\n",
      "  3.56090876e+02 -3.01526917e+02 -7.12047682e+02 -4.60215285e+01\n",
      " -4.40138900e+02 -7.51952502e+02 -2.57767704e+02 -5.70687092e+00\n",
      " -3.87760814e+00 -2.22144572e+02 -1.21095575e+01  1.01514265e+00\n",
      " -3.72718635e+02 -6.59208704e+00 -6.69848225e+02 -7.79541794e+02\n",
      " -6.34668300e+02 -6.06344984e+02 -6.06581402e+02 -7.56015911e+02\n",
      " -7.51563245e+02 -7.51843957e+02 -7.31454152e+02]\n",
      "Gradient Descent(19/99): loss=50948265.74420089, weights = [-3.03788556e-01  2.81138794e+02 -2.85455357e+02 -4.15130312e+01\n",
      "  9.14471808e+02  1.09285361e+03  1.06039280e+03  1.09279184e+03\n",
      " -5.16653029e+02  4.38264444e+02  1.03495202e+03  6.66084775e+01\n",
      "  6.40053411e+02  1.09238943e+03  3.75471238e+02  8.20031993e+00\n",
      "  5.68040425e+00  3.23443662e+02  1.76920579e+01 -1.42231453e+00\n",
      "  5.42112482e+02  9.59612100e+00  9.72927484e+02  1.13287101e+03\n",
      "  9.23086450e+02  8.81389910e+02  8.81161699e+02  1.09788753e+03\n",
      "  1.09277082e+03  1.09248035e+03  1.06323848e+03]\n",
      "Gradient Descent(20/99): loss=107629579.79068094, weights = [-3.05963645e-01 -4.08359271e+02  4.14524499e+02  5.94297950e+01\n",
      " -1.32920568e+03 -1.58777855e+03 -1.54079026e+03 -1.58766074e+03\n",
      "  7.51828516e+02 -6.36992300e+02 -1.50423279e+03 -9.70942677e+01\n",
      " -9.29957670e+02 -1.58827701e+03 -5.44919303e+02 -1.20052110e+01\n",
      " -8.21876883e+00 -4.69534570e+02 -2.56316531e+01  2.11227804e+00\n",
      " -7.87562747e+02 -1.39356650e+01 -1.41475827e+03 -1.64673731e+03\n",
      " -1.34103768e+03 -1.28096345e+03 -1.28120358e+03 -1.59667595e+03\n",
      " -1.58788420e+03 -1.58816049e+03 -1.54526651e+03]\n",
      "Gradient Descent(21/99): loss=227370378.54735214, weights = [-3.07703716e-01  5.93778451e+02 -6.02881751e+02 -8.72751598e+01\n",
      "  1.93187475e+03  2.30839420e+03  2.23991846e+03  2.30825076e+03\n",
      " -1.09186048e+03  9.25839841e+02  2.18635170e+03  1.40839291e+02\n",
      "  1.35197854e+03  2.30794552e+03  7.92818199e+02  1.73698857e+01\n",
      "  1.19770508e+01  6.83030277e+02  3.73297137e+01 -3.03186160e+00\n",
      "  1.14504869e+03  2.02643661e+01  2.05564363e+03  2.39328879e+03\n",
      "  1.94976130e+03  1.86191667e+03  1.86169387e+03  2.31974543e+03\n",
      "  2.30832170e+03  2.30802480e+03  2.24607109e+03]\n",
      "Gradient Descent(22/99): loss=480326032.9450312, weights = [-3.09095773e-01 -8.62796523e+02  8.75854264e+02  1.25964119e+02\n",
      " -2.80795417e+03 -3.35451266e+03 -3.25515552e+03 -3.35427661e+03\n",
      "  1.58784626e+03 -1.34566655e+03 -3.17774357e+03 -2.04987205e+02\n",
      " -1.96471115e+03 -3.35503374e+03 -1.15152826e+03 -2.53187454e+01\n",
      " -1.73817068e+01 -9.92161935e+02 -5.41886061e+01  4.43926394e+00\n",
      " -1.66392054e+03 -2.94455962e+01 -2.98841241e+03 -3.47870594e+03\n",
      " -2.83326164e+03 -2.70611285e+03 -2.70636082e+03 -3.37259177e+03\n",
      " -3.35463343e+03 -3.35490036e+03 -3.26446453e+03]\n",
      "Gradient Descent(23/99): loss=1014701648.9081515, weights = [-3.10209418e-01  1.25425398e+03 -1.27343487e+03 -1.83959095e+02\n",
      "  4.08116595e+03  4.87625753e+03  4.73168845e+03  4.87594179e+03\n",
      " -2.30699134e+03  1.95586018e+03  4.61871631e+03  2.97654032e+02\n",
      "  2.85594127e+03  4.87584162e+03  1.67448149e+03  3.67333366e+01\n",
      "  2.52855763e+01  1.44265836e+03  7.88227474e+01 -6.42437027e+00\n",
      "  2.41877784e+03  4.28041118e+01  4.34289930e+03  5.05596015e+03\n",
      "  4.11863966e+03  3.93330580e+03  3.93309443e+03  4.90095414e+03\n",
      "  4.87620685e+03  4.87589635e+03  4.74483929e+03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(24/99): loss=2143584494.4624636, weights = [-3.11100334e-01 -1.82279270e+03  1.85044283e+03  2.66509678e+02\n",
      " -5.93184982e+03 -7.08678478e+03 -6.87680762e+03 -7.08629874e+03\n",
      "  3.35396566e+03 -2.84275668e+03 -6.71307795e+03 -4.32913325e+02\n",
      " -4.15065400e+03 -7.08735361e+03 -2.43299929e+03 -5.34508508e+01\n",
      " -3.67329821e+01 -2.09623371e+03 -1.14508747e+02  9.36149728e+00\n",
      " -3.51524085e+03 -6.22085660e+01 -6.31280999e+03 -7.34878416e+03\n",
      " -5.98562483e+03 -5.71677942e+03 -5.71704398e+03 -7.12426060e+03\n",
      " -7.08693740e+03 -7.08718459e+03 -6.89631391e+03]\n",
      "Gradient Descent(25/99): loss=4528379834.888304, weights = [-3.11813068e-01  2.64953992e+03 -2.68998074e+03 -3.88216232e+02\n",
      "  8.62160174e+03  1.03009389e+04  9.99561027e+03  1.03002594e+04\n",
      " -4.87397925e+03  4.13180833e+03  9.75715422e+03  6.28930365e+02\n",
      "  6.03310450e+03  1.03005923e+04  3.53702557e+03  7.76328069e+01\n",
      "  5.34050097e+01  3.04738641e+03  1.66484406e+02 -1.35857936e+01\n",
      "  5.10957296e+03  9.04216243e+01  9.17477775e+03  1.06809295e+04\n",
      "  8.70045336e+03  8.30917066e+03  8.30898341e+03  1.03538279e+04\n",
      "  1.03009344e+04  1.03005952e+04  1.00235570e+04]\n",
      "Gradient Descent(26/99): loss=9566324062.770292, weights = [-3.12383254e-01 -3.85079779e+03  3.90930167e+03  5.63406512e+02\n",
      " -1.25311629e+04 -1.49713088e+04 -1.45276540e+04 -1.49702945e+04\n",
      "  7.08494839e+03 -6.00539788e+03 -1.41815643e+04 -9.14412095e+02\n",
      " -8.76851554e+03 -1.49719785e+04 -5.14013459e+03 -1.12886488e+02\n",
      " -7.76089874e+01 -4.42862101e+03 -2.41930760e+02  1.97643000e+01\n",
      " -7.42619758e+03 -1.31419903e+02 -1.33356995e+04 -1.55244240e+04\n",
      " -1.26450770e+04 -1.20768939e+04 -1.20771935e+04 -1.50497608e+04\n",
      " -1.49715287e+04 -1.49717341e+04 -1.45686905e+04]\n",
      "Gradient Descent(27/99): loss=20209116596.36975, weights = [-3.12839403e-01  5.59713064e+03 -5.68245885e+03 -8.19724047e+02\n",
      "  1.82133948e+04  2.17607252e+04  2.11157777e+04  2.17592774e+04\n",
      " -1.02968027e+04  8.72855869e+03  2.06122450e+04  1.32876205e+03\n",
      "  1.27449486e+04  2.17605250e+04  7.47170663e+03  1.64028722e+02\n",
      "  1.12811531e+02  6.43741209e+03  3.51677380e+02 -2.87107805e+01\n",
      "  1.07939563e+04  1.91015920e+02  1.93822408e+04  2.25638325e+04\n",
      "  1.83796576e+04  1.75533022e+04  1.75531659e+04  2.18731713e+04\n",
      "  2.17608183e+04  2.17604184e+04  2.11749927e+04]\n",
      "Gradient Descent(28/99): loss=42692301758.75378, weights = [-3.13204323e-01 -8.13500224e+03  8.25870832e+03  1.19060024e+03\n",
      " -2.64723885e+04 -3.16275766e+04 -3.06902814e+04 -3.16254460e+04\n",
      "  1.49667566e+04 -1.26865613e+04 -2.99589397e+04 -1.93158945e+03\n",
      " -1.85238708e+04 -3.16284592e+04 -1.08590241e+04 -2.38450727e+02\n",
      " -1.63957711e+02 -9.35585681e+03 -5.11108308e+02  4.17435961e+01\n",
      " -1.56881931e+04 -2.77629921e+02 -2.81717519e+04 -3.27956787e+04\n",
      " -2.67133488e+04 -2.55128071e+04 -2.55131808e+04 -3.17925919e+04\n",
      " -3.16279384e+04 -3.16280557e+04 -3.07767894e+04]\n",
      "Gradient Descent(29/99): loss=90188634459.98717, weights = [-3.13496258e-01  1.18240052e+04 -1.20041381e+04 -1.73130439e+03\n",
      "  3.84763158e+04  4.59698327e+04  4.46074215e+04  4.59667620e+04\n",
      " -2.17526563e+04  1.84393181e+04  4.35439187e+04  2.80717717e+03\n",
      "  2.69239008e+04  4.59699419e+04  1.57838326e+04  3.46537976e+02\n",
      "  2.38311891e+02  1.35989279e+04  7.42906953e+02 -6.06601392e+01\n",
      "  2.28023739e+04  4.03524685e+02  4.09457712e+04  4.76667885e+04\n",
      "  3.88272364e+04  3.70817779e+04  3.70817492e+04  4.62080946e+04\n",
      "  4.59701319e+04  4.59696039e+04  4.47327031e+04]\n",
      "Gradient Descent(30/99): loss=190525913354.1036, weights = [-3.13729806e-01 -1.71854901e+04  1.74469570e+04  2.51555569e+03\n",
      " -5.59236016e+04 -6.68143888e+04 -6.48342742e+04 -6.68098998e+04\n",
      "  3.16172945e+04 -2.68006981e+04 -6.32890660e+04 -4.08040193e+03\n",
      " -3.91323196e+04 -6.68157212e+04 -2.29403253e+04 -5.03712606e+02\n",
      " -3.46369601e+02 -1.97647790e+04 -1.07974907e+03  8.81776039e+01\n",
      " -3.31418760e+04 -5.86502042e+02 -5.95133205e+04 -6.92816643e+04\n",
      " -5.64329514e+04 -5.38965354e+04 -5.38970656e+04 -6.71622714e+04\n",
      " -6.68150504e+04 -6.68149815e+04 -6.50168308e+04]\n",
      "Gradient Descent(31/99): loss=402491110734.79974, weights = [-3.13916645e-01  2.49784533e+04 -2.53588461e+04 -3.65705019e+03\n",
      "  8.12822814e+04  9.71122235e+04  9.42341421e+04  9.71057246e+04\n",
      " -4.59534489e+04  3.89535585e+04  9.19876986e+04  5.93037097e+03\n",
      "  5.68773193e+04  9.71129863e+04  3.33434283e+04  7.32089984e+02\n",
      "  5.03436562e+02  2.87278185e+04  1.56939362e+03 -1.28152221e+02\n",
      "  4.81704828e+04  8.52455523e+02  8.64992997e+04  1.00697461e+05\n",
      "  8.20232955e+04  7.83362067e+04  7.83364055e+04  9.76162747e+04\n",
      "  9.71129583e+04  9.71121596e+04  9.44990023e+04]\n",
      "Gradient Descent(32/99): loss=850273284975.8186, weights = [-3.14066116e-01 -3.63048811e+04  3.68573877e+04  5.31455213e+03\n",
      " -1.18140080e+05 -1.41147474e+05 -1.36964384e+05 -1.41138003e+05\n",
      "  6.67920088e+04 -5.66171911e+04 -1.33699851e+05 -8.61982380e+03\n",
      " -8.26682205e+04 -1.41149756e+05 -4.84623925e+04 -1.06408882e+03\n",
      " -7.31718099e+02 -4.17539104e+04 -2.28101459e+03  1.86272191e+02\n",
      " -7.00132465e+04 -1.23900241e+03 -1.25723235e+05 -1.46359285e+05\n",
      " -1.19216411e+05 -1.13857925e+05 -1.13858786e+05 -1.41881669e+05\n",
      " -1.41148769e+05 -1.41148307e+05 -1.37349835e+05]\n",
      "Gradient Descent(33/99): loss=1796225158423.683, weights = [-3.14185693e-01  5.27675855e+04 -5.35710173e+04 -7.72524715e+03\n",
      "  1.71711034e+05  2.05151899e+05  1.99071915e+05  2.05138158e+05\n",
      " -9.70782682e+04  8.22904426e+04  1.94326488e+05  1.25282041e+04\n",
      "  1.20154722e+05  2.05154043e+05  7.04385521e+04  1.54657589e+03\n",
      "  1.06352084e+03  6.06880030e+04  3.31537196e+03 -2.70729808e+02\n",
      "  1.01761298e+05  1.80083419e+03  1.82732344e+05  2.12726186e+05\n",
      "  1.73276131e+05  1.65487295e+05  1.65487974e+05  2.06217437e+05\n",
      "  2.05153554e+05  2.05152184e+05  1.99631649e+05]\n",
      "Gradient Descent(34/99): loss=3794573905548.9014, weights = [-3.14281354e-01 -7.66951017e+04  7.78624517e+04  1.12275004e+04\n",
      " -2.49574062e+05 -2.98178113e+05 -2.89341188e+05 -2.98158116e+05\n",
      "  1.41099543e+05 -1.19605334e+05 -2.82444521e+05 -1.82094721e+04\n",
      " -1.74638980e+05 -2.98182402e+05 -1.02378439e+05 -2.24790143e+03\n",
      " -1.54577606e+03 -8.82065444e+04 -4.81871749e+03  3.93500684e+02\n",
      " -1.47905004e+05 -2.61742682e+03 -2.65593466e+05 -3.09187827e+05\n",
      " -2.51848156e+05 -2.40527972e+05 -2.40529531e+05 -2.99728403e+05\n",
      " -2.98180745e+05 -2.98179452e+05 -2.90155248e+05]\n",
      "Gradient Descent(35/99): loss=8016139322594.4375, weights = [-3.14357884e-01  1.11472880e+05 -1.13169980e+05 -1.63194292e+04\n",
      "  3.62744205e+05  4.33388617e+05  4.20544518e+05  4.33359578e+05\n",
      " -2.05080818e+05  1.73840775e+05  4.10519933e+05  2.64663061e+04\n",
      "  2.53829935e+05  4.33393678e+05  1.48803003e+05  3.26719668e+03\n",
      "  2.24671486e+03  1.28204748e+05  7.00380067e+03 -5.71927925e+02\n",
      "  2.14973336e+05  3.80430974e+03  3.86027227e+05  4.49389881e+05\n",
      "  3.66050196e+05  3.49596310e+05  3.49598004e+05  4.35640312e+05\n",
      "  4.33392217e+05  4.33389638e+05  4.21727194e+05]\n",
      "Gradient Descent(36/99): loss=16934309685016.008, weights = [-3.14419105e-01 -1.62020522e+05  1.64486757e+05  2.37187560e+04\n",
      " -5.27231880e+05 -6.29909555e+05 -6.11241287e+05 -6.29867323e+05\n",
      "  2.98076209e+05 -2.52669477e+05 -5.96671632e+05 -3.84678595e+04\n",
      " -3.68929679e+05 -6.29918084e+05 -2.16277531e+05 -4.74873971e+03\n",
      " -3.26549459e+03 -1.86338984e+05 -1.01796807e+04  8.31277162e+02\n",
      " -3.12453433e+05 -5.52938532e+03 -5.61073095e+05 -6.53167492e+05\n",
      " -5.32036281e+05 -5.08121849e+05 -5.08124883e+05 -6.33183868e+05\n",
      " -6.29915013e+05 -6.29911964e+05 -6.12960784e+05]\n",
      "Gradient Descent(37/99): loss=35774184176132.2, weights = [-3.14468088e-01  2.35489397e+05 -2.39074385e+05 -3.44748757e+04\n",
      "  7.66306910e+05  9.15544817e+05  8.88411336e+05  9.15483460e+05\n",
      " -4.33239100e+05  3.67243310e+05  8.67234397e+05  5.59109234e+04\n",
      "  5.36222416e+05  9.15556041e+05  3.14350018e+05  6.90205222e+03\n",
      "  4.74624469e+03  2.70835685e+05  1.47957075e+04 -1.20821644e+03\n",
      "  4.54136811e+05  8.03670572e+03  8.15493123e+05  9.49348297e+05\n",
      "  7.73290588e+05  7.38531551e+05  7.38535389e+05  9.20302299e+05\n",
      "  9.15552524e+05  9.15547394e+05  8.90910001e+05]\n",
      "Gradient Descent(38/99): loss=75573925201110.33, weights = [-3.14507265e-01 -3.42272762e+05  3.47482936e+05  5.01068634e+04\n",
      " -1.11379148e+06 -1.33070114e+06 -1.29126390e+06 -1.33061193e+06\n",
      "  6.29693639e+05 -5.33771055e+05 -1.26048484e+06 -8.12642448e+04\n",
      " -7.79374000e+05 -1.33071862e+06 -4.56892418e+05 -1.00318297e+04\n",
      " -6.89844560e+03 -3.93646403e+05 -2.15048577e+04  1.75609223e+03\n",
      " -6.60066409e+05 -1.16809758e+04 -1.18528176e+06 -1.37983377e+06\n",
      " -1.12394122e+06 -1.07342111e+06 -1.07342726e+06 -1.33761750e+06\n",
      " -1.33071257e+06 -1.33070581e+06 -1.29489615e+06]\n",
      "Gradient Descent(39/99): loss=159651947398246.94, weights = [-3.14538619e-01  4.97477624e+05 -5.05050816e+05 -7.28287499e+04\n",
      "  1.61884397e+06  1.93411274e+06  1.87679255e+06  1.93398311e+06\n",
      " -9.15229598e+05  7.75811363e+05  1.83205598e+06  1.18113471e+05\n",
      "  1.13278412e+06  1.93413699e+06  6.64072538e+05  1.45807804e+04\n",
      "  1.00265691e+04  5.72147367e+05  3.12563218e+04 -2.55239261e+03\n",
      "  9.59376079e+05  1.69777560e+04  1.72275136e+06  2.00552389e+06\n",
      "  1.63359686e+06  1.56016769e+06  1.56017606e+06  1.94416377e+06\n",
      "  1.93412913e+06  1.93411861e+06  1.88207129e+06]\n",
      "Gradient Descent(40/99): loss=337269028176376.4, weights = [-3.14563685e-01 -7.23060456e+05  7.34067284e+05  1.05852444e+05\n",
      " -2.35291439e+06 -2.81114218e+06 -2.72782995e+06 -2.81095375e+06\n",
      "  1.33024437e+06 -1.12760569e+06 -2.66280808e+06 -1.71672756e+05\n",
      " -1.64644865e+06 -2.81117859e+06 -9.65197824e+05 -2.11925008e+04\n",
      " -1.45731518e+04 -8.31588896e+05 -4.54295967e+04  3.70978925e+03\n",
      " -1.39440816e+06 -2.46763760e+04 -2.50393934e+06 -2.91493581e+06\n",
      " -2.37435632e+06 -2.26763097e+06 -2.26764371e+06 -2.82575246e+06\n",
      " -2.81116622e+06 -2.81115163e+06 -2.73550293e+06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/99): loss=712489883278977.6, weights = [-3.14583763e-01  1.05093480e+06 -1.06693317e+06 -1.53852345e+05\n",
      "  3.41985141e+06  4.08586486e+06  3.96477442e+06  4.08559100e+06\n",
      " -1.93344747e+06  1.63892235e+06  3.87026749e+06  2.49518026e+05\n",
      "  2.39303675e+06  4.08591660e+06  1.40287076e+06  3.08022986e+04\n",
      "  2.11813966e+04  1.20867639e+06  6.60298133e+04 -5.39200060e+03\n",
      "  2.02670761e+06  3.58659649e+04  3.63935875e+06  4.23672320e+06\n",
      "  3.45101696e+06  3.29589612e+06  3.29591406e+06  4.10709863e+06\n",
      "  4.08589957e+06  4.08587766e+06  3.97592614e+06]\n",
      "Gradient Descent(42/99): loss=1505154020574363.5, weights = [-3.14599789e-01 -1.52748469e+06  1.55073711e+06  2.23616475e+05\n",
      " -4.97059483e+06 -5.93861367e+06 -5.76261434e+06 -5.93821560e+06\n",
      "  2.81017662e+06 -2.38209732e+06 -5.62525359e+06 -3.62663163e+05\n",
      " -3.47816716e+06 -5.93869005e+06 -2.03900665e+06 -4.47697198e+04\n",
      " -3.07861746e+04 -1.75675418e+06 -9.59712492e+04  7.83702693e+03\n",
      " -2.94572483e+06 -5.21295080e+04 -5.28963890e+06 -6.15788011e+06\n",
      " -5.01589180e+06 -4.79043140e+06 -4.79045805e+06 -5.96947755e+06\n",
      " -5.93866435e+06 -5.93863320e+06 -5.77882345e+06]\n",
      "Gradient Descent(43/99): loss=3179678306764335.0, weights = [-3.14612662e-01  2.22012795e+06 -2.25392473e+06 -3.25016870e+05\n",
      "  7.22452802e+06  8.63149900e+06  8.37569219e+06  8.63092046e+06\n",
      " -4.08446008e+06  3.46226750e+06  8.17604407e+06  5.27113683e+05\n",
      "  5.05535428e+06  8.63160885e+06  2.96360187e+06  6.50706904e+04\n",
      "  4.47462697e+04  2.55336105e+06  1.39489749e+05 -1.13907486e+04\n",
      "  4.28147414e+06  7.57678137e+04  7.68824335e+06  8.95019170e+06\n",
      "  7.29036576e+06  6.96266906e+06  6.96270721e+06  8.67635664e+06\n",
      "  8.63157244e+06  8.63152647e+06  8.39925077e+06]\n",
      "Gradient Descent(44/99): loss=6717155850036743.0, weights = [-3.14622885e-01 -3.22685248e+06  3.27597406e+06  4.72396150e+05\n",
      " -1.05005151e+07 -1.25454812e+07 -1.21736779e+07 -1.25446403e+07\n",
      "  5.93657326e+06 -5.03224457e+06 -1.18834993e+07 -7.66135525e+05\n",
      " -7.34772170e+06 -1.25456420e+07 -4.30745663e+06 -9.45772294e+04\n",
      " -6.50366205e+04 -3.71119070e+06 -2.02741847e+05  1.65559276e+04\n",
      " -6.22292297e+06 -1.10124987e+05 -1.11745041e+07 -1.30086871e+07\n",
      " -1.05962065e+07 -1.01199151e+07 -1.01199711e+07 -1.26106813e+07\n",
      " -1.25455882e+07 -1.25455221e+07 -1.22079198e+07]\n",
      "Gradient Descent(45/99): loss=1.4190172200029438e+16, weights = [-3.14631174e-01  4.69007989e+06 -4.76147630e+06 -6.86606534e+05\n",
      "  1.52620096e+07  1.82342733e+07  1.76938745e+07  1.82330511e+07\n",
      " -8.62853200e+06  7.31413312e+06  1.72721128e+07  1.11354197e+06\n",
      "  1.06795716e+07  1.82345058e+07  6.26068827e+06  1.37463588e+05\n",
      "  9.45276975e+04  5.39404352e+06  2.94675839e+05 -2.40632647e+04\n",
      "  9.04472901e+06  1.60061541e+05  1.62416208e+07  1.89075205e+07\n",
      "  1.54010932e+07  1.47088254e+07  1.47089062e+07  1.83290369e+07\n",
      "  1.82344285e+07  1.82343317e+07  1.77436429e+07]\n",
      "Gradient Descent(46/99): loss=2.997712001358205e+16, weights = [-3.14637645e-01 -6.81681261e+06  6.92058348e+06  9.97949894e+05\n",
      " -2.21826202e+07 -2.65026664e+07 -2.57172219e+07 -2.65008899e+07\n",
      "  1.25411702e+07 -1.06307519e+07 -2.51042116e+07 -1.61848173e+06\n",
      " -1.55222596e+07 -2.65030056e+07 -9.09961817e+06 -1.99796932e+05\n",
      " -1.37391607e+05 -7.83999037e+06 -4.28297602e+05  3.49748395e+04\n",
      " -1.31460921e+07 -2.32641994e+05 -2.36064399e+07 -2.74812008e+07\n",
      " -2.23847711e+07 -2.13785926e+07 -2.13787107e+07 -2.66404025e+07\n",
      " -2.65028922e+07 -2.65027523e+07 -2.57895586e+07]\n",
      "Gradient Descent(47/99): loss=6.332747141058957e+16, weights = [-3.14643055e-01  9.90791974e+06 -1.00587464e+07 -1.45047442e+06\n",
      "  3.22414053e+07  3.85203919e+07  3.73787850e+07  3.85178099e+07\n",
      " -1.82280060e+07  1.54513027e+07  3.64878023e+07  2.35238747e+06\n",
      "  2.25608818e+07  3.85208838e+07  1.32258719e+07  2.90395527e+05\n",
      "  1.99692301e+05  1.13950615e+07  6.22510624e+05 -5.08342955e+04\n",
      "  1.91072329e+07  3.38134306e+05  3.43108606e+07  3.99426452e+07\n",
      "  3.25352230e+07  3.10727888e+07  3.10729598e+07  3.87205834e+07\n",
      "  3.85207199e+07  3.85205158e+07  3.74839223e+07]\n",
      "Gradient Descent(48/99): loss=1.3378098474576496e+17, weights = [-3.14647043e-01 -1.44006998e+07  1.46199188e+07  2.10819629e+06\n",
      " -4.68613811e+07 -5.59875950e+07 -5.43283225e+07 -5.59838423e+07\n",
      "  2.64935586e+07 -2.24577489e+07 -5.30333214e+07 -3.41908598e+06\n",
      " -3.27911907e+07 -5.59883111e+07 -1.92231881e+07 -4.22076383e+05\n",
      " -2.90243461e+05 -1.65621906e+07 -9.04790195e+05  7.38852859e+04\n",
      " -2.77714729e+07 -4.91462464e+05 -4.98692383e+07 -5.80547749e+07\n",
      " -4.72884307e+07 -4.51628512e+07 -4.51631004e+07 -5.62785657e+07\n",
      " -5.59880721e+07 -5.59877760e+07 -5.44811354e+07]\n",
      "Gradient Descent(49/99): loss=2.8261592450940624e+17, weights = [-3.14650721e-01  2.09307466e+07 -2.12493717e+07 -3.06416585e+06\n",
      "  6.81108349e+07  8.13753625e+07  7.89636872e+07  8.13699081e+07\n",
      " -3.85071528e+07  3.26412921e+07  7.70814627e+07  4.96948189e+06\n",
      "  4.76604688e+07  8.13764021e+07  2.79400093e+07  6.13468358e+05\n",
      "  4.21855352e+05  2.40723734e+07  1.31507041e+06 -1.07388817e+05\n",
      "  4.03645427e+07  7.14317805e+05  7.24826149e+07  8.43799113e+07\n",
      "  6.87315323e+07  6.56421010e+07  6.56424627e+07  8.17982732e+07\n",
      "  8.13760556e+07  8.13756247e+07  7.91857930e+07]\n",
      "Gradient Descent(50/99): loss=5.970337334419647e+17, weights = [-3.14652953e-01 -3.04218653e+07  3.08849715e+07  4.45362164e+06\n",
      " -9.89959265e+07 -1.18275299e+08 -1.14770041e+08 -1.18267371e+08\n",
      "  5.59683539e+07 -4.74425979e+07 -1.12034316e+08 -7.22291084e+06\n",
      " -6.92722714e+07 -1.18276811e+08 -4.06095017e+07 -8.91647694e+05\n",
      " -6.13147109e+05 -3.49880729e+07 -1.91139360e+06  1.56084649e+05\n",
      " -5.86679827e+07 -1.03822766e+06 -1.05350106e+08 -1.22642272e+08\n",
      " -9.98980806e+07 -9.54077367e+07 -9.54082628e+07 -1.18889982e+08\n",
      " -1.18276307e+08 -1.18275681e+08 -1.15092862e+08]\n",
      "Gradient Descent(51/99): loss=1.2612498021348943e+18, weights = [-3.14655771e-01  4.42167645e+07 -4.48898683e+07 -6.47313235e+06\n",
      "  1.43885969e+08  1.71907639e+08  1.66812910e+08  1.71896117e+08\n",
      " -8.13473951e+07  6.89556066e+07  1.62836661e+08  1.04981642e+07\n",
      "  1.00684021e+08  1.71909836e+08  5.90240201e+07  1.29596839e+06\n",
      "  8.91180766e+05  5.08535352e+07  2.77812158e+06 -2.26861764e+05\n",
      "  8.52711809e+07  1.50901555e+06  1.53121472e+08  1.78254830e+08\n",
      "  1.45197208e+08  1.38670702e+08  1.38671466e+08  1.72801050e+08\n",
      "  1.71909104e+08  1.71908194e+08  1.67282115e+08]\n",
      "Gradient Descent(52/99): loss=2.6644240924452137e+18, weights = [-3.14656520e-01 -6.42670078e+07  6.52453323e+07  9.40839569e+06\n",
      " -2.09131554e+08 -2.49859747e+08 -2.42454796e+08 -2.49842999e+08\n",
      "  1.18234651e+08 -1.00223763e+08 -2.36675503e+08 -1.52585931e+07\n",
      " -1.46339535e+08 -2.49862941e+08 -8.57886637e+07 -1.88362971e+06\n",
      " -1.29528974e+06 -7.39132443e+07 -4.03787033e+06  3.29733013e+05\n",
      " -1.23937690e+08 -2.19328383e+06 -2.22554928e+08 -2.59085093e+08\n",
      " -2.11037380e+08 -2.01551407e+08 -2.01552518e+08 -2.51158280e+08\n",
      " -2.49861876e+08 -2.49860553e+08 -2.43136764e+08]\n",
      "Gradient Descent(53/99): loss=5.62866747918272e+18, weights = [-3.14659339e-01  9.34091029e+07 -9.48310525e+07 -1.36746657e+07\n",
      "  3.03962975e+08  3.63159506e+08  3.52396755e+08  3.63135164e+08\n",
      " -1.71848558e+08  1.45670572e+08  3.43996821e+08  2.21776540e+07\n",
      "  2.12697698e+08  3.63164148e+08  1.24689828e+08  2.73776805e+06\n",
      "  1.88264331e+06  1.07429459e+08  5.86885647e+06 -4.79251573e+05\n",
      "  1.80137660e+08  3.18783590e+06  3.23473223e+08  3.76568117e+08\n",
      "  3.06733003e+08  2.92945584e+08  2.92947198e+08  3.65046862e+08\n",
      "  3.63162600e+08  3.63160678e+08  3.53387962e+08]\n",
      "Gradient Descent(54/99): loss=1.1890711272669053e+19, weights = [-3.14658327e-01 -1.35765781e+08  1.37832518e+08  1.98754890e+07\n",
      " -4.41796027e+08 -5.27835429e+08 -5.12192270e+08 -5.27800049e+08\n",
      "  2.49773877e+08 -2.11725392e+08 -4.99983359e+08 -3.22341876e+07\n",
      " -3.09146199e+08 -5.27842176e+08 -1.81230857e+08 -3.97921836e+06\n",
      " -2.73633438e+06 -1.56143715e+08 -8.53010956e+06  6.96569846e+05\n",
      " -2.61821700e+08 -4.63337101e+06 -4.70153265e+08 -5.47324221e+08\n",
      " -4.45822134e+08 -4.25782762e+08 -4.25785110e+08 -5.30578613e+08\n",
      " -5.27839925e+08 -5.27837132e+08 -5.13632945e+08]\n",
      "Gradient Descent(55/99): loss=2.511944702594314e+19, weights = [-3.14662209e-01  1.97329239e+08 -2.00333146e+08 -2.88880983e+07\n",
      "  6.42129950e+08  7.67184214e+08  7.44447611e+08  7.67132790e+08\n",
      " -3.63034696e+08  3.07732996e+08  7.26702526e+08  4.68508901e+07\n",
      "  4.49329602e+08  7.67194019e+08  2.63410611e+08  5.78360856e+06\n",
      "  3.97713458e+06  2.26947618e+08  1.23981170e+07 -1.01243183e+06\n",
      "  3.80545647e+08  6.73438898e+06  6.83345874e+08  7.95510264e+08\n",
      "  6.47981710e+08  6.18855416e+08  6.18858827e+08  7.71171303e+08\n",
      "  7.67190749e+08  7.67186688e+08  7.46541565e+08]\n",
      "Gradient Descent(56/99): loss=5.306550671527155e+19, weights = [-3.14658581e-01 -2.86808859e+08  2.91174897e+08  4.19875048e+07\n",
      " -9.33305977e+08 -1.11506652e+09 -1.08201993e+09 -1.11499178e+09\n",
      "  5.27654024e+08 -4.47275578e+08 -1.05622828e+09 -6.80955872e+07\n",
      " -6.53079650e+08 -1.11508078e+09 -3.82855055e+08 -8.40620569e+06\n",
      " -5.78057988e+06 -3.29857793e+08 -1.80200856e+07  1.47152251e+06\n",
      " -5.53105375e+08 -9.78812074e+06 -9.93211403e+08 -1.15623712e+09\n",
      " -9.41811237e+08 -8.99477525e+08 -8.99482483e+08 -1.12086157e+09\n",
      " -1.11507602e+09 -1.11507012e+09 -1.08506340e+09]\n",
      "Gradient Descent(57/99): loss=1.1210230862328234e+20, weights = [-3.14665463e-01  4.16863320e+08 -4.23209153e+08 -6.10268835e+07\n",
      "  1.35651677e+09  1.62069726e+09  1.57266558e+09  1.62058863e+09\n",
      " -7.66920550e+08  6.50094222e+08  1.53517861e+09  9.89737642e+07\n",
      "  9.49220857e+08  1.62071798e+09  5.56461993e+08  1.22180285e+07\n",
      "  8.40180364e+06  4.79432940e+08  2.61913552e+07 -2.13878944e+06\n",
      "  8.03912903e+08  1.42265776e+07  1.44358652e+09  1.68053681e+09\n",
      "  1.36887877e+09  1.30734869e+09  1.30735590e+09  1.62912010e+09\n",
      "  1.62071107e+09  1.62070249e+09  1.57708911e+09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(58/99): loss=2.3681913876934525e+20, weights = [-3.14656863e-01 -6.05891421e+08  6.15114793e+08  8.86997323e+07\n",
      " -1.97163395e+09 -2.35560799e+09 -2.28579618e+09 -2.35545010e+09\n",
      "  1.11468330e+09 -9.44881675e+08 -2.23131061e+09 -1.43853757e+08\n",
      " -1.37964831e+09 -2.35563810e+09 -8.08791590e+08 -1.77583354e+07\n",
      " -1.22116303e+07 -6.96833449e+08 -3.80679150e+07  3.10863084e+06\n",
      " -1.16844996e+09 -2.06776680e+07 -2.09818577e+09 -2.44258198e+09\n",
      " -1.98960154e+09 -1.90017044e+09 -1.90018091e+09 -2.36785019e+09\n",
      " -2.35562806e+09 -2.35561559e+09 -2.29222558e+09]\n",
      "Gradient Descent(59/99): loss=5.0028679316428174e+20, weights = [-3.14670366e-01  8.80634963e+08 -8.94040705e+08 -1.28920931e+08\n",
      "  2.86567813e+09  3.42376651e+09  3.32229829e+09  3.42353702e+09\n",
      " -1.62014027e+09  1.37334151e+09  3.24310606e+09  2.09084736e+08\n",
      "  2.00525456e+09  3.42381027e+09  1.17554091e+09  2.58109134e+07\n",
      "  1.77490359e+07  1.01281497e+09  5.53299415e+07 -4.51825015e+06\n",
      "  1.69828761e+09  3.00540275e+07  3.04961530e+09  3.55017915e+09\n",
      "  2.89179318e+09  2.76180924e+09  2.76182446e+09  3.44155997e+09\n",
      "  3.42379568e+09  3.42377755e+09  3.33164312e+09]\n",
      "Gradient Descent(60/99): loss=1.0568692915414048e+21, weights = [-3.14651437e-01 -1.27996190e+09  1.29944653e+09  1.87380568e+08\n",
      " -4.16512969e+09 -4.97628517e+09 -4.82880583e+09 -4.97595162e+09\n",
      "  2.35479842e+09 -1.99608792e+09 -4.71370362e+09 -3.03894927e+08\n",
      " -2.91454412e+09 -4.97634877e+09 -1.70859396e+09 -3.75149605e+07\n",
      " -2.57973971e+07 -1.47207938e+09 -8.04194932e+07  6.56706617e+06\n",
      " -2.46838194e+09 -4.36821293e+07 -4.43247382e+09 -5.16002006e+09\n",
      " -4.20308671e+09 -4.01416110e+09 -4.01418323e+09 -5.00214714e+09\n",
      " -4.97632756e+09 -4.97630122e+09 -4.84238812e+09]\n",
      "Gradient Descent(61/99): loss=2.2326647728165335e+21, weights = [-3.14679925e-01  1.86036502e+09 -1.88868501e+09 -2.72348929e+08\n",
      "  6.05382203e+09  7.23279874e+09  7.01844439e+09  7.23231393e+09\n",
      " -3.42258984e+09  2.90122084e+09  6.85114868e+09  4.41697123e+08\n",
      "  4.23615414e+09  7.23289118e+09  2.48336175e+09  5.45262480e+07\n",
      "  3.74953153e+07  2.13959883e+09  1.16885988e+08 -9.54492483e+06\n",
      "  3.58767820e+09  6.34899405e+07  6.44239426e+09  7.49984884e+09\n",
      "  6.10899079e+09  5.83439621e+09  5.83442837e+09  7.27038791e+09\n",
      "  7.23286035e+09  7.23282207e+09  7.03818561e+09]\n",
      "Gradient Descent(62/99): loss=4.716564316582381e+21, weights = [-3.14639355e-01 -2.70395391e+09  2.74511570e+09  3.95846484e+08\n",
      " -8.79894838e+09 -1.05125361e+10 -1.02009821e+10 -1.05118315e+10\n",
      "  4.97457494e+09 -4.21678937e+09 -9.95782554e+09 -6.41986197e+08\n",
      " -6.15705275e+09 -1.05126705e+10 -3.60945065e+09 -7.92513620e+07\n",
      " -5.44976945e+07 -3.10980725e+09 -1.69888340e+08  1.38731037e+07\n",
      " -5.21452317e+09 -9.22796717e+07 -9.36372002e+09 -1.09006810e+10\n",
      " -8.87913360e+09 -8.48002317e+09 -8.48006992e+09 -1.05671702e+10\n",
      " -1.05126257e+10 -1.05125700e+10 -1.02296750e+10]\n",
      "Gradient Descent(63/99): loss=9.963868836607652e+21, weights = [-3.14698801e-01  3.93007108e+09 -3.98989782e+09 -5.75344430e+08\n",
      "  1.27888617e+10  1.52794817e+10  1.48266523e+10  1.52784575e+10\n",
      " -7.23031299e+09  6.12890696e+09  1.44732357e+10  9.33097039e+08\n",
      "  8.94898942e+09  1.52796769e+10  5.24616843e+09  1.15188164e+08\n",
      "  7.92098608e+07  4.51996001e+09  2.46924790e+08 -2.01639101e+07\n",
      "  7.57906658e+09  1.34124205e+08  1.36097310e+10  1.58436322e+10\n",
      "  1.29054072e+10  1.23253187e+10  1.23253867e+10  1.53588898e+10\n",
      "  1.52796118e+10  1.52795309e+10  1.48683562e+10]\n",
      "Gradient Descent(64/99): loss=2.1048940612148196e+22, weights = [-3.14612203e-01 -5.71217529e+09  5.79913066e+09  8.36236333e+08\n",
      " -1.85880149e+10 -2.22080150e+10 -2.15498487e+10 -2.22065264e+10\n",
      "  1.05089232e+10 -8.90808083e+09 -2.10361740e+10 -1.35621309e+09\n",
      " -1.30069394e+10 -2.22082988e+10 -7.62506148e+09 -1.67420632e+08\n",
      " -1.15127844e+08 -6.56955136e+09 -3.58893682e+08  2.93073043e+07\n",
      " -1.10158203e+10 -1.94943286e+08 -1.97811101e+10 -2.30279815e+10\n",
      " -1.87574083e+10 -1.79142768e+10 -1.79143755e+10 -2.23234310e+10\n",
      " -2.22082042e+10 -2.22080866e+10 -2.16104633e+10]\n",
      "Gradient Descent(65/99): loss=4.446645255565039e+22, weights = [-3.14738393e-01  8.30238075e+09 -8.42876634e+09 -1.21543056e+09\n",
      "  2.70168139e+10  3.22783155e+10  3.13217014e+10  3.22761519e+10\n",
      " -1.52742304e+10  1.29474806e+10  3.05750992e+10  1.97119256e+09\n",
      "  1.89049806e+10  3.22787281e+10  1.10826717e+10  2.43338091e+08\n",
      "  1.67332959e+08  9.54853695e+09  5.21635253e+08 -4.25968019e+07\n",
      "  1.60109818e+10  2.83340987e+08  2.87509223e+10  3.34700986e+10\n",
      "  2.72630194e+10  2.60375670e+10  2.60377105e+10  3.24460673e+10\n",
      "  3.22785905e+10  3.22784196e+10  3.14098019e+10]\n",
      "Gradient Descent(66/99): loss=9.393657568413728e+22, weights = [-3.14553944e-01 -1.20671238e+10  1.22508193e+10  1.76657172e+09\n",
      " -3.92676808e+10 -4.69150283e+10 -4.55246343e+10 -4.69118837e+10\n",
      "  2.22003826e+10 -1.88185601e+10 -4.44394827e+10 -2.86503659e+09\n",
      " -2.74775088e+10 -4.69156279e+10 -1.61081472e+10 -3.53680582e+08\n",
      " -2.43210663e+08 -1.38783538e+10 -7.58172546e+08  6.19124677e+07\n",
      " -2.32712163e+10 -4.11822928e+08 -4.17881266e+10 -4.86472296e+10\n",
      " -3.96255289e+10 -3.78443910e+10 -3.78445996e+10 -4.71588478e+10\n",
      " -4.69154279e+10 -4.69151796e+10 -4.56526842e+10]\n",
      "Gradient Descent(67/99): loss=1.9844353988477005e+23, weights = [-3.14823529e-01  1.75390024e+10 -1.78059954e+10 -2.56762972e+09\n",
      "  5.70737452e+10  6.81888087e+10  6.61679357e+10  6.81842381e+10\n",
      " -3.22672222e+10  2.73519007e+10  6.45907185e+10  4.16419725e+09\n",
      "  3.99372794e+10  6.81896802e+10  2.34124418e+10  5.14058254e+08\n",
      "  3.53495372e+08  2.01715408e+10  1.10196849e+09 -8.99868884e+07\n",
      "  3.38236291e+10  5.98565447e+08  6.07370958e+10  7.07064826e+10\n",
      "  5.75938609e+10  5.50050598e+10  5.50053630e+10  6.85431889e+10\n",
      "  6.81893896e+10  6.81890287e+10  6.63540504e+10]\n",
      "Gradient Descent(68/99): loss=4.192173095005498e+23, weights = [-3.14425930e-01 -2.54921231e+10  2.58801850e+10  3.73193022e+09\n",
      " -8.29540306e+10 -9.91092577e+10 -9.61720130e+10 -9.91026146e+10\n",
      "  4.68989047e+10 -3.97547138e+10 -9.38796011e+10 -6.05246677e+09\n",
      " -5.80469756e+10 -9.91105245e+10 -3.40288938e+10 -7.47159732e+08\n",
      " -5.13789060e+08 -2.93183952e+10 -1.60165987e+09  1.30791751e+08\n",
      " -4.91610697e+10 -8.69987001e+08 -8.82785401e+10 -1.02768579e+11\n",
      " -8.37099945e+10 -7.99472927e+10 -7.99477334e+10 -9.96243329e+10\n",
      " -9.91101020e+10 -9.91095774e+10 -9.64425220e+10]\n",
      "Gradient Descent(69/99): loss=8.856078292441639e+23, weights = [-3.14996303e-01  3.70516138e+10 -3.76156436e+10 -5.42418677e+09\n",
      "  1.20569820e+11  1.44050690e+11  1.39781542e+11  1.44041035e+11\n",
      " -6.81653738e+10  5.77816250e+10  1.36449628e+11  8.79697857e+09\n",
      "  8.43685757e+10  1.44052532e+11  4.94594124e+10  1.08596187e+09\n",
      "  7.46768470e+08  4.26129220e+10  2.32793803e+09 -1.90099719e+08\n",
      "  7.14533253e+10  1.26448559e+09  1.28308747e+11  1.49369344e+11\n",
      "  1.21668578e+11  1.16199667e+11  1.16200307e+11  1.44799328e+11\n",
      "  1.44051917e+11  1.44051155e+11  1.40174714e+11]\n",
      "Gradient Descent(70/99): loss=1.8708703325083651e+24, weights = [-3.14166050e-01 -5.38527950e+10  5.46725861e+10  7.88380285e+09\n",
      " -1.75242618e+11 -2.09370970e+11 -2.03165961e+11 -2.09356936e+11\n",
      "  9.90751962e+10 -8.39829009e+10 -1.98323180e+11 -1.27859987e+10\n",
      " -1.22625796e+11 -2.09373646e+11 -7.18869524e+10 -1.57839501e+09\n",
      " -1.08539319e+09 -6.19358975e+10 -3.38354951e+09  2.76301089e+08\n",
      " -1.03854081e+11 -1.83787091e+09 -1.86490788e+11 -2.17101385e+11\n",
      " -1.76839612e+11 -1.68890804e+11 -1.68891735e+11 -2.10459080e+11\n",
      " -2.09372753e+11 -2.09371645e+11 -2.03737419e+11]\n",
      "Gradient Descent(71/99): loss=3.952263841261754e+24, weights = [-3.15360719e-01  7.82725295e+10 -7.94640576e+10 -1.14587403e+10\n",
      "  2.54706984e+11  3.04310954e+11  2.95292263e+11  3.04290556e+11\n",
      " -1.44001183e+11  1.22065235e+11  2.88253505e+11  1.85838536e+10\n",
      "  1.78230883e+11  3.04314843e+11  1.04484337e+11  2.29412363e+09\n",
      "  1.57756846e+09  9.00209425e+10  4.91783164e+09 -4.01590765e+08\n",
      "  1.50947069e+11  2.67125978e+09  2.71055675e+11  3.15546752e+11\n",
      "  2.57028141e+11  2.45474918e+11  2.45476271e+11  3.05892471e+11\n",
      "  3.04313546e+11  3.04311935e+11  2.96122850e+11]\n",
      "Gradient Descent(72/99): loss=8.349263548373177e+24, weights = [-3.13613060e-01 -1.13765476e+11  1.15497307e+11  1.66547453e+10\n",
      " -3.70204739e+11 -4.42301798e+11 -4.29193552e+11 -4.42272152e+11\n",
      "  2.09299014e+11 -1.77416134e+11 -4.18963045e+11 -2.70107657e+10\n",
      " -2.59050288e+11 -4.42307452e+11 -1.51863118e+11 -3.33440186e+09\n",
      " -2.29292228e+09 -1.30841247e+11 -7.14783925e+09  5.83693474e+08\n",
      " -2.19394535e+11 -3.88255168e+09 -3.93966799e+11 -4.58632509e+11\n",
      " -3.73578432e+11 -3.56786360e+11 -3.56788327e+11 -4.44600460e+11\n",
      " -4.42305566e+11 -4.42303225e+11 -4.30400771e+11]\n",
      "Gradient Descent(73/99): loss=1.7638043561873825e+25, weights = [-3.16145529e-01  1.65352822e+11 -1.67869957e+11 -2.42068967e+10\n",
      "  5.38075348e+11  6.42865065e+11  6.23812839e+11  6.42821975e+11\n",
      " -3.04206369e+11  2.57866089e+11  6.08943274e+11  3.92588900e+10\n",
      "  3.76517528e+11  6.42873282e+11  2.20725970e+11  4.84639781e+09\n",
      "  3.33265575e+09  1.90171660e+11  1.03890515e+10 -8.48371280e+08\n",
      "  3.18879739e+11  5.64310805e+09  5.72612395e+11  6.66600992e+11\n",
      "  5.42978853e+11  5.18572358e+11  5.18575217e+11  6.46206063e+11\n",
      "  6.42870541e+11  6.42867139e+11  6.25567476e+11]\n",
      "Gradient Descent(74/99): loss=3.726084089789888e+25, weights = [-3.12443515e-01 -2.40332625e+11  2.43991164e+11  3.51835970e+10\n",
      " -7.82067458e+11 -9.34374432e+11 -9.06682909e+11 -9.34311803e+11\n",
      "  4.42149790e+11 -3.74796351e+11 -8.85070689e+11 -5.70609682e+10\n",
      " -5.47250692e+11 -9.34386375e+11 -3.20814918e+11 -7.04401351e+09\n",
      " -4.84385992e+09 -2.76405650e+11 -1.51000025e+10  1.23306814e+09\n",
      " -4.63476850e+11 -8.20199473e+09 -8.32265448e+11 -9.68873497e+11\n",
      " -7.89194475e+11 -7.53720772e+11 -7.53724927e+11 -9.39230417e+11\n",
      " -9.34382392e+11 -9.34377447e+11 -9.09233192e+11]\n",
      "Gradient Descent(75/99): loss=7.871452746718488e+25, weights = [-3.17776996e-01  3.49312278e+11 -3.54629794e+11 -5.11377196e+10\n",
      "  1.13669863e+12  1.35806972e+12  1.31782138e+12  1.35797869e+12\n",
      " -6.42644127e+11  5.44749041e+11  1.28640903e+12  8.29354598e+10\n",
      "  7.95403395e+11  1.35808708e+12  4.66289542e+11  1.02381456e+10\n",
      "  7.04032480e+09  4.01742740e+11  2.19471503e+10 -1.79220712e+09\n",
      "  6.73642017e+11  1.19212174e+10  1.20965907e+12  1.40821251e+12\n",
      "  1.14705741e+12  1.09549804e+12  1.09550408e+12  1.36512767e+12\n",
      "  1.35808129e+12  1.35807410e+12  1.32152810e+12]\n",
      "Gradient Descent(76/99): loss=1.6628655406248584e+26, weights = [-3.10034482e-01 -5.07709128e+11  5.15437889e+11  7.43262939e+10\n",
      " -1.65213852e+12 -1.97389109e+12 -1.91539200e+12 -1.97375878e+12\n",
      "  9.34053310e+11 -7.91767360e+11 -1.86973561e+12 -1.20542828e+11\n",
      " -1.15608179e+12 -1.97391632e+12 -6.77730132e+11 -1.48806678e+10\n",
      " -1.02327842e+10 -5.83914363e+11 -3.18991609e+10  2.60488958e+09\n",
      " -9.79107298e+11 -1.73269342e+10 -1.75818312e+12 -2.04677129e+12\n",
      " -1.66719453e+12 -1.59225538e+12 -1.59226416e+12 -1.98414949e+12\n",
      " -1.97390791e+12 -1.97389746e+12 -1.92077955e+12]\n",
      "Gradient Descent(77/99): loss=3.512848130036028e+26, weights = [-3.21244257e-01  7.37931572e+11 -7.49164967e+11 -1.08029807e+11\n",
      "  2.40130639e+12  2.86895877e+12  2.78393307e+12  2.86876647e+12\n",
      " -1.35760298e+12  1.15079698e+12  2.71757363e+12  1.75203387e+11\n",
      "  1.68031104e+12  2.86899544e+12  9.85049184e+11  2.16283576e+10\n",
      "  1.48728753e+10  8.48692332e+11  4.63639447e+10 -3.78608569e+09\n",
      "  1.42308686e+12  2.51838920e+10  2.55543728e+12  2.97488675e+12\n",
      "  2.42318961e+12  2.31426904e+12  2.31428180e+12  2.88386888e+12\n",
      "  2.86898321e+12  2.86896803e+12  2.79176361e+12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(78/99): loss=7.420986052823126e+26, weights = [-3.04778144e-01 -1.07254917e+12  1.08887639e+12  1.57016293e+11\n",
      " -3.49018700e+12 -4.16989796e+12 -4.04631707e+12 -4.16961846e+12\n",
      "  1.97321271e+12 -1.67262982e+12 -3.94986671e+12 -2.54649963e+11\n",
      " -2.44225384e+12 -4.16995126e+12 -1.43172311e+12 -3.14358105e+10\n",
      " -2.16170316e+10 -1.23353478e+12 -6.73878344e+10  5.50289923e+09\n",
      " -2.06839047e+12 -3.66036141e+10 -3.71420908e+12 -4.32385935e+12\n",
      " -3.52199324e+12 -3.36368227e+12 -3.36370081e+12 -4.19156911e+12\n",
      " -4.16993348e+12 -4.16991142e+12 -4.05769840e+12]\n",
      "Gradient Descent(79/99): loss=1.567703241290743e+27, weights = [-3.28445851e-01  1.55890027e+12 -1.58263112e+12 -2.28215869e+11\n",
      "  5.07282427e+12  6.06075249e+12  5.88113341e+12  6.06034624e+12\n",
      " -2.86797278e+12  2.43108955e+12  5.74094731e+12  3.70121862e+11\n",
      "  3.54970222e+12  6.06082995e+12  2.08094287e+12  4.56904865e+10\n",
      "  3.14193487e+10  1.79288535e+12  9.79450789e+10 -7.99820775e+09\n",
      "  3.00630922e+12  5.32016484e+10  5.39842991e+12  6.28452819e+12\n",
      "  5.11905314e+12  4.88895553e+12  4.88898248e+12  6.09225049e+12\n",
      "  6.06080412e+12  6.06077204e+12  5.89767564e+12]\n",
      "Gradient Descent(80/99): loss=3.3118152160096324e+27, weights = [-2.93788826e-01 -2.26578894e+12  2.30028063e+12  3.31701137e+11\n",
      " -7.37311382e+12 -8.80902148e+12 -8.54795352e+12 -8.80843102e+12\n",
      "  4.16846487e+12 -3.53347543e+12 -8.34419954e+12 -5.37954889e+11\n",
      " -5.15932686e+12 -8.80913407e+12 -3.02455354e+12 -6.64089943e+10\n",
      " -4.56665600e+10 -2.60587536e+12 -1.42358611e+11  1.16250225e+10\n",
      " -4.36953045e+12 -7.73261182e+10 -7.84636646e+12 -9.13426903e+12\n",
      " -7.44030534e+12 -7.10586917e+12 -7.10590834e+12 -8.85480235e+12\n",
      " -8.80909652e+12 -8.80904990e+12 -8.57199688e+12]\n",
      "Gradient Descent(81/99): loss=6.996298620880894e+27, weights = [-3.43690776e-01  3.29321871e+12 -3.34335076e+12 -4.82112154e+11\n",
      "  1.07164775e+13  1.28035025e+13  1.24240523e+13  1.28026443e+13\n",
      " -6.05866955e+12  5.13574198e+12  1.21279054e+13  7.81892378e+11\n",
      "  7.49884130e+12  1.28036661e+13  4.39604771e+12  9.65223803e+10\n",
      "  6.63742182e+10  3.78751848e+12  2.06911611e+11 -1.68964288e+10\n",
      "  6.35090900e+12  1.12389912e+11  1.14043282e+13  1.32762346e+13\n",
      "  1.08141373e+13  1.03280499e+13  1.03281068e+13  1.28700428e+13\n",
      "  1.28036115e+13  1.28035438e+13  1.24589982e+13]\n",
      "Gradient Descent(82/99): loss=1.4779868803041686e+28, weights = [-2.69905426e-01 -4.78654002e+12  4.85940462e+12  7.00727563e+11\n",
      " -1.55759010e+13 -1.86092945e+13 -1.80577815e+13 -1.86080472e+13\n",
      "  8.80599402e+12 -7.46456179e+12 -1.76273457e+13 -1.13644416e+12\n",
      " -1.08992166e+13 -1.86095324e+13 -6.38945062e+12 -1.40290785e+11\n",
      " -9.64718349e+10 -5.50498171e+12 -3.00736390e+11  2.45581723e+10\n",
      " -9.23075046e+12 -1.63353502e+11 -1.65756600e+13 -1.92963887e+13\n",
      " -1.57178449e+13 -1.50113395e+13 -1.50114223e+13 -1.87060079e+13\n",
      " -1.86094531e+13 -1.86093546e+13 -1.81085737e+13]\n",
      "Gradient Descent(83/99): loss=3.1222869930561324e+28, weights = [-3.76469826e-01  6.95701299e+12 -7.06291830e+12 -1.01847488e+12\n",
      "  2.26388467e+13  2.70477429e+13  2.62461444e+13  2.70459300e+13\n",
      " -1.27991022e+13  1.08493929e+13  2.56205260e+13  1.65176865e+12\n",
      "  1.58415037e+13  2.70480886e+13  9.28676890e+12  2.03906122e+11\n",
      "  1.40217319e+11  8.00123454e+12  4.37106336e+11 -3.56941597e+10\n",
      "  1.34164659e+13  2.37426708e+11  2.40919498e+13  2.80464024e+13\n",
      "  2.28451556e+13  2.18182829e+13  2.18184032e+13  2.71883112e+13\n",
      "  2.70479733e+13  2.70478302e+13  2.63199685e+13]\n",
      "Gradient Descent(84/99): loss=6.595915157921477e+28, weights = [-2.22941326e-01 -1.01116944e+13  1.02656228e+13  1.48030580e+12\n",
      " -3.29045094e+13 -3.93126347e+13 -3.81475485e+13 -3.93099996e+13\n",
      "  1.86028990e+13 -1.57690873e+13 -3.72382414e+13 -2.40076881e+12\n",
      " -2.30248879e+13 -3.93131372e+13 -1.34978861e+13 -2.96368051e+11\n",
      " -2.03799343e+11 -1.16294218e+13 -6.35313703e+11  5.18797987e+10\n",
      " -1.95002083e+13 -3.45088662e+11 -3.50165270e+13 -4.07641396e+13\n",
      " -3.32043697e+13 -3.17118580e+13 -3.17120329e+13 -3.95169442e+13\n",
      " -3.93129696e+13 -3.93127615e+13 -3.82548485e+13]\n",
      "Gradient Descent(85/99): loss=1.3934047980616574e+29, weights = [-4.49930526e-01  1.46968769e+13 -1.49206047e+13 -2.15155554e+12\n",
      "  4.78251720e+13  5.71390836e+13  5.54456851e+13  5.71352537e+13\n",
      " -2.70384473e+13  2.29196340e+13  5.41240496e+13  3.48940566e+12\n",
      "  3.34656022e+13  5.71398139e+13  1.96185488e+13  4.30757160e+11\n",
      "  2.96212854e+11  1.69028229e+13  9.23398879e+11 -7.54048713e+10\n",
      "  2.83426447e+13  5.01570299e+11  5.08948912e+13  5.92487785e+13\n",
      "  4.82610049e+13  4.60917088e+13  4.60919629e+13  5.74360380e+13\n",
      "  5.71395704e+13  5.71392680e+13  5.56016406e+13]\n",
      "Gradient Descent(86/99): loss=2.9436050718896936e+29, weights = [-1.22774526e-01 -2.13612260e+13  2.16864040e+13  3.12718578e+12\n",
      " -6.95116601e+13 -8.30489970e+13 -8.05877211e+13 -8.30434303e+13\n",
      "  3.92991238e+13 -3.33126206e+13 -7.86667854e+13 -5.07168862e+12\n",
      " -4.86406943e+13 -8.30500584e+13 -2.85146470e+13 -6.26085471e+11\n",
      " -4.30531588e+11 -2.45674659e+13 -1.34211727e+12  1.09597469e+11\n",
      " -4.11947140e+13 -7.29009070e+11 -7.39733540e+13 -8.61153402e+13\n",
      " -7.01451231e+13 -6.69921521e+13 -6.69925214e+13 -8.34806063e+13\n",
      " -8.30497044e+13 -8.30492649e+13 -8.08143951e+13]\n",
      "Gradient Descent(87/99): loss=6.2184447988900146e+29, weights = [-6.03558926e-01  3.10475472e+13 -3.15201782e+13 -4.54521889e+12\n",
      "  1.01031961e+14  1.20707849e+14  1.17130499e+14  1.20699758e+14\n",
      " -5.71194463e+13  4.84183426e+13  1.14338509e+14  7.37146320e+12\n",
      "  7.06969838e+13  1.20709392e+14  4.14447114e+13  9.09986074e+11\n",
      "  6.25757612e+11  3.57076676e+13  1.95070494e+12 -1.59294817e+11\n",
      "  5.98745983e+13  1.05958073e+12  1.07516825e+14  1.25164636e+14\n",
      "  1.01952670e+14  9.73699731e+13  9.73705099e+13  1.21335173e+14\n",
      "  1.20708878e+14  1.20708239e+14  1.17459960e+14]\n",
      "Gradient Descent(88/99): loss=1.313663170583491e+30, weights = [ 9.13432743e-02 -4.51261639e+13  4.58131111e+13  6.60626398e+12\n",
      " -1.46845250e+14 -1.75443237e+14 -1.70243726e+14 -1.75431478e+14\n",
      "  8.30204550e+13 -7.03738061e+13 -1.66185698e+14 -1.07140785e+13\n",
      " -1.02754773e+14 -1.75445480e+14 -6.02379579e+13 -1.32262238e+12\n",
      " -9.09509546e+11 -5.18994318e+13 -2.83525878e+12  2.31527597e+11\n",
      " -8.70249402e+13 -1.54005125e+12 -1.56270698e+14 -1.81920970e+14\n",
      " -1.48183457e+14 -1.41522721e+14 -1.41523501e+14 -1.76355024e+14\n",
      " -1.75444732e+14 -1.75443803e+14 -1.70722581e+14]\n",
      "Gradient Descent(89/99): loss=2.775148741459432e+30, weights = [-9.08601126e-01  6.55887776e+13 -6.65872234e+13 -9.60189703e+12\n",
      "  2.13432731e+14  2.54998575e+14  2.47441327e+14  2.54981482e+14\n",
      " -1.20666365e+14  1.02285050e+14  2.41543172e+14  1.55724141e+13\n",
      "  1.49349278e+14  2.55001834e+14  8.75530664e+13  1.92237003e+12\n",
      "  1.32192977e+12  7.54334071e+13  4.12091660e+12 -3.36514580e+11\n",
      "  1.26486698e+14  2.23839276e+12  2.27132181e+14  2.64413657e+14\n",
      "  2.15377754e+14  2.05696684e+14  2.05697818e+14  2.56323813e+14\n",
      "  2.55000747e+14  2.54999397e+14  2.48137320e+14]\n",
      "Gradient Descent(90/99): loss=5.862576275014974e+30, weights = [ 5.41107274e-01 -9.53302337e+13  9.67814280e+13  1.39559101e+13\n",
      " -3.10214535e+14 -3.70628552e+14 -3.59644445e+14 -3.70603710e+14\n",
      "  1.75382942e+14 -1.48666556e+14 -3.51071751e+14 -2.26337786e+13\n",
      " -2.17072220e+14 -3.70633290e+14 -1.27254304e+14 -2.79407531e+12\n",
      " -1.92136335e+12 -1.09638944e+14 -5.98956035e+12  4.89108269e+11\n",
      " -1.83842524e+14 -3.25339963e+12 -3.30126048e+14 -3.84312936e+14\n",
      " -3.13041534e+14 -2.98970551e+14 -2.98972199e+14 -3.72554725e+14\n",
      " -3.70631710e+14 -3.70629748e+14 -3.60656039e+14]\n",
      "Gradient Descent(91/99): loss=1.2384849888187923e+31, weights = [-1.55090953e+00  1.38558055e+14 -1.40667298e+14 -2.02842654e+13\n",
      "  4.50882379e+14  5.38691340e+14  5.22726451e+14  5.38655232e+14\n",
      " -2.54910938e+14  2.16079915e+14  5.10266440e+14  3.28971431e+13\n",
      "  3.15504362e+14  5.38698225e+14  1.84958205e+14  4.06105833e+12\n",
      "  2.79261215e+12  1.59355099e+14  8.70554702e+12 -7.10896090e+11\n",
      "  2.67206547e+14  4.72866485e+12  4.79822835e+14  5.58580954e+14\n",
      "  4.54991291e+14  4.34539772e+14  4.34542167e+14  5.41490943e+14\n",
      "  5.38695929e+14  5.38693078e+14  5.24196756e+14]\n",
      "Gradient Descent(92/99): loss=2.61633281270281e+31, weights = [ 1.54830167e+00 -2.01387680e+14  2.04453367e+14  2.94822351e+13\n",
      " -6.55336539e+14 -7.82962775e+14 -7.59758552e+14 -7.82910294e+14\n",
      "  3.70501176e+14 -3.14062094e+14 -7.41648507e+14 -4.78144654e+13\n",
      " -4.58570896e+14 -7.82972782e+14 -2.68828137e+14 -5.90255916e+12\n",
      " -4.05893170e+12 -2.31615215e+14 -1.26531072e+13  1.03325436e+12\n",
      " -3.88372271e+14 -6.87289413e+12 -6.97400145e+14 -8.11871403e+14\n",
      " -6.61308651e+14 -6.31583321e+14 -6.31586802e+14 -7.87031866e+14\n",
      " -7.82969444e+14 -7.82965300e+14 -7.61895572e+14]\n",
      "Gradient Descent(93/99): loss=5.527073358680136e+31, weights = [-2.89804073e+00  2.92707614e+14 -2.97163447e+14 -4.28510557e+13\n",
      "  9.52501139e+14  1.13799993e+15  1.10427367e+15  1.13792365e+15\n",
      " -5.38506204e+14  4.56474627e+14  1.07795156e+15  6.94960986e+13\n",
      "  6.66511440e+14  1.13801447e+15  3.90729177e+14  8.57909484e+12\n",
      "  5.89946819e+12  3.36641928e+14  1.83907020e+13 -1.50178709e+12\n",
      "  5.64481009e+14  9.98943153e+12  1.01363863e+15  1.18001727e+15\n",
      "  9.61181326e+14  9.17976942e+14  9.17982003e+14  1.14391416e+15\n",
      "  1.13800962e+15  1.13800360e+15  1.10737973e+15]\n",
      "Gradient Descent(94/99): loss=1.1676090963623422e+32, weights = [ 3.56948567e+00 -4.25436884e+14  4.31913231e+14  6.22820138e+13\n",
      " -1.38441605e+15 -1.65402990e+15 -1.60501035e+15 -1.65391903e+15\n",
      "  7.82693688e+14 -6.63464611e+14 -1.56675240e+15 -1.01009343e+14\n",
      " -9.68743338e+14 -1.65405104e+15 -5.67906662e+14 -1.24693148e+13\n",
      " -8.57460226e+12 -4.89293364e+14 -2.67300288e+13  2.18277759e+12\n",
      " -8.20446858e+14 -1.45191735e+13 -1.47327654e+15 -1.71510015e+15\n",
      " -1.39703229e+15 -1.33423673e+15 -1.33424409e+15 -1.66262596e+15\n",
      " -1.65404399e+15 -1.65403523e+15 -1.60952487e+15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(95/99): loss=2.466605585697679e+32, weights = [-5.74121833e+00  6.18352698e+14 -6.27765765e+14 -9.05240064e+13\n",
      "  2.01218425e+15  2.40405542e+15  2.33280780e+15  2.40389428e+15\n",
      " -1.13760882e+15  9.64314913e+14  2.27720164e+15  1.46812375e+14\n",
      "  1.40802332e+15  2.40408615e+15  8.25425884e+14  1.81235683e+13\n",
      "  1.24627851e+13  7.11165117e+14  3.88508520e+13 -3.17256558e+12\n",
      "  1.19248130e+15  2.11029425e+13  2.14133884e+15  2.49281819e+15\n",
      "  2.03052137e+15  1.93925095e+15  1.93926164e+15  2.41654940e+15\n",
      "  2.40407590e+15  2.40406318e+15  2.33936943e+15]\n",
      "Gradient Descent(96/99): loss=5.2107705689772255e+32, weights = [ 7.85601047e+00 -8.98746848e+14  9.12428304e+14  1.31572427e+14\n",
      " -2.92461609e+15 -3.49418259e+15 -3.39062749e+15 -3.49394838e+15\n",
      "  1.65346144e+15 -1.40158682e+15 -3.30980653e+15 -2.13384950e+14\n",
      " -2.04649633e+15 -3.49422726e+15 -1.19971808e+15 -2.63417625e+13\n",
      " -1.81140777e+13 -1.03364538e+15 -5.64679040e+13  4.61117631e+12\n",
      " -1.73321603e+15 -3.06721441e+13 -3.11233628e+15 -3.62319515e+15\n",
      " -2.95126825e+15 -2.81861095e+15 -2.81862648e+15 -3.51234202e+15\n",
      " -3.49421236e+15 -3.49419387e+15 -3.40016452e+15]\n",
      "Gradient Descent(97/99): loss=1.1007892822410492e+33, weights = [-1.17337623e+01  1.30628669e+15 -1.32617205e+15 -1.91234396e+14\n",
      "  4.25079328e+15  5.07863167e+15  4.92811915e+15  5.07829126e+15\n",
      " -2.40322920e+15  2.03714116e+15  4.81064965e+15  3.10145087e+14\n",
      "  2.97448710e+15  5.07869658e+15  1.74373436e+15  3.82865250e+13\n",
      "  2.63279683e+13  1.50235542e+15  8.20734687e+13 -6.70213002e+12\n",
      "  2.51914878e+15  4.45805330e+13  4.52363583e+15  5.26614541e+15\n",
      "  4.28953096e+15  4.09671974e+15  4.09674232e+15  5.10502554e+15\n",
      "  5.07867493e+15  5.07864805e+15  4.94198078e+15]\n",
      "Gradient Descent(98/99): loss=2.3254469331483273e+33, weights = [ 1.65151081e+01 -1.89862686e+15  1.92752931e+15  2.77950288e+14\n",
      " -6.17833007e+15 -7.38155461e+15 -7.16279167e+15 -7.38105983e+15\n",
      "  3.49298173e+15 -2.96088980e+15 -6.99205522e+15 -4.50781440e+14\n",
      " -4.32327847e+15 -7.38164895e+15 -2.53443668e+15 -5.56476810e+13\n",
      " -3.82664757e+13 -2.18360363e+15 -1.19289964e+14  9.74123384e+12\n",
      " -3.66146543e+15 -6.47957285e+13 -6.57489401e+15 -7.65409709e+15\n",
      " -6.23463347e+15 -5.95439135e+15 -5.95442417e+15 -7.41991687e+15\n",
      " -7.38161749e+15 -7.38157842e+15 -7.18293890e+15]\n",
      "Gradient Descent(99/99): loss=4.9125691230202206e+33, weights = [-2.56828951e+01  2.75956572e+15 -2.80157408e+15 -4.03987799e+14\n",
      "  8.97991501e+15  1.07287458e+16  1.04107841e+16  1.07280267e+16\n",
      " -5.07688626e+15  4.30351543e+15  1.01626266e+16  6.55189830e+14\n",
      "  6.28368392e+15  1.07288830e+16  3.68368568e+15  8.08813128e+13\n",
      "  5.56185403e+13  3.17376618e+15  1.73382408e+14 -1.41584297e+13\n",
      "  5.32176948e+15  9.41775737e+13  9.55630223e+15  1.11248737e+16\n",
      "  9.06174939e+15  8.65443052e+15  8.65447822e+15  1.07845036e+16\n",
      "  1.07288372e+16  1.07287805e+16  1.04400672e+16]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.2\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for the best value of gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value of gamma = 0.1573469387755102 \n",
      " Loss = 0.3416184031028482 \n",
      " Weights = [-0.31466397  0.02122175 -0.22394445 -0.23741686  0.07283407 -0.64029456\n",
      "  0.32063651  0.18545041  0.27148355 -0.01327037  0.21223052 -0.1880142\n",
      "  0.10677472  0.31020756  0.12774827 -0.00066759 -0.00127198  0.21235525\n",
      " -0.00092742  0.00239408  0.06603571  0.00096355 -0.0623784  -0.08729272\n",
      "  0.29875188  0.05242474 -0.24318223 -0.02404682 -0.14133677  0.17936732\n",
      " -0.33584493]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.linspace(0.01, 0.2, 50)\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    w, l = least_squares_GD(y, tx, w_initial, max_iters, gamma, printing=False)\n",
    "    ws.append(w)\n",
    "    losses.append(l)\n",
    "\n",
    "loss = np.amin(losses)\n",
    "weights = ws[np.argmin(losses)]\n",
    "gamma_star = gammas[np.argmin(losses)]\n",
    "\n",
    "plt.plot(gammas, losses, color='k')\n",
    "plt.plot(gamma_star, loss, 'r*', markersize=15, label=\"Minimal loss\")\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss in function of gamma\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss, we = weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tx_test = build_model_data(tX_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
