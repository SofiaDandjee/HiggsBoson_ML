{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=33.82971117181637, weights = [ 4.63386366e-01  5.55980980e-03  2.98123043e-01  3.60690121e-04\n",
      " -6.77349087e-02 -5.94164634e-01  1.31422645e-01  1.82469672e-02\n",
      "  1.03843686e+00  4.49677905e-01  5.66316519e-02  1.54284632e-01\n",
      "  4.76074214e-01  9.14609195e-02  4.71673871e-01  3.86255843e-01\n",
      "  1.60658402e-01  3.81784517e-01  1.49253486e-01  3.51993322e-01\n",
      " -3.20908191e-03  3.49527600e-01 -2.46818055e-01 -3.30314968e-01\n",
      " -4.07367988e-01 -2.50274634e-01  3.95326802e-01 -1.38981592e-01\n",
      " -4.28718513e-01 -1.23698294e-01 -1.22739383e-01]\n",
      "Gradient Descent(1/99): loss=3.379092665455509, weights = [ 0.38558133  0.02714016  0.20134235 -0.09823732  0.06328457 -0.41839885\n",
      "  0.30425035  0.19391387  0.89345948  0.46230689  0.18785756  0.11082994\n",
      "  0.55762631  0.26720274  0.51123959  0.33978944  0.15235951  0.3571435\n",
      "  0.11443937  0.31977719  0.07087256  0.31498424 -0.12261861 -0.15997984\n",
      " -0.26956906 -0.11821854  0.52745839  0.03695944 -0.25299513  0.05204681\n",
      "  0.02867103]\n",
      "Gradient Descent(2/99): loss=1.4575717550104481, weights = [ 0.3155568   0.0011037   0.16744972 -0.1704331   0.02822306 -0.44705401\n",
      "  0.27856285  0.16518572  0.8543489   0.40060774  0.13400199  0.06818913\n",
      "  0.52028099  0.23852529  0.48437932  0.29868779  0.14325129  0.29112212\n",
      "  0.08285685  0.2908211   0.04703997  0.28231706 -0.17281754 -0.19914146\n",
      " -0.30133132 -0.14831269  0.49743023  0.00757227 -0.28168758  0.02337425\n",
      " -0.0139004 ]\n",
      "Gradient Descent(3/99): loss=1.1144180928151974, weights = [ 0.25253472 -0.00964849  0.1295361  -0.22934887  0.03335838 -0.43208166\n",
      "  0.29506847  0.18009183  0.80223703  0.36280363  0.12799531  0.0383234\n",
      "  0.50932092  0.25347928  0.47548718  0.26376602  0.13466489  0.2526478\n",
      "  0.05745807  0.26451812  0.04738094  0.25333886 -0.17823545 -0.19114971\n",
      " -0.29394005 -0.14116209  0.50463946  0.0221587  -0.26674668  0.03833229\n",
      " -0.01029674]\n",
      "Gradient Descent(4/99): loss=0.9257724807500044, weights = [ 0.19581485 -0.02016136  0.10047882 -0.27589508  0.03117045 -0.42906396\n",
      "  0.29984368  0.18305178  0.76218018  0.3255804   0.11563543  0.01539843\n",
      "  0.49218696  0.2564811   0.46486554  0.23371393  0.12634493  0.22184745\n",
      "  0.03645615  0.24068658  0.04363478  0.22723152 -0.18973374 -0.19406535\n",
      " -0.2949381  -0.14212696  0.50372653  0.02482038 -0.26375615  0.04133784\n",
      " -0.01552502]\n",
      "Gradient Descent(5/99): loss=0.7939734108520141, weights = [ 0.14476696 -0.02768582  0.07560257 -0.31295245  0.03188107 -0.42490078\n",
      "  0.30561156  0.1871633   0.72694014  0.2933719   0.1081431  -0.00165217\n",
      "  0.47653913  0.2606305   0.4565243   0.20787732  0.11838183  0.19995226\n",
      "  0.01931537  0.21907713  0.04180032  0.20381132 -0.19668408 -0.19463151\n",
      " -0.2937094  -0.14103481  0.50486478  0.02870461 -0.2596166   0.04549051\n",
      " -0.01753335]\n",
      "Gradient Descent(6/99): loss=0.6982551019814993, weights = [ 0.09882386 -0.03340137  0.0546219  -0.3423231   0.03276881 -0.42218459\n",
      "  0.30986714  0.18983289  0.69687746  0.26447212  0.10215369 -0.01441266\n",
      "  0.46084127  0.26333456  0.44918494  0.18560038  0.11077621  0.18415869\n",
      "  0.00536528  0.19948242  0.04023182  0.18278569 -0.20219597 -0.1957875\n",
      " -0.29278009 -0.14027367  0.50566686  0.03118945 -0.2569212   0.04819737\n",
      " -0.01937017]\n",
      "Gradient Descent(7/99): loss=0.6272865104282209, weights = [ 0.05747508 -0.03750558  0.0365694  -0.36555327  0.03415708 -0.42004128\n",
      "  0.31346859  0.19193366  0.67085699  0.2386869   0.09779986 -0.02391762\n",
      "  0.44558372  0.26546706  0.44283821  0.16635196  0.10353981  0.17315759\n",
      " -0.00590846  0.18171017  0.03910435  0.16391882 -0.20610729 -0.19682019\n",
      " -0.29166368 -0.13936105  0.50661587  0.03314125 -0.25479636  0.05033224\n",
      " -0.02058205]\n",
      "Gradient Descent(8/99): loss=0.5736982456619693, weights = [ 0.02026117 -0.04034066  0.02087162 -0.38385046  0.03576075 -0.41845195\n",
      "  0.31644346  0.19348377  0.64826904  0.21557465  0.09456781 -0.03099466\n",
      "  0.4307533   0.26704662  0.43724406  0.14967764  0.09667438  0.16574881\n",
      " -0.01494979  0.16558723  0.03824206  0.14699137 -0.20887659 -0.19785909\n",
      " -0.29053213 -0.13844829  0.50756099  0.03456955 -0.25322363  0.05191373\n",
      " -0.02145045]\n",
      "Gradient Descent(9/99): loss=0.5325524653589196, weights = [-0.01323135 -0.04214972  0.00704864 -0.39818424  0.03749314 -0.41725631\n",
      "  0.3189557   0.1946429   0.6285376   0.19482629  0.09220295 -0.03626411\n",
      "  0.4164179   0.26823334  0.43226108  0.1351943   0.09017802  0.16105344\n",
      " -0.02212943  0.15095669  0.03759203  0.13180699 -0.21072819 -0.19885333\n",
      " -0.28938987 -0.13753043  0.50850767  0.03562819 -0.25204306  0.05310198\n",
      " -0.02204573]\n",
      "Gradient Descent(10/99): loss=0.5004808567649517, weights = [-0.04337461 -0.04314237 -0.00526099 -0.4093281   0.03926716 -0.41636592\n",
      "  0.3210988   0.19549903  0.61120437  0.1761644   0.09047861 -0.04019856\n",
      "  0.40260228  0.26911551  0.42776312  0.12257758  0.08404497  0.15838692\n",
      " -0.02775959  0.13767675  0.03710356  0.11818839 -0.21186716 -0.19980071\n",
      " -0.28826579 -0.13662862  0.50943516  0.03640046 -0.25116646  0.05398528\n",
      " -0.02245251]\n",
      "Gradient Descent(11/99): loss=0.47514478618948985, weights = [-0.07050355 -0.04348876 -0.01633879 -0.41790132  0.04102846 -0.41570618\n",
      "  0.32295205  0.19612637  0.59588715  0.15935452  0.08922851 -0.04315514\n",
      "  0.38932259  0.2697676   0.42365123  0.11155305  0.07826673  0.15722692\n",
      " -0.03210246  0.12561922  0.03674606  0.10597602 -0.21244685 -0.20069369\n",
      " -0.28717335 -0.13575125  0.51033541  0.03695698 -0.25051946  0.05463813\n",
      " -0.02272689]\n",
      "Gradient Descent(12/99): loss=0.45489108317292276, weights = [-0.0949196  -0.04332863 -0.02640277 -0.42439998  0.04273917 -0.41522221\n",
      "  0.32457496  0.19657952  0.58227078  0.14419362  0.088324   -0.04540324\n",
      "  0.37658333  0.27024438  0.4198452   0.10188841  0.07283263  0.15717356\n",
      " -0.03537827  0.11466829  0.03649829  0.09502624 -0.21258889 -0.20152996\n",
      " -0.28612229 -0.13490468  0.5112024   0.03734954 -0.25004736  0.05511534\n",
      " -0.02291235]\n",
      "Gradient Descent(13/99): loss=0.43853005515961346, weights = [-0.11689404 -0.04277581 -0.03562289 -0.42922218  0.04437501 -0.41487194\n",
      "  0.32601394  0.19690028  0.5700941   0.13050551  0.08766745 -0.04714524\n",
      "  0.36438171  0.27058784  0.41628084  0.09338694  0.06773048  0.157922\n",
      " -0.03777185  0.10471933  0.03634608  0.08520987 -0.2123883  -0.20230892\n",
      " -0.28511801 -0.13409216  0.51203314  0.03761776 -0.24970825  0.0554589\n",
      " -0.02304087]\n",
      "Gradient Descent(14/99): loss=0.42519059882694643, weights = [-0.13667103 -0.04192279 -0.04413218 -0.43268779  0.04592107 -0.41462354\n",
      "  0.32730494  0.19712029  0.5591405   0.11813623  0.08718458 -0.04853243\n",
      "  0.3527094   0.27082977  0.41290687  0.085882    0.06294698  0.15924052\n",
      " -0.03943825  0.09567784  0.03627966  0.07641073 -0.21191983 -0.20303159\n",
      " -0.28416319 -0.13331517  0.51282639  0.03779161 -0.2494704   0.05570063\n",
      " -0.02313622]\n",
      "Gradient Descent(15/99): loss=0.41422437215008445, weights = [-0.15447033 -0.04084434 -0.05203538 -0.43505427  0.04736923 -0.41445277\n",
      "  0.328476    0.19726363  0.54922989  0.1069506   0.08681925 -0.04967732\n",
      "  0.34155423  0.27099432  0.40968246  0.07923233  0.05846811  0.16095372\n",
      " -0.04050745  0.08745848  0.03629201  0.06852439 -0.21124232 -0.20369995\n",
      " -0.28325853 -0.13257385  0.51358224  0.03789382 -0.24930967  0.05586472\n",
      " -0.02321587]\n",
      "Gradient Descent(16/99): loss=0.4051414948593811, weights = [-0.1704897  -0.03960069 -0.05941551 -0.43652916  0.04871608 -0.41434121\n",
      "  0.32954907  0.19734858  0.5402123   0.09682929  0.08652921 -0.05066293\n",
      "  0.33090135  0.27109991  0.40657535  0.07331811  0.05427942  0.16292953\n",
      " -0.04108831  0.07998412  0.03637776  0.06145703 -0.2104022  -0.20431669\n",
      " -0.28240345 -0.13186751  0.51430157  0.03794163 -0.24920772  0.05596958\n",
      " -0.02329258]\n",
      "Gradient Descent(17/99): loss=0.39756673440364454, weights = [-0.18490713 -0.03824011 -0.06633886 -0.43728006  0.04996151 -0.41427482\n",
      "  0.33054141  0.19738908  0.53196245  0.0876666   0.08628288 -0.05154997\n",
      "  0.32073419  0.27116051  0.40356015  0.06803767  0.05036631  0.16506913\n",
      " -0.04127193  0.07318511  0.03653247  0.05512436 -0.20943616 -0.20488484\n",
      " -0.28159646 -0.13119488  0.5149858   0.03794814 -0.24915058  0.05602923\n",
      " -0.02337557]\n",
      "Gradient Descent(18/99): loss=0.391209165618771, weights = [-0.19788281 -0.03680105 -0.07285887 -0.43744256  0.05110759 -0.41424289\n",
      "  0.33146671  0.19739573  0.5243755   0.0793685   0.08605691 -0.05238239\n",
      "  0.31103508  0.27118681  0.40061706  0.06330467  0.04671419  0.16729917\n",
      " -0.04113447  0.0669985   0.03675218  0.04945067 -0.20837326 -0.20540766\n",
      " -0.28083555 -0.13055438  0.51563667  0.03792332 -0.2491276   0.05605436\n",
      " -0.02347138]\n",
      "Gradient Descent(19/99): loss=0.3858408113059041, weights = [-0.20956093 -0.03531397 -0.07901897 -0.43712658  0.05215781 -0.41423726\n",
      "  0.33233586  0.19737663  0.51736342  0.07185109  0.08583423 -0.05319143\n",
      "  0.30178578  0.27118697  0.39773081  0.05904569  0.04330867  0.16956582\n",
      " -0.0407395   0.06136739  0.03703312  0.04436798 -0.20723655 -0.20588848\n",
      " -0.28011841 -0.1299443   0.51625603  0.03787474 -0.24913064  0.05605315\n",
      " -0.02358455]\n",
      "Gradient Descent(20/99): loss=0.38128135561591137, weights = [-0.22007124 -0.03380281 -0.08485488 -0.4364214   0.05311656 -0.41425166\n",
      "  0.33315758  0.19733797  0.51085207  0.06503931  0.08560257 -0.05399894\n",
      "  0.29296785  0.27116722  0.39488973  0.05519829  0.04013563  0.17183012\n",
      " -0.04014003  0.05624035  0.03737159  0.03981526 -0.20604433 -0.20633059\n",
      " -0.27944258 -0.12936283  0.51684579  0.0378082  -0.24915349  0.05603185\n",
      " -0.02371812]\n",
      "Gradient Descent(21/99): loss=0.3773870265815556, weights = [-0.22953052 -0.0322862  -0.09039629 -0.43539968  0.05398868 -0.41428129\n",
      "  0.33393894  0.1972845   0.50477886  0.05886585  0.08535334 -0.05481969\n",
      "  0.2845629   0.27113235  0.39208508  0.05170926  0.03718138  0.17406438\n",
      " -0.03938017  0.05157085  0.03776388  0.03573773 -0.20481118 -0.2067372\n",
      " -0.27880559 -0.1288082   0.51740781  0.0377281  -0.24919139  0.05599526\n",
      " -0.02387399]\n",
      "Gradient Descent(22/99): loss=0.3740423861796767, weights = [-0.23804386 -0.03077853 -0.09566815 -0.43412071  0.05477926 -0.41432247\n",
      "  0.33468569  0.19721987  0.49909075  0.05327021  0.08508075 -0.05566326\n",
      "  0.27655283  0.27108603  0.38931043  0.04853325  0.03443267  0.17624948\n",
      " -0.03849659  0.04731683  0.03820622  0.03208623 -0.20354868 -0.20711136\n",
      " -0.278205   -0.12827874  0.5179439   0.03763782 -0.24924065  0.05594708\n",
      " -0.02405322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(23/99): loss=0.37115418056823557, weights = [-0.24570588 -0.02929072 -0.10069169 -0.43263298  0.05549336 -0.41437234\n",
      "  0.33540254  0.19714687  0.49374262  0.04819791  0.08478112 -0.05653544\n",
      "  0.26891992  0.2710311   0.38656117  0.04563154  0.03187675  0.17837272\n",
      " -0.03751967  0.0434402   0.03869485  0.02881663 -0.20226607 -0.20745599\n",
      " -0.27763847 -0.12777282  0.51845572  0.03753998 -0.24929847  0.05589014\n",
      " -0.02425627]\n",
      "Gradient Descent(24/99): loss=0.36864667323059025, weights = [-0.25260169 -0.02783092 -0.1054852  -0.43097621  0.05613596 -0.41442871\n",
      "  0.33609339  0.19706767  0.488696    0.04359986  0.08445237 -0.05743924\n",
      "  0.26164696  0.27096975  0.38383417  0.042971    0.02950143  0.18042622\n",
      " -0.03647453  0.0399065   0.03922598  0.02588934 -0.2009707  -0.20777381\n",
      " -0.27710381 -0.12728897  0.51894485  0.03743655 -0.24936267  0.05582665\n",
      " -0.02448308]\n",
      "Gradient Descent(25/99): loss=0.3664580618708342, weights = [-0.25880792 -0.02640506 -0.11006458 -0.42918307  0.05671186 -0.41448987\n",
      "  0.3367615   0.19698395  0.48391791  0.03943172  0.08409357 -0.05837571\n",
      "  0.25471729  0.27090367  0.38112739  0.04052326  0.02729508  0.18240565\n",
      " -0.03538186  0.0366845   0.03979587  0.02326884 -0.19966842 -0.20806736\n",
      " -0.27659894 -0.12682582  0.51941271  0.03732908 -0.24943154  0.05575832\n",
      " -0.02473331]\n",
      "Gradient Descent(26/99): loss=0.36453769882636156, weights = [-0.26439353 -0.02501732 -0.11444384 -0.42728046  0.05722561 -0.4145545\n",
      "  0.33740961  0.196897    0.47938005  0.03565344  0.08370472 -0.0593445\n",
      "  0.24811487  0.27083418  0.37843972  0.03826401  0.02524665  0.18430928\n",
      " -0.03425868  0.03374595  0.04040083  0.02092328 -0.1983639  -0.208339\n",
      " -0.27612195 -0.12638211  0.51986062  0.03721878 -0.24950379  0.05568647\n",
      " -0.02500632]\n",
      "Gradient Descent(27/99): loss=0.3628439152274464, weights = [-0.26942058 -0.02367049 -0.11863542 -0.42529063  0.05768148 -0.41462156\n",
      "  0.33804004  0.19680783  0.47505797  0.03222878  0.08328642 -0.06034435\n",
      "  0.24182428  0.27076232  0.37577065  0.03617231  0.02334563  0.18613729\n",
      " -0.03311888  0.03106521  0.04103727  0.01882408 -0.19706082 -0.2085909\n",
      " -0.27567105 -0.12595669  0.52028977  0.03710656 -0.24957839  0.05561215\n",
      " -0.02530133]\n",
      "Gradient Descent(28/99): loss=0.3613423043429826, weights = [-0.27394492 -0.02236623 -0.12265049 -0.42323199  0.0580835  -0.41469025\n",
      "  0.33865476  0.19671725  0.47093052  0.02912493  0.08283974 -0.06137334\n",
      "  0.23583072  0.27068888  0.37312021  0.03423016  0.02158212  0.18789115\n",
      " -0.03197377  0.02861906  0.04170172  0.01694559 -0.19576209 -0.20882506\n",
      " -0.27524461 -0.12554853  0.52070125  0.03699316 -0.24965453  0.05553616\n",
      " -0.02561741]\n",
      "Gradient Descent(29/99): loss=0.3600043579853969, weights = [-0.27801683 -0.02110539 -0.12649913 -0.42111985  0.05843539 -0.41475992\n",
      "  0.33925547  0.19662585  0.46697931  0.02631219  0.08236605 -0.0624292\n",
      "  0.23012003  0.2706145   0.37048878  0.03242199  0.01994675  0.18957329\n",
      " -0.03083249  0.02638643  0.04239086  0.01526485 -0.19447002 -0.20904329\n",
      " -0.27484112 -0.12515669  0.52109603  0.03687914 -0.24973159  0.05545916\n",
      " -0.02595356]\n",
      "Gradient Descent(30/99): loss=0.3588063769999523, weights = [-0.28168154 -0.01988811 -0.13019054 -0.41896695  0.05874062 -0.41483007\n",
      "  0.33984364  0.19653413  0.46318831  0.02376363  0.08186696 -0.06350945\n",
      "  0.22467868  0.27053967  0.367877    0.03073435  0.01843072  0.19118669\n",
      " -0.02970238  0.02434821  0.04310151  0.01376122 -0.1931864  -0.20924727\n",
      " -0.27445919 -0.1247803   0.52147503  0.03676494 -0.24980909  0.05538162\n",
      " -0.02630873]\n",
      "Gradient Descent(31/99): loss=0.35772859632823834, weights = [-0.28497979 -0.01871406 -0.13373314 -0.41678395  0.05900241 -0.41490032\n",
      "  0.34042054  0.19644247  0.45954347  0.02145483  0.08134417 -0.06461151\n",
      "  0.21949375  0.27046479  0.36528567  0.02915553  0.01702574  0.19273472\n",
      " -0.02858924  0.02248706  0.04383068  0.01241624 -0.1919126  -0.20943852\n",
      " -0.27409755 -0.1244186   0.52183903  0.03665093 -0.24988662  0.05530397\n",
      " -0.02668184]\n",
      "Gradient Descent(32/99): loss=0.356754479289906, weights = [-0.28794821 -0.01758252 -0.1371347  -0.41457976  0.05922372 -0.41497036\n",
      "  0.34098728  0.19635116  0.45603247  0.01936364  0.08079948 -0.06573282\n",
      "  0.2145529   0.27039015  0.36271572  0.02767538  0.01572402  0.19422095\n",
      " -0.02749765  0.02078723  0.04457556  0.01121334 -0.19064969 -0.2096184\n",
      " -0.27375503 -0.12407087  0.5221888   0.03653736 -0.2499639   0.0552265\n",
      " -0.02707177]\n",
      "Gradient Descent(33/99): loss=0.3558701461606651, weights = [-0.29061979 -0.01649249 -0.14040239 -0.41236189  0.05940728 -0.41503994\n",
      "  0.34154483  0.19626044  0.45264444  0.01746998  0.08023467 -0.06687088\n",
      "  0.2098444   0.270316    0.36016809  0.02628501  0.01451827  0.19564897\n",
      " -0.02643112  0.01923442  0.0453335   0.01013765 -0.18939847 -0.20978816\n",
      " -0.27343055 -0.12373646  0.52252499  0.03642447 -0.25004067  0.05514945\n",
      " -0.02747745]\n",
      "Gradient Descent(34/99): loss=0.35506390991496434, weights = [-0.29302421 -0.01544275 -0.1435429  -0.41013663  0.05955562 -0.41510887\n",
      "  0.34209405  0.19617047  0.44936976  0.01575559  0.07965155 -0.06802329\n",
      "  0.20535705  0.27024253  0.35764378  0.02497664  0.01340168  0.1970224\n",
      " -0.02539231  0.01781562  0.04610206  0.00917587 -0.1881595  -0.20994893\n",
      " -0.27312311 -0.12341478  0.52284824  0.03631241 -0.25011677  0.05507303\n",
      " -0.02789779]\n",
      "Gradient Descent(35/99): loss=0.3543258978872964, weights = [-0.29518819 -0.01443196 -0.14656245 -0.40790932  0.05967108 -0.41517701\n",
      "  0.34263568  0.19608141  0.44619991  0.01420393  0.07905187 -0.06918777\n",
      "  0.20108017  0.27016987  0.35514374  0.02374345  0.01236785  0.19834472\n",
      " -0.02438314  0.01651904  0.04687896  0.00831606 -0.18693319 -0.21010172\n",
      " -0.27283178 -0.12310527  0.52315911  0.03620134 -0.25019204  0.05499738\n",
      " -0.02833172]\n",
      "Gradient Descent(36/99): loss=0.35364774260053866, weights = [-0.29713577 -0.01345869 -0.14946686 -0.40568447  0.05975581 -0.41524425\n",
      "  0.34317039  0.19599336  0.4431273   0.01279996  0.07843733 -0.0703622\n",
      "  0.19700364  0.27009815  0.3526689   0.02257939  0.01141084  0.1996193\n",
      " -0.02340499  0.01533394  0.04766211  0.00754753 -0.1857198  -0.21024743\n",
      " -0.27255572 -0.12280743  0.52345813  0.03609134 -0.25026637  0.05492262\n",
      " -0.02877822]\n",
      "Gradient Descent(37/99): loss=0.35302232847446985, weights = [-0.29888859 -0.01252147 -0.15226159 -0.40346587  0.05981178 -0.41531049\n",
      "  0.34369876  0.1959064   0.44014516  0.01153002  0.07780954 -0.07154459\n",
      "  0.19311778  0.27002744  0.35022011  0.02147914  0.01052507  0.20084935\n",
      " -0.0224587   0.01425054  0.0484496   0.0068607  -0.1845195  -0.21038689\n",
      " -0.27229412 -0.12252078  0.5237458   0.03598251 -0.25033968  0.05484883\n",
      " -0.02923628]\n",
      "Gradient Descent(38/99): loss=0.35244358382168495, weights = [-0.30046613 -0.01161878 -0.15495177 -0.40125678  0.05984084 -0.41537569\n",
      "  0.34422131  0.19582057  0.43724743  0.01038171  0.07717008 -0.0727331\n",
      "  0.18941342  0.26995781  0.34779817  0.02043796  0.00970535  0.20203788\n",
      " -0.02154475  0.01325996  0.04923966  0.00624698 -0.18333233 -0.21052082\n",
      " -0.27204624 -0.12224489  0.52402256  0.0358749  -0.25041191  0.05477608\n",
      " -0.02970491]\n",
      "Gradient Descent(39/99): loss=0.35190630964831454, weights = [-0.30188592 -0.01074912 -0.15754222 -0.39905995  0.05984466 -0.41543978\n",
      "  0.34473851  0.19573593  0.43442866  0.00934376  0.07652042 -0.07392606\n",
      "  0.18588182  0.26988929  0.34540381  0.01945161  0.00894686  0.20318773\n",
      " -0.02066329  0.01235411  0.05003071  0.00569867 -0.18215828 -0.21064988\n",
      " -0.2718114  -0.12197932  0.52428886  0.03576855 -0.25048302  0.05470442\n",
      " -0.03018319]\n",
      "Gradient Descent(40/99): loss=0.35140603844060603, weights = [-0.30316373 -0.00991101 -0.16003747 -0.3968777   0.05982483 -0.41550276\n",
      "  0.34525075  0.1956525   0.43168398  0.00840594  0.07586194 -0.0751219\n",
      "  0.18251465  0.26982192  0.34303767  0.01851631  0.00824511  0.20430154\n",
      " -0.01981421  0.01152563  0.05082128  0.00520889 -0.1809973  -0.21077464\n",
      " -0.27158894 -0.12172369  0.5245451   0.03566351 -0.25055298  0.05463387\n",
      " -0.03067021]\n",
      "Gradient Descent(41/99): loss=0.35093891743749606, weights = [-0.30431376 -0.00910301 -0.16244183 -0.39471204  0.05978279 -0.41556459\n",
      "  0.3457584   0.19557028  0.42900895  0.00755895  0.07519597 -0.07631922\n",
      "  0.17930402  0.26975571  0.3407003   0.01762865  0.0075959   0.20538178\n",
      " -0.01899718  0.01076781  0.05161008  0.00477147 -0.17984925 -0.21089562\n",
      " -0.27137826 -0.12147763  0.52479166  0.03555978 -0.25062178  0.05456445\n",
      " -0.0311651 ]\n",
      "Gradient Descent(42/99): loss=0.35050161194009694, weights = [-0.30534878 -0.0083237  -0.16475935 -0.39256464  0.05971988 -0.41562527\n",
      "  0.34626178  0.19548929  0.4263996   0.00679434  0.07452373 -0.07751673\n",
      "  0.17624242  0.26969066  0.33839221  0.01678556  0.00699534  0.20643071\n",
      " -0.01821173  0.01007454  0.05239592  0.00438088 -0.178714   -0.21101328\n",
      " -0.27117878 -0.12124078  0.52502891  0.03545739 -0.25068942  0.05449617\n",
      " -0.03166704]\n",
      "Gradient Descent(43/99): loss=0.3500912250498366, weights = [-0.3062803  -0.00757172 -0.16699385 -0.39043695  0.05963735 -0.41568482\n",
      "  0.34676116  0.1954095   0.42385232  0.00610442  0.07384636 -0.07871324\n",
      "  0.17332269  0.26962678  0.33611378  0.01598425  0.00643983  0.20745044\n",
      " -0.01745723  0.00944025  0.05317775  0.00403216 -0.17759137 -0.21112803\n",
      " -0.27098996 -0.12101281  0.5252572   0.03535632 -0.25075589  0.05442903\n",
      " -0.03217524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(44/99): loss=0.3497052309020497, weights = [-0.30711867 -0.00684577 -0.16914896 -0.38833017  0.05953635 -0.41574322\n",
      "  0.34725679  0.19533092  0.42136381  0.00548223  0.07316495 -0.0799077\n",
      "  0.17053804  0.26956405  0.33386535  0.01522218  0.005926    0.2084429\n",
      " -0.01673297  0.00885987  0.05395464  0.00372088 -0.17648116 -0.2112402\n",
      " -0.27081128 -0.12079339  0.52547685  0.03525659 -0.2508212   0.05436302\n",
      " -0.03268893]\n",
      "Gradient Descent(45/99): loss=0.34934141900569093, weights = [-0.3078732  -0.00614459 -0.17122812 -0.38624532  0.05941796 -0.41580052\n",
      "  0.34774889  0.19525352  0.4189311   0.00492143  0.07248049 -0.08109914\n",
      "  0.16788201  0.26950245  0.33164717  0.01449705  0.00545075  0.20940987\n",
      " -0.01603817  0.00832877  0.05472575  0.00344308 -0.17538316 -0.21135013\n",
      " -0.27064228 -0.12058221  0.52568819  0.03515818 -0.25088539  0.05429812\n",
      " -0.03320741]\n",
      "Gradient Descent(46/99): loss=0.3489978477390421, weights = [-0.30855228 -0.00546699 -0.17323458 -0.38418328  0.05928319 -0.41585671\n",
      "  0.34823767  0.19517728  0.41655146  0.00441624  0.0717939  -0.08228669\n",
      "  0.16534845  0.26944196  0.32945944  0.01380673  0.00501118  0.21035298\n",
      " -0.01537198  0.00784272  0.05549033  0.00319521 -0.17429715 -0.21145808\n",
      " -0.27048248 -0.12037897  0.52589152  0.03506107 -0.25094845  0.05423432\n",
      " -0.03372998]\n",
      "Gradient Descent(47/99): loss=0.3486728054071065, weights = [-0.30916346 -0.00481182 -0.17517144 -0.38214474  0.05913297 -0.41591184\n",
      "  0.34872327  0.19510218  0.41422242  0.00396143  0.07110605 -0.08346955\n",
      "  0.16293153  0.26938257  0.32730228  0.01314928  0.00460462  0.21127374\n",
      " -0.01473351  0.00739787  0.05624776  0.00297408 -0.17322291 -0.21156429\n",
      " -0.27033145 -0.12018338  0.52608714  0.03496526 -0.25101043  0.05417159\n",
      " -0.03425601]\n",
      "Gradient Descent(48/99): loss=0.3483647775549096, weights = [-0.30971351 -0.00417801 -0.17704162 -0.38013032  0.05896817 -0.41596592\n",
      "  0.34920586  0.19502817  0.41194169  0.00355226  0.07041772 -0.08464703\n",
      "  0.1606257   0.26932423  0.32517575  0.01252288  0.00422857  0.21217352\n",
      " -0.01412186  0.00699071  0.05699747  0.00277685 -0.1721602  -0.21166897\n",
      " -0.27018878 -0.11999517  0.52627532  0.03487072 -0.25107135  0.0541099\n",
      " -0.03478487]\n",
      "Gradient Descent(49/99): loss=0.348072419465738, weights = [-0.31020856 -0.00356451 -0.17884793 -0.37814049  0.0587896  -0.41601898\n",
      "  0.34968557  0.19495525  0.40970719  0.00318439  0.06972965 -0.08581848\n",
      "  0.15842566  0.26926693  0.32307986  0.01192589  0.00388075  0.21305357\n",
      " -0.01353611  0.00661803  0.05773896  0.00260097 -0.17110878 -0.21177229\n",
      " -0.27005407 -0.11981407  0.52645635  0.03477743 -0.25113124  0.05404924\n",
      " -0.035316  ]\n",
      "Gradient Descent(50/99): loss=0.34779453296445145, weights = [-0.3106541  -0.00297037 -0.180593   -0.37617565  0.05859803 -0.41607107\n",
      "  0.3501625   0.19488336  0.407517    0.00285389  0.06904252 -0.08698332\n",
      "  0.15632641  0.26921062  0.32101458  0.01135675  0.00355901  0.21391504\n",
      " -0.01297533  0.00627689  0.05847183  0.00244417 -0.17006845 -0.21187441\n",
      " -0.26992695 -0.11963981  0.52663048  0.03468537 -0.25119012  0.05398955\n",
      " -0.03584883]\n",
      "Gradient Descent(51/99): loss=0.3475300468019509, weights = [-0.31105509 -0.00239465 -0.18227935 -0.37423609  0.05839418 -0.4161222\n",
      "  0.35063676  0.19481248  0.40536936  0.0025572   0.06835694 -0.08814104\n",
      "  0.15432317  0.26915527  0.31897981  0.01081402  0.00326138  0.21475899\n",
      " -0.0124386   0.00596463  0.05919572  0.00230441 -0.16903895 -0.21197546\n",
      " -0.26980705 -0.11947216  0.52679797  0.0345945  -0.25124805  0.05393082\n",
      " -0.03638286]\n",
      "Gradient Descent(52/99): loss=0.34727800002436315, weights = [-0.31141598 -0.00183647 -0.18390938 -0.37232206  0.05817871 -0.41617242\n",
      "  0.35110842  0.19474256  0.40326265  0.00229107  0.0676735  -0.08929118\n",
      "  0.15241139  0.26910086  0.31697543  0.01029637  0.00298604  0.21558637\n",
      " -0.01192502  0.00567879  0.05991034  0.00217986 -0.16802008 -0.21207555\n",
      " -0.26969405 -0.11931086  0.52695907  0.03450481 -0.25130504  0.05387302\n",
      " -0.0369176 ]\n",
      "Gradient Descent(53/99): loss=0.3470375278347257, weights = [-0.31174079 -0.00129502 -0.18548538 -0.37043372  0.05795226 -0.41622177\n",
      "  0.35157758  0.19467358  0.40119537  0.00205256  0.0669927  -0.09043332\n",
      "  0.15058676  0.26904734  0.31500126  0.00980253  0.00273129  0.21639807\n",
      " -0.01143368  0.00541714  0.06061544  0.0020689  -0.16701161 -0.21217477\n",
      " -0.2695876  -0.11915569  0.52711402  0.03441626 -0.25136114  0.05381609\n",
      " -0.03745259]\n",
      "Gradient Descent(54/99): loss=0.34680784954037364, weights = [-0.31203311 -0.00076951 -0.18700952 -0.36857119  0.05771541 -0.41627026\n",
      "  0.35204428  0.19460551  0.39916612  0.00183899  0.06631503 -0.0915671\n",
      "  0.14884517  0.26899468  0.31305712  0.00933134  0.00249557  0.21719488\n",
      " -0.01096371  0.00517763  0.06131082  0.00197006 -0.16601333 -0.2122732\n",
      " -0.2694874  -0.1190064   0.52726304  0.03432882 -0.25141638  0.05376002\n",
      " -0.0379874 ]\n",
      "Gradient Descent(55/99): loss=0.34658825824934275, weights = [-3.12296196e-01 -2.59192168e-04 -1.88483867e-01 -3.66734528e-01\n",
      "  5.74687392e-02 -4.16317954e-01  3.52508580e-01  1.94538290e-01\n",
      "  3.97173623e-01  1.64792953e-03  6.56409314e-02 -9.26921913e-02\n",
      "  1.47182715e-01  2.68942849e-01  3.11142757e-01  8.88167458e-03\n",
      "  2.27743050e-03  2.17977552e-01 -1.05142388e-02  4.95840840e-03\n",
      "  6.19963477e-02  1.88205530e-03 -1.65025034e-01 -2.12370900e-01\n",
      " -2.69393159e-01 -1.18862795e-01  5.27406359e-01  3.42424735e-02\n",
      " -2.51470789e-01  5.37047641e-02 -3.85216187e-02]\n",
      "Gradient Descent(56/99): loss=0.3463781120366505, weights = [-3.12532976e-01  2.36617544e-04 -1.89910399e-01 -3.64923751e-01\n",
      "  5.72127620e-02 -4.16364871e-01  3.52970530e-01  1.94471901e-01\n",
      "  3.95216677e-01  1.47716910e-03  6.49707968e-02 -9.38083020e-02\n",
      "  1.45595682e-01  2.68891804e-01  3.09257926e-01  8.45250422e-03\n",
      "  2.07553420e-03  2.18746745e-01 -1.00844395e-02  4.75775802e-03\n",
      "  6.26718957e-02  1.80370807e-03 -1.64046523e-01 -2.12467928e-01\n",
      " -2.69304580e-01 -1.18724647e-01  5.27544193e-01  3.41571791e-02\n",
      " -2.51524412e-01  5.36502908e-02 -3.90548781e-02]\n",
      "Gradient Descent(57/99): loss=0.34617682634864866, weights = [-0.31274608  0.00071858 -0.191291   -0.36313884  0.05694798 -0.41641105\n",
      "  0.35343016  0.19440631  0.39329417  0.0013247   0.06430499 -0.09491518\n",
      "  0.14408054  0.26884151  0.30740234  0.00804285  0.00188864  0.21950308\n",
      " -0.0096735   0.00457412  0.06333739  0.00173398 -0.1630776  -0.21256432\n",
      " -0.2692214  -0.11859175  0.52767675  0.03407291 -0.25157728  0.05359657\n",
      " -0.03958682]\n",
      "Gradient Descent(58/99): loss=0.34598386745268106, weights = [-0.31293787  0.00118733 -0.19262747 -0.36137973  0.05667486 -0.41645652\n",
      "  0.35388752  0.19434147  0.39140507  0.00118872  0.06364384 -0.09601259\n",
      "  0.14263391  0.26879194  0.3055757   0.00765177  0.00171561  0.22024712\n",
      " -0.00928062  0.00440606  0.06399278  0.00167194 -0.16211809 -0.21266011\n",
      " -0.26914335 -0.11846391  0.52780423  0.03398964 -0.25162943  0.05354356\n",
      " -0.04011712]\n",
      "Gradient Descent(59/99): loss=0.34579874677138517, weights = [-0.31311048  0.00164344 -0.19392152 -0.35964634  0.05639386 -0.41650132\n",
      "  0.35434261  0.19427735  0.3895484   0.00106755  0.06298764 -0.09710035\n",
      "  0.14125262  0.26874306  0.30377768  0.00727841  0.00155538  0.22097938\n",
      " -0.00890506  0.00425228  0.06463806  0.00161677 -0.1611678  -0.21275532\n",
      " -0.26907018 -0.11834094  0.52792683  0.03390733 -0.25168089  0.05349123\n",
      " -0.04064546]\n",
      "Gradient Descent(60/99): loss=0.3456210159675635, weights = [-0.31326584  0.00208747 -0.19517479 -0.35793855  0.05610539 -0.41654548\n",
      "  0.35479547  0.19421393  0.38772325  0.00095971  0.06233667 -0.09817828\n",
      "  0.1399336   0.26869483  0.30200795  0.00692193  0.00140698  0.22170033\n",
      " -0.00854607  0.00411158  0.06527322  0.00156771 -0.16022657 -0.21284996\n",
      " -0.26900167 -0.11822264  0.52804474  0.03382596 -0.25173169  0.05343955\n",
      " -0.04117156]\n",
      "Gradient Descent(61/99): loss=0.3454502626674649, weights = [-0.31340565  0.00251994 -0.19638885 -0.35625622  0.05580986 -0.41658904\n",
      "  0.35524612  0.19415117  0.38592879  0.00086382  0.06169115 -0.09924623\n",
      "  0.13867397  0.26864723  0.30026615  0.00658155  0.0012695   0.22241041\n",
      " -0.00820294  0.00398287  0.06589831  0.0015241  -0.15929423 -0.21294404\n",
      " -0.26893757 -0.11810883  0.52815814  0.0337455  -0.25178187  0.05338849\n",
      " -0.04169516]\n",
      "Gradient Descent(62/99): loss=0.34528610672852267, weights = [-0.31353149  0.00294135 -0.19756519 -0.35459918  0.05550764 -0.41663202\n",
      "  0.35569456  0.19408904  0.3841642   0.00077867  0.06105132 -0.10030409\n",
      "  0.13747097  0.26860021  0.29855193  0.00625652  0.00114211  0.22311002\n",
      " -0.00787499  0.00386514  0.06651336  0.00148536 -0.15837062 -0.21303758\n",
      " -0.26887768 -0.11799935  0.52826721  0.03366592 -0.25183146  0.05333802\n",
      " -0.042216  ]\n",
      "Gradient Descent(63/99): loss=0.34512819697266334, weights = [-0.31364474  0.00335218 -0.19870524 -0.35296726  0.05519911 -0.41667445\n",
      "  0.3561408   0.1940275   0.38242874  0.00070312  0.06041735 -0.10135173\n",
      "  0.13632197  0.26855376  0.29686492  0.00594614  0.00102403  0.22379953\n",
      " -0.00756158  0.00375747  0.06711845  0.00145095 -0.1574556  -0.21313056\n",
      " -0.26882179 -0.11789401  0.52837211  0.0335872  -0.25188049  0.0532881\n",
      " -0.04273385]\n",
      "Gradient Descent(64/99): loss=0.34497620831875325, weights = [-0.31374666  0.00375285 -0.19981038 -0.35136026  0.0548846  -0.41671636\n",
      "  0.35658486  0.19396654  0.3807217   0.00063616  0.05978942 -0.10238908\n",
      "  0.13522448  0.26850784  0.29520473  0.00564973  0.00091456  0.22447927\n",
      " -0.00726205  0.00365903  0.06771366  0.00142041 -0.15654901 -0.21322299\n",
      " -0.26876969 -0.11779267  0.52847301  0.0335093  -0.25192898  0.05323872\n",
      " -0.04324852]\n",
      "Gradient Descent(65/99): loss=0.3448298392582736, weights = [-0.3138384   0.00414378 -0.20088191 -0.34977797  0.05456444 -0.41675779\n",
      "  0.35702673  0.19390611  0.3790424   0.00057687  0.05916768 -0.10341606\n",
      "  0.13417613  0.26846243  0.29357099  0.00536666  0.00081304  0.22514956\n",
      " -0.00697582  0.00356905  0.06829908  0.00139331 -0.15565073 -0.21331486\n",
      " -0.2687212  -0.11769516  0.52857007  0.0334322  -0.25197697  0.05318984\n",
      " -0.0437598 ]\n",
      "Gradient Descent(66/99): loss=0.3446888096268366, weights = [-3.13920958e-01  4.52538441e-03 -2.01921082e-01 -3.48220143e-01\n",
      "  5.42389501e-02 -4.16798750e-01  3.57466427e-01  1.93846206e-01\n",
      "  3.77390213e-01  5.24419543e-04  5.85522668e-02 -1.04432613e-01\n",
      "  1.33174646e-01  2.68417494e-01  2.91963310e-01  5.09631266e-03\n",
      "  7.18856521e-04  2.25810683e-01 -6.70229156e-03  3.48681761e-03\n",
      "  6.88748290e-02  1.36927366e-03 -1.54760623e-01 -2.13406157e-01\n",
      " -2.68676135e-01 -1.17601333e-01  5.28663432e-01  3.33558710e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -2.52024470e-01  5.31414412e-02 -4.42675059e-02]\n",
      "Gradient Descent(67/99): loss=0.34455285863152696, weights = [-3.13995262e-01  4.89802003e-03 -2.02929099e-01 -3.46686559e-01\n",
      "  5.39084275e-02 -4.16839272e-01  3.57903943e-01  1.93786790e-01\n",
      "  3.75764537e-01  4.78045963e-04  5.79432836e-02 -1.05438695e-01\n",
      "  1.32217879e-01  2.68373015e-01  2.90381294e-01  4.83811997e-03\n",
      "  6.31456174e-04  2.26462910e-01 -6.44091618e-03  3.41168951e-03\n",
      "  6.94410211e-02  1.34797075e-03 -1.53878559e-01 -2.13496871e-01\n",
      " -2.68634323e-01 -1.17511041e-01  5.28753254e-01  3.32802949e-02\n",
      " -2.52071520e-01  5.30934920e-02 -4.47714872e-02]\n",
      "Gradient Descent(68/99): loss=0.3444217430999905, weights = [-3.14062136e-01  5.26204448e-03 -2.03907108e-01 -3.45176965e-01\n",
      "  5.35731572e-02 -4.16879379e-01  3.58339279e-01  1.93727839e-01\n",
      "  3.74164800e-01  4.37067819e-04  5.73408324e-02 -1.06434275e-01\n",
      "  1.31303778e-01  2.68328966e-01  2.88824554e-01  4.59152802e-03\n",
      "  5.50319026e-04  2.27106487e-01 -6.19115723e-03  3.34307459e-03\n",
      "  6.99977848e-02  1.32910113e-03 -1.53004422e-01 -2.13586989e-01\n",
      " -2.68595598e-01 -1.17424144e-01  5.28839674e-01  3.32054445e-02\n",
      " -2.52118139e-01  5.30459712e-02 -4.52715897e-02]\n",
      "Gradient Descent(69/99): loss=0.3442952359223257, weights = [-3.14122322e-01  5.61779210e-03 -2.04856211e-01 -3.43691103e-01\n",
      "  5.32334125e-02 -4.16919095e-01  3.58772434e-01  1.93669332e-01\n",
      "  3.72590455e-01  4.00865823e-04  5.67449959e-02 -1.07419334e-01\n",
      "  1.30430390e-01  2.68285324e-01  2.87292695e-01  4.35601257e-03\n",
      "  4.74967388e-04  2.27741644e-01 -5.95250278e-03  3.28043053e-03\n",
      "  7.05452557e-02  1.31239847e-03 -1.52138098e-01 -2.13676496e-01\n",
      " -2.68559801e-01 -1.17340508e-01  5.28922828e-01  3.31312967e-02\n",
      " -2.52164350e-01  5.29988562e-02 -4.57676776e-02]\n",
      "Gradient Descent(70/99): loss=0.34417312466103284, weights = [-3.14176490e-01  5.96557938e-03 -2.05777461e-01 -3.42228710e-01\n",
      "  5.28894548e-02 -4.16958442e-01  3.59203405e-01  1.93611246e-01\n",
      "  3.71040982e-01  3.68879798e-04  5.61558432e-02 -1.08393862e-01\n",
      "  1.29595855e-01  2.68242067e-01  2.85785324e-01  4.13107398e-03\n",
      "  4.04960304e-04  2.28368595e-01 -5.72446254e-03  3.22326028e-03\n",
      "  7.10835757e-02  1.29762536e-03 -1.51279479e-01 -2.13765377e-01\n",
      " -2.68526782e-01 -1.17260001e-01  5.29002847e-01  3.30578291e-02\n",
      " -2.52210175e-01  5.29521252e-02 -4.62596276e-02]\n",
      "Gradient Descent(71/99): loss=0.34405521030790803, weights = [-3.14225241e-01  6.30570604e-03 -2.06671869e-01 -3.40789513e-01\n",
      "  5.25415342e-02 -4.16997440e-01  3.59632188e-01  1.93553561e-01\n",
      "  3.69515885e-01  3.40603319e-04  5.55734305e-02 -1.09357858e-01\n",
      "  1.28798403e-01  2.68199174e-01  2.84302051e-01  3.91623593e-03\n",
      "  3.39890751e-04  2.28987540e-01 -5.50656715e-03  3.17110824e-03\n",
      "  7.16128921e-02  1.28457019e-03 -1.50428462e-01 -2.13853614e-01\n",
      " -2.68496396e-01 -1.17182497e-01  5.29079857e-01  3.29850202e-02\n",
      " -2.52255635e-01  5.29057578e-02 -4.67473287e-02]\n",
      "Gradient Descent(72/99): loss=0.3439413061696688, weights = [-3.14269117e-01  6.63845599e-03 -2.07540403e-01 -3.39373238e-01\n",
      "  5.21898904e-02 -4.17036109e-01  3.60058780e-01  1.93496256e-01\n",
      "  3.68014686e-01  3.15578838e-04  5.49978022e-02 -1.10311331e-01\n",
      "  1.28036346e-01  2.68156626e-01  2.82842484e-01  3.71104422e-03\n",
      "  2.79383053e-04  2.29598665e-01 -5.29836734e-03  3.12355674e-03\n",
      "  7.21333567e-02  1.27304438e-03 -1.49584947e-01 -2.13941192e-01\n",
      " -2.68468506e-01 -1.17107877e-01  5.29153979e-01  3.29128492e-02\n",
      " -2.52300749e-01  5.28597344e-02 -4.72306812e-02]\n",
      "Gradient Descent(73/99): loss=0.3438312368667844, weights = [-3.14308605e-01  6.96409831e-03 -2.08383995e-01 -3.37979602e-01\n",
      "  5.18347532e-02 -4.17074468e-01  3.60483176e-01  1.93439314e-01\n",
      "  3.66536929e-01  2.93393251e-04  5.44289919e-02 -1.11254298e-01\n",
      "  1.27308077e-01  2.68114405e-01  2.81406234e-01  3.51506559e-03\n",
      "  2.23090481e-04  2.30202145e-01 -5.09943327e-03  3.08022286e-03\n",
      "  7.26451245e-02  1.26287995e-03 -1.48748842e-01 -2.14028092e-01\n",
      " -2.68442981e-01 -1.17036023e-01  5.29225331e-01  3.28412961e-02\n",
      " -2.52345535e-01  5.28140366e-02 -4.77095958e-02]\n",
      "Gradient Descent(74/99): loss=0.34372483743200305, weights = [-3.14344145e-01  7.28288810e-03 -2.09203534e-01 -3.36608322e-01\n",
      "  5.14763428e-02 -4.17112534e-01  3.60905371e-01  1.93382716e-01\n",
      "  3.65082175e-01  2.73673861e-04  5.38670232e-02 -1.12186782e-01\n",
      "  1.26612066e-01  2.68072491e-01  2.79992918e-01  3.32788668e-03\n",
      "  1.70693039e-04  2.30798143e-01 -4.90935373e-03  3.04075550e-03\n",
      "  7.31483535e-02  1.25392725e-03 -1.47920056e-01 -2.14114299e-01\n",
      " -2.68419694e-01 -1.16966823e-01  5.29294026e-01  3.27703419e-02\n",
      " -2.52390012e-01  5.27686472e-02 -4.81839938e-02]\n",
      "Gradient Descent(75/99): loss=0.343621952496963, weights = [-3.14376130e-01  7.59506733e-03 -2.09999876e-01 -3.35259109e-01\n",
      "  5.11148707e-02 -4.17150323e-01  3.61325359e-01  1.93326446e-01\n",
      "  3.63650001e-01  2.56084701e-04  5.33119102e-02 -1.13108815e-01\n",
      "  1.25946854e-01  2.68030870e-01  2.78602153e-01  3.14911303e-03\n",
      "  1.21895414e-04  2.31386814e-01 -4.72773541e-03  3.00483268e-03\n",
      "  7.36432037e-02  1.24605302e-03 -1.47098504e-01 -2.14199794e-01\n",
      " -2.68398527e-01 -1.16900170e-01  5.29360170e-01  3.26999680e-02\n",
      " -2.52434195e-01  5.27235496e-02 -4.86538055e-02]\n",
      "Gradient Descent(76/99): loss=0.3435224355567894, weights = [-3.14404917e-01  7.90086558e-03 -2.10773843e-01 -3.33931675e-01\n",
      "  5.07505396e-02 -4.17187852e-01  3.61743136e-01  1.93270488e-01\n",
      "  3.62240000e-01  2.40323197e-04  5.27636590e-02 -1.14020431e-01\n",
      "  1.25311051e-01  2.67989526e-01  2.77233560e-01  2.97836818e-03\n",
      "  7.64250769e-05  2.31968304e-01 -4.55420222e-03  2.97215913e-03\n",
      "  7.41298361e-02  1.23913862e-03 -1.46284104e-01 -2.14284560e-01\n",
      " -2.68379364e-01 -1.16835961e-01  5.29423868e-01  3.26301570e-02\n",
      " -2.52478101e-01  5.26787286e-02 -4.91189705e-02]\n",
      "Gradient Descent(77/99): loss=0.3434261483039246, weights = [-3.14430826e-01  8.20050079e-03 -2.11526221e-01 -3.32625726e-01\n",
      "  5.03835445e-02 -4.17225134e-01  3.62158695e-01  1.93214828e-01\n",
      "  3.60851777e-01  2.26117117e-04  5.22222672e-02 -1.14921674e-01\n",
      "  1.24703333e-01  2.67948443e-01  2.75886767e-01  2.81529274e-03\n",
      "  3.40305369e-05  2.32542749e-01 -4.38839456e-03  2.94246405e-03\n",
      "  7.46084129e-02  1.23307844e-03 -1.45476778e-01 -2.14368583e-01\n",
      " -2.68362098e-01 -1.16774096e-01  5.29485220e-01  3.25608918e-02\n",
      " -2.52521743e-01  5.26341698e-02 -4.95794367e-02]\n",
      "Gradient Descent(78/99): loss=0.3433329600235536, weights = [-3.14454143e-01  8.49417996e-03 -2.12257768e-01 -3.31340971e-01\n",
      "  5.00140725e-02 -4.17262184e-01  3.62572033e-01  1.93159451e-01\n",
      "  3.59484952e-01  2.13221812e-04  5.16877258e-02 -1.15812590e-01\n",
      "  1.24122438e-01  2.67907609e-01  2.74561404e-01  2.65954361e-03\n",
      " -5.52028249e-06  2.33110280e-01 -4.22996865e-03  2.91549904e-03\n",
      "  7.50790965e-02  1.22777849e-03 -1.44676452e-01 -2.14451844e-01\n",
      " -2.68346624e-01 -1.16714479e-01  5.29544321e-01  3.24921563e-02\n",
      " -2.52565136e-01  5.25898595e-02 -5.00351600e-02]\n",
      "Gradient Descent(79/99): loss=0.343242747043935, weights = [-3.14475129e-01  8.78209975e-03 -2.12969211e-01 -3.30077117e-01\n",
      "  4.96423035e-02 -4.17299013e-01  3.62983144e-01  1.93104347e-01\n",
      "  3.58139157e-01  2.01417685e-04  5.11600189e-02 -1.16693231e-01\n",
      "  1.23567163e-01  2.67867010e-01  2.73257107e-01  2.51079321e-03\n",
      " -4.24415380e-05  2.33671022e-01 -4.07859587e-03  2.89103625e-03\n",
      "  7.55420491e-02  1.22315511e-03 -1.43883053e-01 -2.14534329e-01\n",
      " -2.68332843e-01 -1.16657019e-01  5.29601263e-01  3.24239352e-02\n",
      " -2.52608292e-01  5.25457852e-02 -5.04861036e-02]\n",
      "Gradient Descent(80/99): loss=0.34315539223580366, weights = [-3.14494016e-01  9.06444712e-03 -2.13661246e-01 -3.28833869e-01\n",
      "  4.92684105e-02 -4.17335635e-01  3.63392022e-01  1.93049501e-01\n",
      "  3.56814034e-01  1.90507902e-04  5.06391244e-02 -1.17563653e-01\n",
      "  1.23036360e-01  2.67826636e-01  2.71973516e-01  2.36872872e-03\n",
      " -7.69308488e-05  2.34225091e-01 -3.93396212e-03  2.86886666e-03\n",
      "  7.59974321e-02  1.21913392e-03 -1.43096514e-01 -2.14616022e-01\n",
      " -2.68320662e-01 -1.16601628e-01  5.29656136e-01  3.23562136e-02\n",
      " -2.52651223e-01  5.25019351e-02 -5.09322377e-02]\n",
      "Gradient Descent(81/99): loss=0.34307078455571666, weights = [-3.14511014e-01  9.34139987e-03 -2.14334546e-01 -3.27610934e-01\n",
      "  4.88925599e-02 -4.17372059e-01  3.63798664e-01  1.92994904e-01\n",
      "  3.55509235e-01  1.80316304e-04  5.01250150e-02 -1.18423914e-01\n",
      "  1.22528938e-01  2.67786473e-01  2.70710277e-01  2.23305138e-03\n",
      " -1.09170574e-04  2.34772601e-01 -3.79576722e-03  2.84879852e-03\n",
      "  7.64454064e-02  1.21564874e-03 -1.42316769e-01 -2.14696910e-01\n",
      " -2.68309989e-01 -1.16548221e-01  5.29709023e-01  3.22889776e-02\n",
      " -2.52693940e-01  5.24582983e-02 -5.13735392e-02]\n",
      "Gradient Descent(82/99): loss=0.3429888186288112, weights = [-3.14526313e-01  9.61312716e-03 -2.14989754e-01 -3.26408019e-01\n",
      "  4.85149115e-02 -4.17408296e-01  3.64203065e-01  1.92940544e-01\n",
      "  3.54224424e-01  1.70685501e-04  4.96176581e-02 -1.19274079e-01\n",
      "  1.22043853e-01  2.67746514e-01  2.69467041e-01  2.10347588e-03\n",
      " -1.39328990e-04  2.35313658e-01 -3.66372427e-03  2.83065589e-03\n",
      "  7.68861314e-02  1.21264070e-03 -1.41543755e-01 -2.14776979e-01\n",
      " -2.68300740e-01 -1.16496716e-01  5.29760007e-01  3.22222136e-02\n",
      " -2.52736453e-01  5.24148645e-02 -5.18099908e-02]\n",
      "Gradient Descent(83/99): loss=0.3429093943670056, weights = [-3.14540082e-01  9.87979007e-03 -2.15627490e-01 -3.25224833e-01\n",
      "  4.81356193e-02 -4.17444357e-01  3.64605220e-01  1.92886413e-01\n",
      "  3.52959272e-01  1.61475146e-04  4.91170166e-02 -1.20114214e-01\n",
      "  1.21580112e-01  2.67706747e-01  2.68243464e-01  1.97972971e-03\n",
      " -1.67561382e-04  2.35848367e-01 -3.53755913e-03  2.81427738e-03\n",
      "  7.73197649e-02  1.21005748e-03 -1.40777412e-01 -2.14856216e-01\n",
      " -2.68292833e-01 -1.16447037e-01  5.29809164e-01  3.21559089e-02\n",
      " -2.52778772e-01  5.23716244e-02 -5.22415812e-02]\n",
      "Gradient Descent(84/99): loss=0.342832416619097, weights = [-3.14552473e-01  1.01415420e-02 -2.16248349e-01 -3.24061085e-01\n",
      "  4.77548314e-02 -4.17480248e-01  3.65005127e-01  1.92832501e-01\n",
      "  3.51713458e-01  1.52560357e-04  4.86230489e-02 -1.20944386e-01\n",
      "  1.21136768e-01  2.67667164e-01  2.67039210e-01  1.86155256e-03\n",
      " -1.94011048e-04  2.36376826e-01 -3.41700984e-03  2.79951486e-03\n",
      "  7.77464632e-02  1.20785256e-03 -1.40017681e-01 -2.14934609e-01\n",
      " -2.68286190e-01 -1.16399107e-01  5.29856571e-01  3.20900513e-02\n",
      " -2.52820905e-01  5.23285693e-02 -5.26683043e-02]\n",
      "Gradient Descent(85/99): loss=0.3427577948496514, weights = [-3.14563626e-01  1.03985291e-02 -2.16852904e-01 -3.22916485e-01\n",
      "  4.73726903e-02 -4.17515979e-01  3.65402781e-01  1.92778801e-01\n",
      "  3.50486672e-01  1.43830281e-04  4.81357100e-02 -1.21764668e-01\n",
      "  1.20712916e-01  2.67627757e-01  2.65853945e-01  1.74869577e-03\n",
      " -2.18810228e-04  2.36899132e-01 -3.30182608e-03  2.78623244e-03\n",
      "  7.81663804e-02  1.20598458e-03 -1.39264506e-01 -2.15012147e-01\n",
      " -2.68280737e-01 -1.16352856e-01  5.29902299e-01  3.20246292e-02\n",
      " -2.52862861e-01  5.22856911e-02 -5.30901590e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(86/99): loss=0.34268544284389924, weights = [-3.14573663e-01  1.06508907e-02 -2.17441706e-01 -3.21790747e-01\n",
      "  4.69893332e-02 -4.17551557e-01  3.65798179e-01  1.92725304e-01\n",
      "  3.49278607e-01  1.35186790e-04  4.76549512e-02 -1.22575134e-01\n",
      "  1.20307696e-01  2.67588519e-01  2.64687343e-01  1.64092183e-03\n",
      " -2.42080961e-04  2.37415375e-01 -3.19176870e-03  2.77430543e-03\n",
      "  7.85796685e-02  1.20441679e-03 -1.38517834e-01 -2.15088819e-01\n",
      " -2.68276404e-01 -1.16308214e-01  5.29946417e-01  3.19596316e-02\n",
      " -2.52904647e-01  5.22429824e-02 -5.35071490e-02]\n",
      "Gradient Descent(87/99): loss=0.34261527843617334, weights = [-3.14582697e-01  1.08987598e-02 -2.18015283e-01 -3.20683583e-01\n",
      "  4.66048922e-02 -4.17586988e-01  3.66191320e-01  1.92672004e-01\n",
      "  3.48088967e-01  1.26543295e-04  4.71807205e-02 -1.23375860e-01\n",
      "  1.19920284e-01  2.67549443e-01  2.63539084e-01  1.53800382e-03\n",
      " -2.63935873e-04  2.37925645e-01 -3.08660917e-03  2.76361940e-03\n",
      "  7.89864773e-02  1.20311654e-03 -1.37777612e-01 -2.15164615e-01\n",
      " -2.68273124e-01 -1.16265116e-01  5.29988991e-01  3.18950480e-02\n",
      " -2.52946269e-01  5.22004366e-02 -5.39192821e-02]\n",
      "Gradient Descent(88/99): loss=0.34254722325967996, weights = [-3.14590827e-01  1.11422634e-02 -2.18574146e-01 -3.19594712e-01\n",
      "  4.62194945e-02 -4.17622280e-01  3.66582201e-01  1.92618895e-01\n",
      "  3.46917460e-01  1.17823656e-04  4.67129634e-02 -1.24166922e-01\n",
      "  1.19549897e-01  2.67510522e-01  2.62408852e-01  1.43972496e-03\n",
      " -2.84478910e-04  2.38430029e-01 -2.98612918e-03  2.75406934e-03\n",
      "  7.93869541e-02  1.20205478e-03 -1.37043789e-01 -2.15239526e-01\n",
      " -2.68270832e-01 -1.16223498e-01  5.30030085e-01  3.18308683e-02\n",
      " -2.52987735e-01  5.21580474e-02 -5.43265702e-02]\n",
      "Gradient Descent(89/99): loss=0.34248120251564984, weights = [-3.14598145e-01  1.13815225e-02 -2.19118784e-01 -3.18523851e-01\n",
      "  4.58332625e-02 -4.17657436e-01  3.66970820e-01  1.92565971e-01\n",
      "  3.45763801e-01  1.08961203e-04  4.62516226e-02 -1.24948401e-01\n",
      "  1.19195786e-01  2.67471751e-01  2.61296338e-01  1.34587815e-03\n",
      " -3.03806014e-04  2.38928609e-01 -2.89012013e-03  2.74555891e-03\n",
      "  7.97812435e-02  1.20120575e-03 -1.36316318e-01 -2.15313545e-01\n",
      " -2.68269469e-01 -1.16183300e-01  5.30069760e-01  3.17670832e-02\n",
      " -2.53029048e-01  5.21158093e-02 -5.47290289e-02]\n",
      "Gradient Descent(90/99): loss=0.34241714476007845, weights = [-3.14604730e-01  1.16166530e-02 -2.19649670e-01 -3.17470723e-01\n",
      "  4.54463141e-02 -4.17692463e-01  3.67357175e-01  1.92513226e-01\n",
      "  3.44627712e-01  9.98978375e-05  4.57966387e-02 -1.25720378e-01\n",
      "  1.18857238e-01  2.67433125e-01  2.60201239e-01  1.25626554e-03\n",
      " -3.22005738e-04  2.39421469e-01 -2.79838274e-03  2.73799967e-03\n",
      "  8.01694876e-02  1.20054651e-03 -1.35595149e-01 -2.15386663e-01\n",
      " -2.68268975e-01 -1.16144463e-01  5.30108073e-01  3.17036836e-02\n",
      " -2.53070216e-01  5.20737172e-02 -5.51266773e-02]\n",
      "Gradient Descent(91/99): loss=0.34235498170651557, weights = [-3.14610657e-01  1.18477655e-02 -2.20167257e-01 -3.16435050e-01\n",
      "  4.50587628e-02 -4.17727365e-01  3.67741267e-01  1.92460656e-01\n",
      "  3.43508918e-01  9.05832131e-05  4.53479501e-02 -1.26482935e-01\n",
      "  1.18533571e-01  2.67394639e-01  2.59123255e-01  1.17069809e-03\n",
      " -3.39159825e-04  2.39908686e-01 -2.71072662e-03  2.73131049e-03\n",
      "  8.05518257e-02  1.20005671e-03 -1.34880238e-01 -2.15458875e-01\n",
      " -2.68269298e-01 -1.16106931e-01  5.30145081e-01  3.16406609e-02\n",
      " -2.53111242e-01  5.20317665e-02 -5.55195378e-02]\n",
      "Gradient Descent(92/99): loss=0.3422946480434512, weights = [-3.14615991e-01  1.20749660e-02 -2.20671982e-01 -3.15416561e-01\n",
      "  4.46707179e-02 -4.17762146e-01  3.68123093e-01  1.92408256e-01\n",
      "  3.42407153e-01  8.09739952e-05  4.49054935e-02 -1.27236154e-01\n",
      "  1.18224135e-01  2.67356289e-01  2.58062095e-01  1.08899519e-03\n",
      " -3.55343734e-04  2.40390337e-01 -2.62696986e-03  2.72541691e-03\n",
      "  8.09283945e-02  1.19971826e-03 -1.34171539e-01 -2.15530173e-01\n",
      " -2.68270383e-01 -1.16070652e-01  5.30180837e-01  3.15780072e-02\n",
      " -2.53152130e-01  5.19899531e-02 -5.59076355e-02]\n",
      "Gradient Descent(93/99): loss=0.3422360812650372, weights = [-3.14620792e-01  1.22983558e-02 -2.21164266e-01 -3.14414983e-01\n",
      "  4.42822846e-02 -4.17796810e-01  3.68502655e-01  1.92356024e-01\n",
      "  3.41322153e-01  7.10331829e-05  4.44692040e-02 -1.27980121e-01\n",
      "  1.17928310e-01  2.67318071e-01  2.57017471e-01  1.01098429e-03\n",
      " -3.70627131e-04  2.40866499e-01 -2.54693869e-03  2.72025061e-03\n",
      "  8.12993275e-02  1.19951508e-03 -1.33469009e-01 -2.15600554e-01\n",
      " -2.68272181e-01 -1.16035573e-01  5.30215392e-01  3.15157146e-02\n",
      " -2.53192884e-01  5.19482733e-02 -5.62909984e-02]\n",
      "Gradient Descent(94/99): loss=0.342179221513996, weights = [-3.14625113e-01  1.25180320e-02 -2.21644515e-01 -3.13430050e-01\n",
      "  4.38935642e-02 -4.17831360e-01  3.68879952e-01  1.92303954e-01\n",
      "  3.40253661e-01  6.07294928e-05  4.40390153e-02 -1.28714921e-01\n",
      "  1.17645503e-01  2.67279983e-01  2.55989103e-01  9.36500522e-04\n",
      " -3.85074337e-04  2.41337245e-01 -2.47046710e-03  2.71574888e-03\n",
      "  8.16647558e-02  1.19943288e-03 -1.32772606e-01 -2.15670012e-01\n",
      " -2.68274646e-01 -1.16001646e-01  5.30248796e-01  3.14537759e-02\n",
      " -2.53233508e-01  5.19067240e-02 -5.66696569e-02]\n",
      "Gradient Descent(95/99): loss=0.3421240114356775, weights = [-3.14629002e-01  1.27340875e-02 -2.22113117e-01 -3.12461497e-01\n",
      "  4.35046543e-02 -4.17865799e-01  3.69254985e-01  1.92252046e-01\n",
      "  3.39201422e-01  5.00367984e-05  4.36148599e-02 -1.29440639e-01\n",
      "  1.17375149e-01  2.67242021e-01  2.54976714e-01  8.65386365e-04\n",
      " -3.98744744e-04  2.41802646e-01 -2.39739649e-03  2.71185421e-03\n",
      "  8.20248071e-02  1.19945895e-03 -1.32082286e-01 -2.15738544e-01\n",
      " -2.68277732e-01 -1.15968824e-01  5.30281095e-01  3.13921842e-02\n",
      " -2.53274004e-01  5.18653021e-02 -5.70436438e-02]\n",
      "Gradient Descent(96/99): loss=0.3420703960423103, weights = [-3.14632502e-01  1.29466116e-02 -2.22570450e-01 -3.11509063e-01\n",
      "  4.31156487e-02 -4.17900129e-01  3.69627756e-01  1.92200295e-01\n",
      "  3.38165187e-01  3.89336199e-05  4.31966694e-02 -1.30157363e-01\n",
      "  1.17116709e-01  2.67204182e-01  2.53980033e-01  7.97491318e-04\n",
      " -4.11693198e-04  2.42262773e-01 -2.32757531e-03  2.70851379e-03\n",
      "  8.23796067e-02  1.19958199e-03 -1.31398011e-01 -2.15806146e-01\n",
      " -2.68281397e-01 -1.15937061e-01  5.30312336e-01  3.13309329e-02\n",
      " -2.53314375e-01  5.18240054e-02 -5.74129940e-02]\n",
      "Gradient Descent(97/99): loss=0.342018322586631, weights = [-3.14635651e-01  1.31556897e-02 -2.23016875e-01 -3.10572489e-01\n",
      "  4.27266379e-02 -4.17934353e-01  3.69998265e-01  1.92148699e-01\n",
      "  3.37144713e-01  2.74026592e-05  4.27843742e-02 -1.30865178e-01\n",
      "  1.16869667e-01  2.67166465e-01  2.52998795e-01  7.32671582e-04\n",
      " -4.23970354e-04  2.42717696e-01 -2.26085883e-03  2.70567920e-03\n",
      "  8.27292767e-02  1.19979196e-03 -1.30719739e-01 -2.15872816e-01\n",
      " -2.68285600e-01 -1.15906315e-01  5.30342560e-01  3.12700158e-02\n",
      " -2.53354622e-01  5.17828315e-02 -5.77777441e-02]\n",
      "Gradient Descent(98/99): loss=0.34196774044408534, weights = [-3.14638486e-01  1.33614041e-02 -2.23452741e-01 -3.09651519e-01\n",
      "  4.23377087e-02 -4.17968471e-01  3.70366515e-01  1.92097257e-01\n",
      "  3.36139759e-01  1.54303779e-05  4.23779044e-02 -1.31564174e-01\n",
      "  1.16633531e-01  2.67128867e-01  2.52032739e-01  6.70789764e-04\n",
      " -4.35623000e-04  2.43167482e-01 -2.19710874e-03  2.70330601e-03\n",
      "  8.30739365e-02  1.20007993e-03 -1.30047431e-01 -2.15938552e-01\n",
      " -2.68290303e-01 -1.15876543e-01  5.30371810e-01  3.12094271e-02\n",
      " -2.53394748e-01  5.17417788e-02 -5.81379327e-02]\n",
      "Gradient Descent(99/99): loss=0.3419186010029546, weights = [-3.14641038e-01  1.35638334e-02 -2.23878384e-01 -3.08745903e-01\n",
      "  4.19489449e-02 -4.18002487e-01  3.70732508e-01  1.92045967e-01\n",
      "  3.35150088e-01  3.00661216e-06  4.19771891e-02 -1.32254437e-01\n",
      "  1.16407831e-01  2.67091388e-01  2.51081609e-01  6.11714590e-04\n",
      " -4.46694356e-04  2.43612197e-01 -2.13619293e-03  2.70135344e-03\n",
      "  8.34137023e-02  1.20043792e-03 -1.29381049e-01 -2.16003353e-01\n",
      " -2.68295469e-01 -1.15847707e-01  5.30400125e-01  3.11491611e-02\n",
      " -2.53434755e-01  5.17008456e-02 -5.84935999e-02]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for the best value of gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value of gamma = 0.11 \n",
      " Loss = 0.3460757363978302 \n",
      " Weights = [-3.14656503e-01  6.16895957e-02 -2.85028726e-01 -1.98913340e-01\n",
      " -1.61245582e-01 -3.48413043e-01  4.14373899e-01 -2.95088959e-01\n",
      "  1.92392606e-01 -2.78099576e-02 -1.15812326e-01  1.90781036e-02\n",
      "  1.41255020e-01  1.16666232e-01  3.25509353e-01 -1.48775473e-03\n",
      " -1.40298028e-03  1.48173980e-01 -3.57012278e-04  2.72942006e-03\n",
      "  1.95728761e-01  1.15294756e-03 -7.89628681e-03  4.91044907e-02\n",
      "  2.00423261e-01 -1.62921421e-01  1.28344244e-02 -1.49239111e-01\n",
      "  1.13354268e-01  2.64971753e-01 -9.40252680e-02]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.arange(0.01, 0.2, 0.05)\n",
    "ws = []\n",
    "losses = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    w, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma, printing=False)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "\n",
    "loss_star = np.amin(losses)\n",
    "gamma_star = gammas[np.argmin(losses)]\n",
    "w_star = ws[np.argmin(losses)]\n",
    "\n",
    "plt.plot(gammas, losses, color='k')\n",
    "plt.plot(gamma_star, loss_star, 'r*', markersize=15, label=\"Minimal loss\")\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss in function of gamma\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss_star, we = w_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_TEST_PATH = 'data/test.csv'\n",
    "#_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = 'data/submission.csv'\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "#create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
