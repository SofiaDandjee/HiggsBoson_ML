{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from data_helpers import *\n",
    "from implementations import least_squares_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx, mean, std = standardize(tX,0)\n",
    "y, tx = build_model_data(tx,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = len(y)\n",
    "num_features = tx.shape[1]\n",
    "\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=31.954905906122114, weights = [ 0.1972758  -0.24501573  0.34175867  0.31064042 -0.97461476 -1.04506586\n",
      " -0.59910155 -0.57760759  1.1289179   0.06763225 -1.29115062 -0.02998668\n",
      " -0.37395234 -0.71579926  0.13135611  0.65827779  0.53204799  0.13422538\n",
      "  0.17533282  0.3057555  -0.38608544  0.25136762 -0.70657401 -1.11939262\n",
      " -0.31235269 -0.63861909 -0.70168309 -0.46959651 -1.18466646 -0.68292251\n",
      " -0.46243254]\n",
      "Gradient Descent(1/99): loss=62.368243581988246, weights = [ 0.09488784  0.26211314 -0.29375111  0.04637163  0.76147441  0.95361203\n",
      "  1.35230946  1.42073762  0.02875845  0.80091459  0.60478025  0.02134461\n",
      "  0.84689916  1.28285397  0.81380475  0.51825784  0.44732023  0.63593645\n",
      "  0.09502863  0.26406428  0.63898239  0.21378344  1.07479042  0.9521097\n",
      "  1.39152038  0.98477276  0.92191038  1.53955241  0.81392833  1.31572196\n",
      "  1.50669653]\n",
      "Gradient Descent(2/99): loss=129.24803894838905, weights = [ 0.01297747 -0.49289185  0.40148268  0.0385686  -1.65129285 -1.98012741\n",
      " -1.48500998 -1.5129688   1.31740374 -0.43208304 -2.17122224 -0.18878528\n",
      " -0.84505633 -1.65096403 -0.20008895  0.38478992  0.35417714 -0.27297663\n",
      " -0.02512508  0.23283448 -0.79528425  0.14430495 -1.53968304 -2.09474768\n",
      " -1.08230137 -1.38113256 -1.44385972 -1.40943789 -2.11991352 -1.61805941\n",
      " -1.33388933]\n",
      "Gradient Descent(3/99): loss=271.61201083548667, weights = [-5.25508236e-02  6.03638048e-01 -7.54275669e-01 -1.96489585e-01\n",
      "  1.93470260e+00  2.26854342e+00  2.64555794e+00  2.73527641e+00\n",
      " -7.58645420e-01  1.22581211e+00  1.85604061e+00  5.95002571e-02\n",
      "  1.65640107e+00  2.59772584e+00  1.25203307e+00  3.35993740e-01\n",
      "  3.12307935e-01  9.68367565e-01 -2.15832746e-03  1.96034720e-01\n",
      "  1.32514852e+00  1.47044158e-01  2.24221363e+00  2.30490822e+00\n",
      "  2.50507993e+00  2.04234377e+00  1.97975038e+00  2.86125159e+00\n",
      "  2.12873103e+00  2.63059636e+00  2.80875172e+00]\n",
      "Gradient Descent(4/99): loss=572.8427952080524, weights = [-0.10497346 -0.98085136  0.82860479 -0.01203994 -3.21924846 -3.91488121\n",
      " -3.34794595 -3.44785906  2.12325162 -1.29269772 -3.99864028 -0.31996229\n",
      " -1.95946287 -3.58580919 -0.87672599  0.22993399  0.22783404 -0.86321993\n",
      " -0.12954118  0.17671301 -1.7339294   0.06553232 -3.26842231 -4.11252787\n",
      " -2.72076422 -2.95070267 -3.01323392 -3.35440621 -4.05479862 -3.55287523\n",
      " -3.20353877]\n",
      "Gradient Descent(5/99): loss=1209.437622300776, weights = [-1.46911567e-01  1.33659424e+00 -1.53994805e+00 -3.81969530e-01\n",
      "  4.31549781e+00  5.06811150e+00  5.37467064e+00  5.53441733e+00\n",
      " -2.15796925e+00  2.27906227e+00  4.51245101e+00  2.30708967e-01\n",
      "  3.30328778e+00  5.39726971e+00  2.20301559e+00  2.53259946e-01\n",
      "  2.31359597e-01  1.79804389e+00  3.71486831e-04  1.40809390e-01\n",
      "  2.72705927e+00  1.22971375e-01  4.72984950e+00  5.19713070e+00\n",
      "  4.86280024e+00  4.29034916e+00  4.22792828e+00  5.67515330e+00\n",
      "  4.92822529e+00  5.43012023e+00  5.54038344e+00]\n",
      "Gradient Descent(6/99): loss=2554.3872572035066, weights = [-0.18046205 -2.01521358  1.85293413  0.08868225 -6.6023015  -7.99054435\n",
      " -7.29172911 -7.52346912  3.99990298 -2.98495577 -7.85540459 -0.56341292\n",
      " -4.34580814 -7.6615796  -2.28414677  0.12108088  0.12828032 -2.0582779\n",
      " -0.21785309  0.13711193 -3.7469424  -0.00846366 -6.90474618 -8.34782164\n",
      " -6.17033964 -6.24812156 -6.31054482 -7.45149754 -8.13058211 -7.62860365\n",
      " -7.16554977]\n",
      "Gradient Descent(7/99): loss=5395.701331915862, weights = [-0.20730244  2.87309812 -3.11621433 -0.64018874  9.29254213 10.9883561\n",
      " 11.12948987 11.45405975 -4.99663691  4.60655512 10.12345353  0.59895244\n",
      "  6.76775406 11.31753846  4.22952114  0.23803929  0.19754833  3.56360729\n",
      "  0.08692035  0.09393095  5.66966279  0.14496996  9.99726702 11.32852881\n",
      "  9.85664532  9.0574989   8.99520433 11.62590464 10.84844336 11.35033222\n",
      " 11.30325924]\n",
      "Gradient Descent(8/99): loss=11398.096924326817, weights = [-2.28774754e-01 -4.21614195e+00  4.07666237e+00  3.88801453e-01\n",
      " -1.37888911e+01 -1.65973244e+01 -1.56342220e+01 -1.61298660e+01\n",
      "  8.04535575e+00 -6.47609064e+00 -1.60055051e+01 -1.08285272e+00\n",
      " -9.39165098e+00 -1.62685182e+01 -5.24402549e+00  9.45259838e-03\n",
      "  3.05462993e-02 -4.58922427e+00 -3.57471849e-01  1.14680959e-01\n",
      " -8.01163101e+00 -1.07472303e-01 -1.45765298e+01 -1.72781912e+01\n",
      " -1.34453786e+01 -1.31978436e+01 -1.32602254e+01 -1.61032720e+01\n",
      " -1.67375060e+01 -1.62354630e+01 -1.55400716e+01]\n",
      "Gradient Descent(9/99): loss=24078.34606243266, weights = [ -0.2459526    6.10201195  -6.40152449  -1.1276761   19.77586202\n",
      "  23.49667638  23.27567729  23.96135722 -10.93619806   9.59211625\n",
      "  21.97374562   1.36774324  14.08768196  23.82597233   8.52048836\n",
      "   0.29509824   0.21859712   7.27842254   0.29393044   0.04830037\n",
      "  11.87789618   0.23644143  21.13345283  24.29431497  20.41693775\n",
      "  19.14184614  19.07967109  24.19896946  23.35680444  23.85864123\n",
      "  23.47493301]\n",
      "Gradient Descent(10/99): loss=50865.730682720736, weights = [ -0.25969488  -8.88225778   8.80871507   1.06193806 -28.99505136\n",
      " -34.77850457 -33.26852626 -34.31000419  16.63289501 -13.79490702\n",
      " -33.22578577  -2.18963431 -20.04664267 -34.44997515 -11.48950216\n",
      "  -0.1578636   -0.09986271  -9.95400787  -0.64317825   0.11364589\n",
      " -17.02667803  -0.28135943 -30.77573561 -36.13379239 -28.80521395\n",
      " -27.86820059 -27.93061179 -34.37919558 -34.91890312 -34.41675353\n",
      " -33.23259242]\n",
      "Gradient Descent(11/99): loss=107454.81212548046, weights = [-2.70688706e-01  1.29077988e+01 -1.33146485e+01 -2.13073150e+00\n",
      "  4.19027031e+01  4.99213466e+01  4.89248399e+01  5.03840861e+01\n",
      " -2.34529688e+01  2.01703938e+01  4.70052428e+01  2.98365024e+00\n",
      "  2.95576087e+01  5.02509377e+01  1.75907798e+01  4.69266882e-01\n",
      "  3.25696034e-01  1.51076484e+01  7.30882056e-01 -8.02573647e-03\n",
      "  2.49884530e+01  4.57276866e-01  4.46658728e+01  5.16925400e+01\n",
      "  4.27330585e+01  4.04542014e+01  4.03921855e+01  5.07607239e+01\n",
      "  4.97816407e+01  5.02833451e+01  4.91877825e+01]\n",
      "Gradient Descent(12/99): loss=227000.77502101642, weights = [ -0.27948376 -18.75336816  18.82723999   2.50251345 -61.13426076\n",
      " -73.18638037 -70.53085912 -72.71547017  34.79760431 -29.21831089\n",
      " -69.60614807  -4.5341194  -42.54819652 -72.85838304 -24.67899115\n",
      "  -0.4684935   -0.32370942 -21.30470919  -1.25310453   0.14598148\n",
      " -36.07504056  -0.62714204 -64.99061445 -75.96101121 -61.24749915\n",
      " -58.85269514 -58.91524332 -72.98691481 -73.32716282 -72.82480475\n",
      " -70.60798864]\n",
      "Gradient Descent(13/99): loss=479544.83897105773, weights = [-2.86519812e-01  2.72730323e+01 -2.79008246e+01 -4.23671962e+00\n",
      "  8.86332863e+01  1.05744487e+02  1.03100552e+02  1.06203325e+02\n",
      " -4.98769286e+01  4.25482035e+01  9.98834035e+01  6.39276265e+00\n",
      "  6.22461221e+01  1.06074752e+02  3.67554329e+01  8.72082982e-01\n",
      "  5.94587602e-01  3.16311352e+01  1.64401288e+00 -9.73594494e-02\n",
      "  5.26813577e+01  9.40606437e-01  9.43846769e+01  1.09576247e+02\n",
      "  8.98810429e+01  8.54826160e+01  8.54208758e+01  1.06873754e+02\n",
      "  1.05605202e+02  1.06106615e+02  1.03507850e+02]\n",
      "Gradient Descent(14/99): loss=1013050.962970081, weights = [  -0.29214865  -39.61695941   40.00657436    5.55493157 -129.03995938\n",
      " -154.32372109 -149.25686534 -153.847528     73.18529647  -61.77541658\n",
      " -146.46172688   -9.48977258  -90.07532606 -153.99679959  -52.53924565\n",
      "   -1.09577158   -0.76133761  -45.29772388   -2.55349177    0.23970491\n",
      "  -76.31855526   -1.34453782 -137.26490469 -160.09408099 -129.77909477\n",
      " -124.30402178 -124.36690827 -154.54614519 -154.46524786 -153.96245843\n",
      " -149.56323025]\n",
      "Gradient Descent(15/99): loss=2140096.986202713, weights = [  -0.29665172   57.61079364  -58.70198489   -8.67919126  187.34389132\n",
      "  223.67273924  217.54032171  224.12352067 -105.68680708   89.84251284\n",
      "  211.58925516   13.59315881  131.30940168  224.00447336   77.24385924\n",
      "    1.7474554     1.19181095   66.52463159    3.56023098   -0.26434853\n",
      "  111.1801084     1.97180821  199.42222062  231.85915488  189.48524794\n",
      "  180.60961189  180.54841454  225.41467915  223.53440686  224.03519588\n",
      "  218.2619406 ]\n",
      "Gradient Descent(16/99): loss=4521011.992181172, weights = [-3.00254176e-01 -8.36997810e+01  8.47590979e+01  1.20078295e+01\n",
      " -2.72500181e+02 -3.25728119e+02 -3.15575155e+02 -3.25240586e+02\n",
      "  1.54289655e+02 -1.30536430e+02 -3.08821547e+02 -1.99590034e+01\n",
      " -1.90469978e+02 -3.25403426e+02 -1.11392374e+02 -2.40000891e+00\n",
      " -1.66165257e+00 -9.59950378e+01 -5.31359140e+00  4.56245080e-01\n",
      " -1.61337261e+02 -2.85219004e+00 -2.89941343e+02 -3.37826214e+02\n",
      " -2.74551731e+02 -2.62569231e+02 -2.62632865e+02 -3.26841535e+02\n",
      " -3.25871157e+02 -3.25367460e+02 -3.16356685e+02]\n",
      "Gradient Descent(17/99): loss=9550758.910821445, weights = [-3.03136141e-01  1.21693291e+02 -1.23761201e+02 -1.80608179e+01\n",
      "  3.95866218e+02  4.72799800e+02  4.59290266e+02  4.73233738e+02\n",
      " -2.23578973e+02  1.89766602e+02  4.47570800e+02  2.88048424e+01\n",
      "  2.77214802e+02  4.73134683e+02  1.62779069e+02  3.61483242e+00\n",
      "  2.47353003e+00  1.40227865e+02  7.59553400e+00 -6.01313789e-01\n",
      "  2.34757145e+02  4.15632566e+00  4.21321529e+02  4.90185690e+02\n",
      "  3.99903521e+02  3.81569807e+02  3.81509730e+02  4.75836278e+02\n",
      "  4.72663541e+02  4.73163010e+02  4.60684728e+02]\n",
      "Gradient Descent(18/99): loss=20176234.409572445, weights = [-3.05441712e-01 -1.76831837e+02  1.79307806e+02  2.56419682e+01\n",
      " -5.75569311e+02 -6.87824011e+02 -6.66933853e+02 -6.87312350e+02\n",
      "  3.25631227e+02 -2.75784924e+02 -6.51810874e+02 -4.20741462e+01\n",
      " -4.02549382e+02 -6.87503981e+02 -2.35718901e+02 -5.13935552e+00\n",
      " -3.54692171e+00 -2.03103653e+02 -1.11566300e+01  9.27138886e-01\n",
      " -3.40944641e+02 -6.03246638e+00 -6.12469990e+02 -7.13290020e+02\n",
      " -5.80386479e+02 -5.54656821e+02 -5.54722054e+02 -6.90819434e+02\n",
      " -6.87970182e+02 -6.87464571e+02 -6.68710613e+02]\n",
      "Gradient Descent(19/99): loss=42622836.871483, weights = [-3.07286170e-01  2.57064163e+02 -2.61193780e+02 -3.78782681e+01\n",
      "  8.36370432e+02  9.99087953e+02  9.69986938e+02  9.99486474e+02\n",
      " -4.72623909e+02  4.00867789e+02  9.46088486e+02  6.09418920e+01\n",
      "  5.85450482e+02  9.99429528e+02  3.43476785e+02  7.57376431e+00\n",
      "  5.19505788e+00  2.95919629e+02  1.61086246e+01 -1.30173476e+00\n",
      "  4.95813955e+02  8.77476927e+00  8.90093982e+02  1.03590713e+03\n",
      "  8.44418817e+02  8.06104625e+02  8.06046896e+02  1.00485940e+03\n",
      "  9.98956129e+02  9.99452811e+02  9.72811116e+02]\n",
      "Gradient Descent(20/99): loss=90041887.6358147, weights = [-3.08761736e-01 -3.73580266e+02  3.79050400e+02  5.44454010e+01\n",
      " -1.21581517e+03 -1.45276074e+03 -1.40919489e+03 -1.45219794e+03\n",
      "  6.87599530e+02 -5.82619153e+02 -1.37638457e+03 -8.87905823e+01\n",
      " -8.50566625e+02 -1.45245052e+03 -4.98359731e+02 -1.09137933e+01\n",
      " -7.51806576e+00 -4.29381036e+02 -2.35110656e+01  1.93161480e+00\n",
      " -7.20372964e+02 -1.27481392e+01 -1.29381644e+03 -1.50646803e+03\n",
      " -1.22646979e+03 -1.17169898e+03 -1.17176761e+03 -1.45973173e+03\n",
      " -1.45291347e+03 -1.45240381e+03 -1.41306603e+03]\n",
      "Gradient Descent(21/99): loss=190215906.36248636, weights = [-3.09942189e-01  5.43034874e+02 -5.51518602e+02 -7.97426157e+01\n",
      "  1.76694376e+03  2.11088642e+03  2.04884124e+03  2.11121028e+03\n",
      " -9.98734045e+02  8.46830402e+02  1.99922106e+03  1.28835001e+02\n",
      "  1.23661243e+03  2.11124217e+03  7.25207877e+02  1.59483079e+01\n",
      "  1.09539704e+01  6.24815732e+02  3.40826612e+01 -2.77316690e+00\n",
      "  1.04730068e+03  1.85334463e+01  1.88039217e+03  2.18875744e+03\n",
      "  1.78346997e+03  1.70294633e+03  1.70289355e+03  2.12243578e+03\n",
      "  2.11076402e+03  2.11125481e+03  2.05469357e+03]\n",
      "Gradient Descent(22/99): loss=401836212.25366306, weights = [-3.10886551e-01 -7.89220025e+02  8.01016592e+02  1.15293679e+02\n",
      " -2.56835498e+03 -3.06870949e+03 -2.97724723e+03 -3.06803852e+03\n",
      "  1.45227075e+03 -1.23081019e+03 -2.90706534e+03 -1.87477333e+02\n",
      " -1.79700969e+03 -3.06841996e+03 -1.05319321e+03 -2.31023861e+01\n",
      " -1.58992002e+01 -9.07404489e+02 -4.96194789e+01  4.06056705e+00\n",
      " -1.52192818e+03 -2.69335945e+01 -2.73317551e+03 -3.18207899e+03\n",
      " -2.59133762e+03 -2.47521680e+03 -2.47529261e+03 -3.08407879e+03\n",
      " -3.06887604e+03 -3.06835783e+03 -2.98553516e+03]\n",
      "Gradient Descent(23/99): loss=848889793.9574071, weights = [-3.11642041e-01  1.14715267e+03 -1.16483350e+03 -1.68182142e+02\n",
      "  3.73280204e+03  4.45959119e+03  4.32794414e+03  4.45975749e+03\n",
      " -2.11015282e+03  1.78894196e+03  4.22399262e+03  2.72264175e+02\n",
      "  2.61221454e+03  4.45997693e+03  1.53162741e+03  3.36488310e+01\n",
      "  2.31265147e+01  1.31961163e+03  7.20446961e+01 -5.87570575e+00\n",
      "  2.21232864e+03  3.91501067e+01  3.97242710e+03  4.62418341e+03\n",
      "  3.76724117e+03  3.59754872e+03  3.59750639e+03  4.48334673e+03\n",
      "  4.45948876e+03  4.45996712e+03  4.34020097e+03]\n",
      "Gradient Descent(24/99): loss=1793302496.0958543, weights = [-3.12246433e-01 -1.66727285e+03  1.69243517e+03  2.43837172e+02\n",
      " -5.42563422e+03 -6.48244399e+03 -6.28980557e+03 -6.48154433e+03\n",
      "  3.06766191e+03 -2.60012841e+03 -6.14066884e+03 -3.95952464e+02\n",
      " -3.79639145e+03 -6.48219812e+03 -2.22529150e+03 -4.88429107e+01\n",
      " -3.35989883e+01 -1.91724745e+03 -1.04782104e+02  8.56302861e+00\n",
      " -3.21523753e+03 -5.68998578e+01 -5.77385699e+03 -6.72185358e+03\n",
      " -5.47465723e+03 -5.22893311e+03 -5.22902410e+03 -6.51555476e+03\n",
      " -6.48263966e+03 -6.48210339e+03 -6.30741688e+03]\n",
      "Gradient Descent(25/99): loss=3788399702.4779453, weights = [-3.12729946e-01  2.42336538e+03 -2.46047439e+03 -3.55013262e+02\n",
      "  7.88572819e+03  9.42129380e+03  9.14260474e+03  9.42112739e+03\n",
      " -4.45805248e+03  3.77918006e+03  8.92388306e+03  5.75265524e+02\n",
      "  5.51821527e+03  9.42174292e+03  3.23521266e+03  7.10490952e+01\n",
      "  4.88460070e+01  2.78738138e+03  1.52233273e+02 -1.24256807e+01\n",
      "  4.67347918e+03  8.27040177e+01  8.39191019e+03  9.76908469e+03\n",
      "  7.95801142e+03  7.59994723e+03  7.59992696e+03  9.47083539e+03\n",
      "  9.42123358e+03  9.42168568e+03  9.16839827e+03]\n",
      "Gradient Descent(26/99): loss=8003096152.346008, weights = [-3.13116757e-01 -3.52218515e+03  3.57558384e+03  5.15388427e+02\n",
      " -1.14617155e+04 -1.36940492e+04 -1.32876794e+04 -1.36926662e+04\n",
      "  6.48022124e+03 -5.49284831e+03 -1.29717423e+04 -8.36358356e+02\n",
      " -8.02013515e+03 -1.36938955e+04 -4.70137830e+03 -1.03213716e+02\n",
      " -7.09863660e+01 -4.05057282e+03 -2.21321259e+02  1.80781674e+01\n",
      " -6.79240122e+03 -1.20203907e+02 -1.21973754e+04 -1.41997241e+04\n",
      " -1.15657471e+04 -1.10462328e+04 -1.10463559e+04 -1.37646391e+04\n",
      " -1.36943063e+04 -1.36937319e+04 -1.33249797e+04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(27/99): loss=16906755637.059944, weights = [-3.13426206e-01  5.11939712e+03 -5.19754790e+03 -7.49699407e+02\n",
      "  1.66588944e+04  1.99030245e+04  1.93136994e+04  1.99021554e+04\n",
      " -9.41805170e+03  7.98361273e+03  1.88525287e+04  1.21536747e+03\n",
      "  1.16572236e+04  1.99036076e+04  6.83408461e+03  1.50064283e+02\n",
      "  1.03182388e+02  5.88807967e+03  3.21627736e+02 -2.62596651e+01\n",
      "  9.87272378e+03  1.74713178e+02  1.77281897e+04  2.06378252e+04\n",
      "  1.68111264e+04  1.60551216e+04  1.60551479e+04  2.00070400e+04\n",
      "  1.99030536e+04  1.99034502e+04  1.93680962e+04]\n",
      "Gradient Descent(28/99): loss=35715975509.16188, weights = [-3.13673764e-01 -7.44073891e+03  7.55378848e+03  1.08904733e+03\n",
      " -2.42131009e+04 -2.89287582e+04 -2.80708811e+04 -2.89263542e+04\n",
      "  1.36893462e+04 -1.16037960e+04 -2.74025679e+04 -1.76672418e+03\n",
      " -1.69429038e+04 -2.89287992e+04 -9.93217590e+03 -2.18067937e+02\n",
      " -1.49965476e+02 -8.55728450e+03 -4.67518890e+02  3.81816986e+01\n",
      " -1.43492576e+04 -2.53935160e+02 -2.57672279e+04 -2.99969282e+04\n",
      " -2.44333379e+04 -2.33354347e+04 -2.33356255e+04 -2.90785237e+04\n",
      " -2.89291451e+04 -2.89284901e+04 -2.81497685e+04]\n",
      "Gradient Descent(29/99): loss=75450957827.8883, weights = [-3.13871812e-01  1.08148351e+04 -1.09796870e+04 -1.58348494e+03\n",
      "  3.51924421e+04  4.20459630e+04  4.08004050e+04  4.20436096e+04\n",
      " -1.98961818e+04  1.68655903e+04  3.98270600e+04  2.56760347e+03\n",
      "  2.46260467e+04  4.20468291e+04  1.44368005e+04  3.16991055e+02\n",
      "  2.17971737e+02  1.24383838e+04  6.79473038e+02 -5.54821809e+01\n",
      "  2.08562658e+04  3.69085180e+02  3.74513336e+04  4.35983297e+04\n",
      "  3.55135699e+04  3.39169044e+04  3.39170292e+04  4.22650562e+04\n",
      "  4.20461806e+04  4.20464601e+04  4.09152336e+04]\n",
      "Gradient Descent(30/99): loss=159392175518.22443, weights = [-3.14030249e-01 -1.57187904e+04  1.59578547e+04  2.30091671e+03\n",
      " -5.11507468e+04 -6.11124895e+04 -5.93007968e+04 -6.11079285e+04\n",
      "  2.89188178e+04 -2.45133367e+04 -5.78880738e+04 -3.73214353e+03\n",
      " -3.57924893e+04 -6.11129420e+04 -2.09823741e+04 -4.60695954e+02\n",
      " -3.16809054e+02 -1.80778382e+04 -9.87622609e+02  8.06527603e+01\n",
      " -3.03133213e+04 -5.36445902e+02 -5.44339035e+04 -6.33689481e+04\n",
      " -5.16164690e+04 -4.92967041e+04 -4.92970380e+04 -6.14295154e+04\n",
      " -6.11131507e+04 -6.11123253e+04 -5.94675316e+04]\n",
      "Gradient Descent(31/99): loss=336720253100.27167, weights = [-3.14157000e-01  2.28465995e+04 -2.31946092e+04 -3.34487973e+03\n",
      "  7.43450582e+04  8.88235162e+04  8.61916431e+04  8.88180272e+04\n",
      " -4.20315117e+04  3.56290067e+04  8.41363217e+04  5.42424176e+03\n",
      "  5.20230367e+04  8.88249801e+04  3.04977444e+04  6.69632539e+02\n",
      "  4.60468925e+02  2.62760732e+04  1.43542665e+03 -1.17214002e+02\n",
      "  4.40592920e+04  7.79701336e+02  7.91170073e+04  9.21030106e+04\n",
      "  7.50229844e+04  7.16504005e+04  7.16507333e+04  8.92857143e+04\n",
      "  8.88241322e+04  8.88241641e+04  8.64341413e+04]\n",
      "Gradient Descent(32/99): loss=711330581186.6128, weights = [-3.14258399e-01 -3.32063977e+04  3.37116722e+04  4.86102289e+03\n",
      " -1.08057249e+05 -1.29101486e+05 -1.25274837e+05 -1.29092368e+05\n",
      "  6.10914869e+04 -5.17850882e+04 -1.22289523e+05 -7.88414136e+03\n",
      " -7.56127427e+04 -1.29102807e+05 -4.43262126e+04 -9.73250340e+02\n",
      " -6.69269251e+02 -3.81902675e+04 -2.08635952e+03  1.70375308e+02\n",
      " -6.40378365e+04 -1.13325738e+03 -1.14993024e+05 -1.33868239e+05\n",
      " -1.09041564e+05 -1.04140583e+05 -1.04141219e+05 -1.29771854e+05\n",
      " -1.29102726e+05 -1.29101541e+05 -1.25627146e+05]\n",
      "Gradient Descent(33/99): loss=1502704963757.4302, weights = [-3.14339520e-01  4.82640241e+04 -4.89989589e+04 -7.06587399e+03\n",
      "  1.57056010e+05  1.87642355e+05  1.82081848e+05  1.87630242e+05\n",
      " -8.87929899e+04  7.52672279e+04  1.77740828e+05  1.14589702e+04\n",
      "  1.09899916e+05  1.87645082e+05  6.44269220e+04  1.41460063e+03\n",
      "  9.72751874e+02  5.55085601e+04  3.03239547e+03 -2.47623046e+02\n",
      "  9.30763066e+04  1.64713903e+03  1.67136864e+05  1.94570450e+05\n",
      "  1.58487684e+05  1.51363425e+05  1.51364197e+05  1.88618118e+05\n",
      "  1.87643812e+05  1.87643321e+05  1.82594058e+05]\n",
      "Gradient Descent(34/99): loss=3174504608440.413, weights = [-3.14404415e-01 -7.01494390e+04  7.12170900e+04  1.02693155e+04\n",
      " -2.28273764e+05 -2.72730067e+05 -2.64646775e+05 -2.72711322e+05\n",
      "  1.29057116e+05 -1.09397396e+05 -2.58339315e+05 -1.66553450e+04\n",
      " -1.59734096e+05 -2.72733224e+05 -9.36406963e+04 -2.05603208e+03\n",
      " -1.41384994e+03 -8.06783183e+04 -4.40747396e+03  3.59917518e+02\n",
      " -1.35281782e+05 -2.39403766e+03 -2.42925790e+05 -2.82799886e+05\n",
      " -2.30353618e+05 -2.19999751e+05 -2.20001026e+05 -2.74146879e+05\n",
      " -2.72732531e+05 -2.72730586e+05 -2.65391109e+05]\n",
      "Gradient Descent(35/99): loss=6706226273327.777, weights = [-3.14456333e-01  1.01959015e+05 -1.03511339e+05 -1.49265747e+04\n",
      "  3.31785118e+05  3.96399807e+05  3.84652479e+05  3.96373701e+05\n",
      " -1.87577868e+05  1.59004035e+05  3.75482863e+05  2.42074995e+04\n",
      "  2.32166382e+05  3.96405202e+05  1.36103223e+05  2.98836862e+03\n",
      "  2.05496416e+03  1.17262970e+05  6.40603209e+03 -5.23114884e+02\n",
      "  1.96626066e+05  3.47962446e+03  3.53081179e+05  4.11035668e+05\n",
      "  3.34809105e+05  3.19759330e+05  3.19761030e+05  3.98460492e+05\n",
      "  3.96403043e+05  3.96401447e+05  3.85734467e+05]\n",
      "Gradient Descent(36/99): loss=14167083175591.338, weights = [-3.14497865e-01 -1.48192607e+05  1.50448294e+05  2.16944787e+04\n",
      " -4.82234383e+05 -5.76149302e+05 -5.59073743e+05 -5.76110220e+05\n",
      "  2.72636334e+05 -2.31104945e+05 -5.45748155e+05 -3.51847453e+04\n",
      " -3.37442712e+05 -5.76156338e+05 -1.97818866e+05 -4.34343497e+03\n",
      " -2.98679600e+03 -1.70435471e+05 -9.31089323e+03  7.60331177e+02\n",
      " -2.85786532e+05 -5.05746987e+03 -5.13187195e+05 -5.97421994e+05\n",
      " -4.86628593e+05 -4.64755357e+05 -4.64757981e+05 -5.79142998e+05\n",
      " -5.76154351e+05 -5.76150800e+05 -5.60646235e+05]\n",
      "Gradient Descent(37/99): loss=29928343829133.863, weights = [-3.14531095e-01  2.15391127e+05 -2.18670207e+05 -3.15325166e+04\n",
      "  7.00905052e+05  8.37405533e+05  8.12588373e+05  8.37349867e+05\n",
      " -3.96263576e+05  3.35900289e+05  7.93218226e+05  5.11391132e+04\n",
      "  4.90457576e+05  8.37416565e+05  2.87521337e+05  6.31299817e+03\n",
      "  4.34116724e+03  2.47720818e+05  1.35329277e+04 -1.10509824e+03\n",
      "  4.15377713e+05  7.35079915e+03  7.45893541e+05  8.68324253e+05\n",
      "  7.07292845e+05  6.75500203e+05  6.75503865e+05  8.41758144e+05\n",
      "  8.37412526e+05  8.37408596e+05  8.14874039e+05]\n",
      "Gradient Descent(38/99): loss=63224430410494.6, weights = [-3.14557671e-01 -3.13060897e+05  3.17826343e+05  4.58304425e+04\n",
      " -1.01873301e+06 -1.21713059e+06 -1.18105863e+06 -1.21704854e+06\n",
      "  5.75951290e+05 -4.88215411e+05 -1.15290705e+06 -7.43285980e+04\n",
      " -7.12856937e+05 -1.21714582e+06 -4.17898047e+05 -9.17563286e+03\n",
      " -6.30968634e+03 -3.60049870e+05 -1.96695004e+04  1.60621625e+03\n",
      " -6.03731862e+05 -1.06840420e+04 -1.08412167e+06 -1.26206965e+06\n",
      " -1.02801636e+06 -9.81808149e+05 -9.81813623e+05 -1.22345549e+06\n",
      " -1.21714110e+06 -1.21713415e+06 -1.18438062e+06]\n",
      "Gradient Descent(39/99): loss=133563307864710.31, weights = [-3.14578944e-01  4.55019509e+05 -4.61946407e+05 -6.66130153e+04\n",
      "  1.48068083e+06  1.76904198e+06  1.71661444e+06  1.76892387e+06\n",
      " -8.37117740e+05  7.09598379e+05  1.67569540e+06  1.08032874e+05\n",
      "  1.03610467e+06  1.76906492e+06  6.07396173e+05  1.33363694e+04\n",
      "  9.17083307e+03  5.23316523e+05  2.85886841e+04 -2.33455334e+03\n",
      "  8.77496443e+05  1.55287597e+04  1.57572021e+06  1.83435867e+06\n",
      "  1.49417475e+06  1.42701236e+06  1.42702016e+06  1.77823635e+06\n",
      "  1.76905691e+06  1.76904805e+06  1.72144292e+06]\n",
      "Gradient Descent(40/99): loss=282156076249949.25, weights = [-3.14595946e-01 -6.61349589e+05  6.71416968e+05  9.68183141e+04\n",
      " -2.15210079e+06 -2.57122078e+06 -2.49501841e+06 -2.57104797e+06\n",
      "  1.21671228e+06 -1.03136818e+06 -2.43554653e+06 -1.57021035e+05\n",
      " -1.50592957e+06 -2.57125331e+06 -8.82821228e+05 -1.93837789e+04\n",
      " -1.33293811e+04 -7.60615386e+05 -4.15523380e+04  3.39317153e+03\n",
      " -1.27539991e+06 -2.25703267e+04 -2.29023609e+06 -2.66615569e+06\n",
      " -2.17171257e+06 -2.07409606e+06 -2.07410756e+06 -2.58458295e+06\n",
      " -2.57124282e+06 -2.57122871e+06 -2.50203626e+06]\n",
      "Gradient Descent(41/99): loss=596062291639315.6, weights = [-3.14609570e-01  9.61240903e+05 -9.75873916e+05 -1.40721512e+05\n",
      "  3.12797810e+06  3.73714900e+06  3.62639380e+06  3.73689897e+06\n",
      " -1.76843400e+06  1.49904563e+06  3.53995215e+06  2.28222474e+05\n",
      "  2.18879880e+06  3.73719710e+06  1.28314036e+06  2.81734315e+04\n",
      "  1.93736321e+04  1.10551985e+06  6.03943723e+04 -4.93180999e+03\n",
      "  1.85373468e+06  3.28049210e+04  3.32875135e+06  3.87513234e+06\n",
      "  3.15648392e+06  3.01460197e+06  3.01461853e+06  3.75657171e+06\n",
      "  3.73718070e+06  3.73716142e+06  3.63659402e+06]\n",
      "Gradient Descent(42/99): loss=1259197605227487.2, weights = [-3.14620436e-01 -1.39711881e+06  1.41838667e+06  2.04531562e+05\n",
      " -4.54637066e+06 -5.43177268e+06 -5.27079375e+06 -5.43140813e+06\n",
      "  2.57033710e+06 -2.17879300e+06 -5.14515689e+06 -3.31711029e+05\n",
      " -3.18131682e+06 -5.43184177e+06 -1.86498394e+06 -4.09487602e+04\n",
      " -2.81586747e+04 -1.60682080e+06 -8.77804221e+04  7.16816278e+03\n",
      " -2.69431672e+06 -4.76804226e+04 -4.83818521e+06 -5.63232516e+06\n",
      " -4.58780153e+06 -4.38158363e+06 -4.38160785e+06 -5.46000126e+06\n",
      " -5.43181909e+06 -5.43178984e+06 -5.28561921e+06]\n",
      "Gradient Descent(43/99): loss=2660088771342851.0, weights = [-3.14629178e-01  2.03064720e+06 -2.06155958e+06 -2.97277701e+05\n",
      "  6.60793786e+06  7.89482796e+06  7.66085400e+06  7.89429925e+06\n",
      " -3.73586464e+06  3.16677417e+06  7.47824461e+06  4.82126238e+05\n",
      "  4.62389614e+06  7.89492920e+06  2.71066811e+06  5.95171251e+04\n",
      "  4.09273195e+04  2.33544002e+06  1.27584742e+05 -1.04185842e+04\n",
      "  3.91606417e+06  6.93012760e+04  7.03207672e+06  8.18632153e+06\n",
      "  6.66815671e+06  6.36842768e+06  6.36846273e+06  7.93585831e+06\n",
      "  7.89489507e+06  7.89485379e+06  7.68240219e+06]\n",
      "Gradient Descent(44/99): loss=5619508996878954.0, weights = [-3.14636101e-01 -2.95145105e+06  2.99638022e+06  4.32078693e+05\n",
      " -9.60433009e+06 -1.14747654e+07 -1.11346937e+07 -1.14739958e+07\n",
      "  5.42990589e+06 -4.60275875e+06 -1.08692816e+07 -7.00748329e+05\n",
      " -6.72061739e+06 -1.14749117e+07 -3.93982904e+06 -8.65053612e+04\n",
      " -5.94859557e+04 -3.39445248e+06 -1.85438493e+05  1.51429334e+04\n",
      " -5.69181659e+06 -1.00726175e+05 -1.02207962e+07 -1.18984379e+07\n",
      " -9.69185438e+06 -9.25621311e+06 -9.25626420e+06 -1.15343997e+07\n",
      " -1.14748633e+07 -1.14748021e+07 -1.11660130e+07]\n",
      "Gradient Descent(45/99): loss=1.1871363732745738e+16, weights = [-3.14641741e-01  4.28979673e+06 -4.35509973e+06 -6.28006858e+05\n",
      "  1.39594463e+07  1.66780365e+07  1.61837596e+07  1.66769190e+07\n",
      " -7.89211470e+06  6.68989550e+06  1.57979932e+07  1.01850476e+06\n",
      "  9.76810468e+06  1.66782500e+07  5.72635884e+06  1.25731521e+05\n",
      "  8.64600633e+04  4.93367942e+06  2.69526202e+05 -2.20095421e+04\n",
      "  8.27279064e+06  1.46400806e+05  1.48554511e+07  1.72938245e+07\n",
      "  1.40866598e+07  1.34534746e+07  1.34535487e+07  1.67647135e+07\n",
      "  1.66781784e+07  1.66780906e+07  1.62292807e+07]\n",
      "Gradient Descent(46/99): loss=2.5078574828053828e+16, weights = [-3.14646105e-01 -6.23501968e+06  6.32993400e+06  9.12778121e+05\n",
      " -2.02894053e+07 -2.42407499e+07 -2.35223400e+07 -2.42391246e+07\n",
      "  1.14708218e+07 -9.72345153e+06 -2.29616487e+07 -1.48034954e+06\n",
      " -1.41974849e+07 -2.42410594e+07 -8.32299507e+06 -1.82744904e+05\n",
      " -1.25665678e+05 -7.17087244e+06 -3.91743792e+05  3.19898507e+04\n",
      " -1.20241156e+07 -2.12786752e+05 -2.15917064e+07 -2.51357693e+07\n",
      " -2.04743031e+07 -1.95539988e+07 -1.95541067e+07 -2.43667296e+07\n",
      " -2.42409566e+07 -2.42408278e+07 -2.35885028e+07]\n",
      "Gradient Descent(47/99): loss=5.297916309913518e+16, weights = [-3.14649809e-01  9.06231080e+06 -9.20026489e+06 -1.32668109e+06\n",
      "  2.94897055e+07  3.52328004e+07  3.41886265e+07  3.52304393e+07\n",
      " -1.66723046e+07  1.41325840e+07  3.33736858e+07  2.15161881e+06\n",
      "  2.06353834e+07  3.52332511e+07  1.20970865e+07  2.65611213e+05\n",
      "  1.82649208e+05  1.04225300e+07  5.69381348e+05 -4.64957519e+04\n",
      "  1.74764923e+07  3.09275636e+05  3.13825396e+07  3.65336693e+07\n",
      "  2.97584469e+07  2.84208266e+07  2.84209832e+07  3.54159074e+07\n",
      "  3.52331004e+07  3.52329144e+07  3.42847910e+07]\n",
      "Gradient Descent(48/99): loss=1.1191990541444064e+17, weights = [-3.14652462e-01 -1.31716466e+07  1.33721559e+07  1.92826858e+06\n",
      " -4.28619137e+07 -5.12092352e+07 -4.96915756e+07 -5.12058023e+07\n",
      "  2.42324189e+07 -2.05410526e+07 -4.85070993e+07 -3.12727804e+06\n",
      " -2.99925685e+07 -5.12098894e+07 -1.75825511e+07 -3.86053522e+05\n",
      " -2.65472120e+05 -1.51486611e+07 -8.27569281e+05  6.75794162e+04\n",
      " -2.54012674e+07 -4.49517731e+05 -4.56130599e+07 -5.30999876e+07\n",
      " -4.32525158e+07 -4.13083479e+07 -4.13085757e+07 -5.14753713e+07\n",
      " -5.12096716e+07 -5.12094000e+07 -4.98313462e+07]\n",
      "Gradient Descent(49/99): loss=2.3643380709012874e+17, weights = [-3.14655038e-01  1.91443750e+07 -1.94358065e+07 -2.80264917e+06\n",
      "  6.22977952e+07  7.44302378e+07  7.22243917e+07  7.44252493e+07\n",
      " -3.52206917e+07  2.98554633e+07  7.05028084e+07  4.54535267e+06\n",
      "  4.35928018e+07  7.44311894e+07  2.55554199e+07  5.61110820e+05\n",
      "  3.85851360e+05  2.20178742e+07  1.20283340e+06 -9.82235280e+04\n",
      "  3.69195595e+07  6.53353089e+05  6.62964581e+07  7.71783583e+07\n",
      "  6.28655183e+07  6.00397598e+07  6.00400907e+07  7.48170556e+07\n",
      "  7.44308717e+07  7.44304783e+07  7.24275418e+07]\n",
      "Gradient Descent(50/99): loss=4.9947276963941914e+17, weights = [-3.14656442e-01 -2.78254574e+07  2.82490389e+07  4.07351944e+06\n",
      " -9.05469441e+07 -1.08180885e+08 -1.04974789e+08 -1.08173633e+08\n",
      "  5.11916358e+07 -4.33935255e+07 -1.02472550e+08 -6.60645874e+06\n",
      " -6.33601069e+07 -1.08182267e+08 -3.71436119e+07 -8.15548441e+05\n",
      " -5.60816985e+05 -3.20019538e+07 -1.74826234e+06  1.42763330e+05\n",
      " -5.36608599e+07 -9.49618284e+05 -9.63588144e+07 -1.12175150e+08\n",
      " -9.13721022e+07 -8.72649947e+07 -8.72654758e+07 -1.08743105e+08\n",
      " -1.08181807e+08 -1.08181234e+08 -1.05270058e+08]\n",
      "Gradient Descent(51/99): loss=1.0551496449751392e+18, weights = [-3.14658521e-01  4.04430065e+07 -4.10586631e+07 -5.92067140e+06\n",
      "  1.31605766e+08  1.57235877e+08  1.52575968e+08  1.57225338e+08\n",
      " -7.44046578e+07  6.30704684e+07  1.48939078e+08  9.60217978e+06\n",
      "  9.20909648e+07  1.57237887e+08  5.39865108e+07  1.18536168e+06\n",
      "  8.15121366e+05  4.65133503e+07  2.54101788e+06 -2.07499845e+05\n",
      "  7.79935611e+07  1.38022595e+06  1.40053049e+08  1.63041356e+08\n",
      "  1.32805096e+08  1.26835606e+08  1.26836305e+08  1.58053040e+08\n",
      "  1.57237217e+08  1.57236385e+08  1.53005127e+08]\n",
      "Gradient Descent(52/99): loss=2.229031973244327e+18, weights = [-3.14658788e-01 -5.87820264e+07  5.96768540e+07  8.60541954e+06\n",
      " -1.91282852e+08 -2.28535025e+08 -2.21762062e+08 -2.28519707e+08\n",
      "  1.08143706e+08 -9.16699885e+07 -2.16476014e+08 -1.39563214e+07\n",
      " -1.33849928e+08 -2.28537946e+08 -7.84668784e+07 -1.72286799e+06\n",
      " -1.18474095e+06 -6.76049877e+07 -3.69325115e+06  3.01591366e+05\n",
      " -1.13360008e+08 -2.00609414e+06 -2.03560584e+08 -2.36973018e+08\n",
      " -1.93026021e+08 -1.84349646e+08 -1.84350662e+08 -2.29722731e+08\n",
      " -2.28536972e+08 -2.28535762e+08 -2.22385826e+08]\n",
      "Gradient Descent(53/99): loss=4.708889929885312e+18, weights = [-3.14661031e-01  8.54369378e+07 -8.67375286e+07 -1.25075771e+07\n",
      "  2.78020717e+08  3.32165015e+08  3.22320830e+08  3.32142751e+08\n",
      " -1.57181839e+08  1.33238059e+08  3.14637803e+08  2.02848629e+07\n",
      "  1.94544637e+08  3.32169261e+08  1.14047954e+08  2.50410839e+06\n",
      "  1.72196579e+06  9.82607019e+07  5.36796853e+06 -4.38348999e+05\n",
      "  1.64763493e+08  2.91576440e+06  2.95865829e+08  3.44429245e+08\n",
      "  2.80554333e+08  2.67943624e+08  2.67945101e+08  3.33891292e+08\n",
      "  3.32167845e+08  3.32166087e+08  3.23227442e+08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(54/99): loss=9.947656488525713e+18, weights = [-3.14659878e-01 -1.24178610e+08  1.26068958e+08  1.81791802e+07\n",
      " -4.04090164e+08 -4.82786381e+08 -4.68478315e+08 -4.82754020e+08\n",
      "  2.28456483e+08 -1.93655313e+08 -4.57311395e+08 -2.94831038e+07\n",
      " -2.82761569e+08 -4.82792551e+08 -1.65763389e+08 -3.63960493e+06\n",
      " -2.50279708e+06 -1.42817353e+08 -7.80209227e+06  6.37119861e+05\n",
      " -2.39476064e+08 -4.23792777e+06 -4.30027203e+08 -5.00611867e+08\n",
      " -4.07772656e+08 -3.89443580e+08 -3.89445726e+08 -4.85295442e+08\n",
      " -4.82790494e+08 -4.82787938e+08 -4.69796033e+08]\n",
      "Gradient Descent(55/99): loss=2.1014691591255757e+19, weights = [-3.14663256e-01  1.80487826e+08 -1.83235359e+08 -2.64225924e+07\n",
      "  5.87326234e+08  7.01707520e+08  6.80911413e+08  7.01660486e+08\n",
      " -3.32050858e+08  2.81468979e+08  6.64680813e+08  4.28523180e+07\n",
      "  4.10980773e+08  7.01716489e+08  2.40929367e+08  5.28999626e+06\n",
      "  3.63769899e+06  2.07578372e+08  1.13399778e+07 -9.26024041e+05\n",
      "  3.48067306e+08  6.15963064e+06  6.25024512e+08  7.27616034e+08\n",
      "  5.92678565e+08  5.66038105e+08  5.66041226e+08  7.05354325e+08\n",
      "  7.01713497e+08  7.01709784e+08  6.82826655e+08]\n",
      "Gradient Descent(56/99): loss=4.43941005788629e+19, weights = [-3.14659749e-01 -2.62330650e+08  2.66324060e+08  3.84040069e+07\n",
      " -8.53651328e+08 -1.01989920e+09 -9.89673027e+08 -1.01983084e+09\n",
      "  4.82620458e+08 -4.09102053e+08 -9.66082608e+08 -6.22838488e+07\n",
      " -5.97341412e+08 -1.01991224e+09 -3.50179613e+08 -7.68876319e+06\n",
      " -5.28722607e+06 -3.01705496e+08 -1.64821295e+07  1.34593282e+06\n",
      " -5.05899618e+08 -8.95273627e+06 -9.08444019e+08 -1.05755602e+09\n",
      " -8.61430691e+08 -8.22710025e+08 -8.22714560e+08 -1.02519966e+09\n",
      " -1.01990789e+09 -1.01990249e+09 -9.92456742e+08]\n",
      "Gradient Descent(57/99): loss=9.37837301893238e+19, weights = [-3.14665942e-01  3.81285382e+08 -3.87089619e+08 -5.58184361e+07\n",
      "  1.24074245e+09  1.48237599e+09  1.43844365e+09  1.48227663e+09\n",
      " -7.01466359e+08  5.94610781e+08  1.40415608e+09  9.05266733e+07\n",
      "  8.68207924e+08  1.48239493e+09  5.08969760e+08  1.11752592e+07\n",
      "  7.68473685e+06  4.38514888e+08  2.39560077e+07 -1.95625067e+06\n",
      "  7.35301535e+08  1.30123852e+07  1.32038107e+09  1.53710842e+09\n",
      "  1.25204939e+09  1.19577071e+09  1.19577730e+09  1.49007996e+09\n",
      "  1.48238862e+09  1.48238077e+09  1.44248965e+09]\n",
      "Gradient Descent(58/99): loss=1.9812064967054898e+20, weights = [-3.14657810e-01 -5.54180546e+08  5.62616733e+08  8.11294967e+07\n",
      " -1.80336137e+09 -2.15456446e+09 -2.09071086e+09 -2.15442004e+09\n",
      "  1.01954868e+09 -8.64239077e+08 -2.04087546e+09 -1.31576304e+08\n",
      " -1.26189978e+09 -2.15459200e+09 -7.39763842e+08 -1.62427188e+07\n",
      " -1.11694071e+07 -6.37360966e+08 -3.48189415e+07  2.84331924e+06\n",
      " -1.06872654e+09 -1.89128959e+07 -1.91911240e+09 -2.23411550e+09\n",
      " -1.81979548e+09 -1.73799703e+09 -1.73800661e+09 -2.16576183e+09\n",
      " -2.15458282e+09 -2.15457141e+09 -2.09659153e+09]\n",
      "Gradient Descent(59/99): loss=4.185351952480589e+20, weights = [-3.14670321e-01  8.05475614e+08 -8.17737220e+08 -1.17917946e+08\n",
      "  2.62110176e+09  3.13155910e+09  3.03875086e+09  3.13134920e+09\n",
      " -1.48186653e+09  1.25613125e+09  2.96631744e+09  1.91240030e+08\n",
      "  1.83411256e+09  3.13159913e+09  1.07521229e+09  2.36080353e+07\n",
      "  1.62342131e+07  9.26374482e+08  5.06077098e+07 -4.13263210e+06\n",
      "  1.55334425e+09  2.74890134e+07  2.78934049e+09  3.24718283e+09\n",
      "  2.64498798e+09  2.52609774e+09  2.52611167e+09  3.14783395e+09\n",
      "  3.13158578e+09  3.13156920e+09  3.04729815e+09]\n",
      "Gradient Descent(60/99): loss=8.84166844559717e+20, weights = [-3.14652328e-01 -1.17072129e+09  1.18854296e+09  1.71388242e+08\n",
      " -3.80964933e+09 -4.55157531e+09 -4.41668286e+09 -4.55127022e+09\n",
      "  2.15382399e+09 -1.82572827e+09 -4.31140425e+09 -2.77958477e+08\n",
      " -2.66579719e+09 -4.55163349e+09 -1.56277099e+09 -3.43131799e+07\n",
      " -2.35956726e+07 -1.34644217e+09 -7.35559493e+07  6.00658829e+06\n",
      " -2.25771352e+09 -3.99540007e+07 -4.05417650e+09 -4.71962902e+09\n",
      " -3.84436684e+09 -3.67156542e+09 -3.67158566e+09 -4.57523005e+09\n",
      " -4.55161408e+09 -4.55158999e+09 -4.42910594e+09]\n",
      "Gradient Descent(61/99): loss=1.8678262136482006e+21, weights = [-3.14678584e-01  1.70158888e+09 -1.72749185e+09 -2.49104828e+08\n",
      "  5.53714788e+09  6.61550274e+09  6.41944284e+09  6.61505931e+09\n",
      " -3.13048286e+09  2.65361101e+09  6.26642528e+09  4.03999701e+08\n",
      "  3.87461207e+09  6.61558729e+09  2.27141485e+09  4.98726089e+07\n",
      "  3.42952113e+07  1.95699098e+09  1.06910147e+08 -8.73029633e+06\n",
      "  3.28148145e+09  5.80712793e+07  5.89255673e+09  6.85976098e+09\n",
      "  5.58760816e+09  5.33644935e+09  5.33647876e+09  6.64988379e+09\n",
      "  6.61555909e+09  6.61552408e+09  6.43749921e+09]\n",
      "Gradient Descent(62/99): loss=3.9458330583846617e+21, weights = [-3.14641126e-01 -2.47318018e+09  2.51082894e+09  3.62062266e+08\n",
      " -8.04798657e+09 -9.61532513e+09 -9.33036120e+09 -9.61468063e+09\n",
      "  4.55001104e+09 -3.85690002e+09 -9.10795731e+09 -5.87194750e+08\n",
      " -5.63156819e+09 -9.61544802e+09 -3.30139570e+09 -7.24875144e+07\n",
      " -4.98464924e+07 -2.84439525e+09 -1.55388919e+08  1.26890791e+07\n",
      " -4.76948047e+09 -8.44038999e+07 -8.56455678e+09 -9.97034311e+09\n",
      " -8.12132824e+09 -7.75628059e+09 -7.75632335e+09 -9.66529639e+09\n",
      " -9.61540704e+09 -9.61535615e+09 -9.35660529e+09]\n",
      "Gradient Descent(63/99): loss=8.335678346772461e+21, weights = [-3.14695645e-01  3.59465221e+09 -3.64937294e+09 -5.26240642e+08\n",
      "  1.16973737e+10  1.39754273e+10  1.35612455e+10  1.39744905e+10\n",
      " -6.61322914e+09  5.60582456e+09  1.32379918e+10  8.53460222e+08\n",
      "  8.18522209e+09  1.39756059e+10  4.79842490e+09  1.05357226e+08\n",
      "  7.24495552e+07  4.13419602e+09  2.25850557e+08 -1.84429856e+07\n",
      "  6.93221774e+09  1.22677138e+08  1.24481844e+10  1.44914294e+10\n",
      "  1.18039724e+10  1.12733926e+10  1.12734548e+10  1.40480582e+10\n",
      "  1.39755463e+10  1.39754724e+10  1.35993900e+10]\n",
      "Gradient Descent(64/99): loss=1.7609344458504754e+22, weights = [-3.14616117e-01 -5.22465958e+09  5.30419360e+09  7.64866264e+08\n",
      " -1.70015878e+10 -2.03126327e+10 -1.97106387e+10 -2.03112711e+10\n",
      "  9.61202056e+09 -8.14780492e+09 -1.92408045e+10 -1.24046468e+09\n",
      " -1.18968391e+10 -2.03128923e+10 -6.97428712e+09 -1.53131822e+08\n",
      " -1.05302054e+08 -6.00886138e+09 -3.28263266e+08  2.68060206e+07\n",
      " -1.00756556e+10 -1.78305506e+08 -1.80928563e+10 -2.10626177e+10\n",
      " -1.71565241e+10 -1.63853511e+10 -1.63854415e+10 -2.04181983e+10\n",
      " -2.03128057e+10 -2.03126982e+10 -1.97660801e+10]\n",
      "Gradient Descent(65/99): loss=3.7200213270985427e+22, weights = [-3.14733873e-01  7.59379938e+09 -7.70939837e+09 -1.11169749e+09\n",
      "  2.47110161e+10  2.95234656e+10  2.86484954e+10  2.95214867e+10\n",
      " -1.39706242e+10  1.18424550e+10  2.79656133e+10  1.80295765e+09\n",
      "  1.72915016e+10  2.95238429e+10  1.01368015e+10  2.22569971e+08\n",
      "  1.53051632e+08  8.73360017e+09  4.77115370e+08 -3.89613025e+07\n",
      "  1.46444962e+10  2.59158749e+08  2.62971240e+10  3.06135339e+10\n",
      "  2.49362088e+10  2.38153448e+10  2.38154761e+10  2.96769003e+10\n",
      "  2.95237171e+10  2.95235608e+10  2.87290768e+10]\n",
      "Gradient Descent(66/99): loss=7.858644997647493e+22, weights = [-3.14566298e-01 -1.10372337e+10  1.12052515e+10  1.61580053e+09\n",
      " -3.59163111e+10 -4.29109822e+10 -4.16392538e+10 -4.29081060e+10\n",
      "  2.03056517e+10 -1.72124568e+10 -4.06467165e+10 -2.62051497e+09\n",
      " -2.51323922e+10 -4.29115307e+10 -1.47333689e+10 -3.23495087e+08\n",
      " -2.22453419e+08 -1.26938811e+10 -6.93464970e+08  5.66284387e+07\n",
      " -2.12850932e+10 -3.76675172e+08 -3.82216450e+10 -4.44953457e+10\n",
      " -3.62436181e+10 -3.46144945e+10 -3.46146853e+10 -4.31339925e+10\n",
      " -4.29113478e+10 -4.29111207e+10 -4.17563752e+10]\n",
      "Gradient Descent(67/99): loss=1.6601598692236367e+23, weights = [-3.14811473e-01  1.60421052e+10 -1.62863112e+10 -2.34849082e+09\n",
      "  5.22026855e+10  6.23691142e+10  6.05207162e+10  6.23649337e+10\n",
      " -2.95133190e+10  2.50175043e+10  5.90781094e+10  3.80879647e+09\n",
      "  3.65287617e+10  6.23699114e+10  2.14142656e+10  4.70185044e+08\n",
      "  3.23325684e+08  1.84499650e+10  1.00791904e+09 -8.23067983e+07\n",
      "  3.09368917e+10  5.47479820e+08  5.55533810e+10  6.46719128e+10\n",
      "  5.26784109e+10  5.03105557e+10  5.03108330e+10  6.26932492e+10\n",
      "  6.23696455e+10  6.23693154e+10  6.06909466e+10]\n",
      "Gradient Descent(68/99): loss=3.5071323265087746e+23, weights = [-3.14453661e-01 -2.33164528e+10  2.36713948e+10  3.41342204e+09\n",
      " -7.58741722e+10 -9.06506028e+10 -8.79640424e+10 -9.06445267e+10\n",
      "  4.28962347e+10 -3.63617774e+10 -8.58672805e+10 -5.53590829e+09\n",
      " -5.30928538e+10 -9.06517615e+10 -3.11246377e+10 -6.83392063e+08\n",
      " -4.69938824e+08 -2.68161649e+10 -1.46496338e+09  1.19629098e+08\n",
      " -4.49653313e+10 -7.95736421e+08 -8.07442520e+10 -9.39976133e+10\n",
      " -7.65656170e+10 -7.31240496e+10 -7.31244527e+10 -9.11217180e+10\n",
      " -9.06513750e+10 -9.06508953e+10 -8.82114644e+10]\n",
      "Gradient Descent(69/99): loss=7.408911264307875e+23, weights = [-3.14969511e-01  3.38893782e+10 -3.44052699e+10 -4.96124996e+09\n",
      "  1.10279576e+11  1.31756429e+11  1.27851639e+11  1.31747598e+11\n",
      " -6.23476794e+10  5.28501500e+10  1.24804093e+11  8.04618487e+09\n",
      "  7.71679904e+10  1.31758113e+11  4.52382113e+10  9.93278537e+08\n",
      "  6.83034194e+08  3.89760468e+10  2.12925605e+09 -1.73875323e+08\n",
      "  6.53550149e+10  1.15656583e+09  1.17358010e+11  1.36621153e+11\n",
      "  1.11284558e+11  1.06282401e+11  1.06282986e+11  1.32441173e+11\n",
      "  1.31757552e+11  1.31756854e+11  1.28211255e+11]\n",
      "Gradient Descent(70/99): loss=1.5651524097760867e+24, weights = [-3.14223136e-01 -4.92566330e+10  5.00064575e+10  7.21094576e+09\n",
      " -1.60286227e+11 -1.91501834e+11 -1.85826403e+11 -1.91488998e+11\n",
      "  9.06194483e+10 -7.68152317e+10 -1.81396937e+11 -1.16947550e+10\n",
      " -1.12160080e+11 -1.91504282e+11 -6.57516333e+10 -1.44368409e+09\n",
      " -9.92758391e+08 -5.66498688e+10 -3.09477450e+09  2.52719684e+08\n",
      " -9.49904704e+10 -1.68101457e+09 -1.70574402e+11 -1.98572483e+11\n",
      " -1.61746923e+11 -1.54476519e+11 -1.54477371e+11 -1.92497077e+11\n",
      " -1.91503465e+11 -1.91502452e+11 -1.86349088e+11]\n",
      "Gradient Descent(71/99): loss=3.306426515902879e+24, weights = [-3.15311836e-01  7.15922220e+10 -7.26820571e+10 -1.04807738e+10\n",
      "  2.32968566e+11  2.78338997e+11  2.70090022e+11  2.78320340e+11\n",
      " -1.31711148e+11  1.11647362e+11  2.63652000e+11  1.69977817e+10\n",
      "  1.63019453e+11  2.78342555e+11  9.55669368e+10  2.09832759e+09\n",
      "  1.44292809e+09  8.23379459e+10  4.49811060e+09 -3.67316291e+08\n",
      "  1.38064225e+11  2.44327639e+09  2.47921949e+11  2.88615857e+11\n",
      "  2.35091619e+11  2.24524427e+11  2.24525664e+11  2.79785537e+11\n",
      "  2.78341368e+11  2.78339895e+11  2.70849721e+11]\n",
      "Gradient Descent(72/99): loss=6.984914847129585e+24, weights = [-3.13718236e-01 -1.04055961e+11  1.05639985e+11  1.52333166e+10\n",
      " -3.38608961e+11 -4.04552769e+11 -3.92563269e+11 -4.04525653e+11\n",
      "  1.91436019e+11 -1.62274241e+11 -3.83205903e+11 -2.47054841e+10\n",
      " -2.36941183e+11 -4.04557940e+11 -1.38902092e+11 -3.04982143e+09\n",
      " -2.09722877e+09 -1.19674370e+11 -6.53779427e+09  5.33877122e+08\n",
      " -2.00669921e+11 -3.55118844e+09 -3.60343006e+11 -4.19489706e+11\n",
      " -3.41694720e+11 -3.26335797e+11 -3.26337596e+11 -4.06655248e+11\n",
      " -4.04556215e+11 -4.04554074e+11 -3.93667456e+11]\n",
      "Gradient Descent(73/99): loss=1.4755820275149327e+25, weights = [-3.16042736e-01  1.51240492e+11 -1.53542798e+11 -2.21409163e+10\n",
      "  4.92152355e+11  5.87998609e+11  5.70572429e+11  5.87959196e+11\n",
      " -2.78243338e+11  2.35858051e+11  5.56971933e+11  3.59082706e+10\n",
      "  3.44382974e+11  5.88006125e+11  2.01887722e+11  4.43277342e+09\n",
      "  3.04822435e+09  1.73941123e+11  9.50237949e+09 -7.75965532e+08\n",
      "  2.91664384e+11  5.16148701e+09  5.23741777e+11  6.09708751e+11\n",
      "  4.96637362e+11  4.74313883e+11  4.74316498e+11  5.91054464e+11\n",
      "  5.88003618e+11  5.88000506e+11  5.72177313e+11]\n",
      "Gradient Descent(74/99): loss=3.117206676928729e+25, weights = [-3.12700336e-01 -2.19821012e+11  2.23167307e+11  3.21807907e+10\n",
      " -7.15320528e+11 -8.54628594e+11 -8.29300452e+11 -8.54571309e+11\n",
      "  4.04413734e+11 -3.42808693e+11 -8.09532766e+11 -5.21909989e+10\n",
      " -5.00544614e+11 -8.54639517e+11 -2.93434401e+11 -6.44282972e+09\n",
      " -4.43045213e+09 -2.52815321e+11 -1.38112660e+10  1.12782976e+09\n",
      " -4.23920599e+11 -7.50198099e+09 -7.61234282e+11 -8.86183275e+11\n",
      " -7.21839276e+11 -6.89393140e+11 -6.89396940e+11 -8.59070136e+11\n",
      " -8.54635874e+11 -8.54631351e+11 -8.31633077e+11]\n",
      "Gradient Descent(75/99): loss=6.5851828536117565e+25, weights = [-3.17588336e-01  3.19499604e+11 -3.24363287e+11 -4.67732805e+10\n",
      "  1.03968507e+12  1.24216286e+12  1.20534958e+12  1.24207960e+12\n",
      " -5.87796528e+11  4.98256470e+11  1.17661817e+12  7.58571863e+10\n",
      "  7.27518285e+11  1.24217874e+12  4.26493236e+11  9.36435294e+09\n",
      "  6.43945583e+09  3.67455295e+11  2.00740320e+10 -1.63924803e+09\n",
      "  6.16148847e+11  1.09037800e+10  1.10641858e+12  1.28802612e+12\n",
      "  1.04915977e+12  1.00200082e+12  1.00200635e+12  1.24861843e+12\n",
      "  1.24217344e+12  1.24216687e+12  1.20873995e+12]\n",
      "Gradient Descent(76/99): loss=1.3911375699421994e+26, weights = [-3.10491936e-01 -4.64377794e+11  4.71446931e+11  6.79827849e+10\n",
      " -1.51113383e+12 -1.80542587e+12 -1.75191949e+12 -1.80530485e+12\n",
      "  8.54334878e+11 -7.24192575e+11 -1.71015972e+12 -1.10254887e+11\n",
      " -1.05741395e+12 -1.80544894e+12 -6.19888056e+11 -1.36106509e+10\n",
      " -9.35944915e+09 -5.34079159e+11 -2.91766706e+10  2.38257068e+09\n",
      " -8.95543654e+11 -1.58481364e+10 -1.60812787e+12 -1.87208598e+12\n",
      " -1.52490486e+12 -1.45636153e+12 -1.45636956e+12 -1.81480874e+12\n",
      " -1.80544125e+12 -1.80543169e+12 -1.75684722e+12]\n",
      "Gradient Descent(77/99): loss=2.938815491574795e+26, weights = [-3.20654336e-01  6.74951497e+11 -6.85226158e+11 -9.88098117e+10\n",
      "  2.19636265e+12  2.62410241e+12  2.54633338e+12  2.62392652e+12\n",
      " -1.24173596e+12  1.05258018e+12  2.48563750e+12  1.60250344e+11\n",
      "  1.53690192e+12  2.62413595e+12  9.00978419e+11  1.97824472e+10\n",
      "  1.36035235e+10  7.76259184e+11  4.24069319e+10 -3.46295551e+09\n",
      "  1.30163099e+12  2.30345282e+10  2.33733897e+12  2.72098978e+12\n",
      "  2.21637820e+12  2.11675366e+12  2.11676533e+12  2.63773999e+12\n",
      "  2.62412477e+12  2.62411088e+12  2.55349561e+12]\n",
      "Gradient Descent(78/99): loss=6.208326681795305e+26, weights = [-3.05819936e-01 -9.81010566e+11  9.95944307e+11  1.43615460e+11\n",
      " -3.19231082e+12 -3.81401065e+12 -3.70097698e+12 -3.81375500e+12\n",
      "  1.80480538e+12 -1.52987627e+12 -3.61275835e+12 -2.32916412e+11\n",
      " -2.23381537e+12 -3.81405939e+12 -1.30953017e+12 -2.87528657e+10\n",
      " -1.97720878e+10 -1.12825657e+12 -6.16365005e+10  5.03324456e+09\n",
      " -1.89186003e+12 -3.34796139e+10 -3.39721333e+12 -3.95483192e+12\n",
      " -3.22140250e+12 -3.07660286e+12 -3.07661982e+12 -3.83383223e+12\n",
      " -3.81404314e+12 -3.81402295e+12 -3.71138695e+12]\n",
      "Gradient Descent(79/99): loss=1.311525691163316e+27, weights = [-3.27631136e-01  1.42585317e+12 -1.44755867e+12 -2.08738382e+11\n",
      "  4.63987511e+12  5.54348685e+12  5.37919768e+12  5.54311528e+12\n",
      " -2.62320057e+12  2.22360391e+12  5.25097601e+12  3.38533157e+11\n",
      "  3.24674660e+12  5.54355770e+12  1.90334112e+12  4.17909511e+10\n",
      "  2.87378088e+10  1.63986838e+12  8.95857829e+10 -7.31558656e+09\n",
      "  2.74973045e+12  4.86610596e+10  4.93769136e+12  5.74816401e+12\n",
      "  4.68215850e+12  4.47169897e+12  4.47172362e+12  5.57229659e+12\n",
      "  5.54353407e+12  5.54350473e+12  5.39432808e+12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(80/99): loss=2.770633258757608e+27, weights = [-2.95827936e-01 -2.07241118e+12  2.10395911e+12  3.03391517e+11\n",
      " -6.74384238e+12 -8.05719996e+12 -7.81841330e+12 -8.05665990e+12\n",
      "  3.81269986e+12 -3.23190472e+12 -7.63204907e+12 -4.92042178e+11\n",
      " -4.71899498e+12 -8.05730294e+12 -2.76641767e+12 -6.07412012e+10\n",
      " -4.17690667e+10 -2.38347232e+12 -1.30208763e+11  1.06328644e+10\n",
      " -3.99660514e+12 -7.07265838e+10 -7.17670444e+12 -8.35468869e+12\n",
      " -6.80529932e+12 -6.49940619e+12 -6.49944202e+12 -8.09907358e+12\n",
      " -8.05726860e+12 -8.05722595e+12 -7.84040464e+12]\n",
      "Gradient Descent(81/99): loss=5.853037196491963e+27, weights = [-3.42181536e-01  3.01215313e+12 -3.05800658e+12 -4.40965440e+11\n",
      "  9.80186082e+12  1.17107649e+13  1.13636996e+13  1.17099799e+13\n",
      " -5.54158168e+12  4.69742300e+12  1.10928279e+13  7.15160390e+11\n",
      "  6.85883943e+12  1.17109146e+13  4.02085925e+12  8.82845071e+10\n",
      "  6.07093931e+10  3.46426601e+12  1.89252373e+11 -1.54543732e+10\n",
      "  5.80887944e+12  1.02797796e+11  1.04310057e+13  1.21431509e+13\n",
      "  9.89118562e+12  9.44658421e+12  9.44663629e+12  1.17716263e+13\n",
      "  1.17108647e+13  1.17108027e+13  1.13956630e+13]\n",
      "Gradient Descent(82/99): loss=1.2364698328525995e+28, weights = [-2.74827936e-01 -4.37802429e+12  4.44467014e+12  6.40922730e+11\n",
      " -1.42465482e+13 -1.70210514e+13 -1.65166082e+13 -1.70199105e+13\n",
      "  8.05443089e+12 -6.82748556e+12 -1.61229087e+13 -1.03945232e+12\n",
      " -9.96900368e+12 -1.70212689e+13 -5.84413165e+12 -1.28317419e+11\n",
      " -8.82382755e+10 -5.03514931e+12 -2.75069511e+11  2.24622116e+10\n",
      " -8.44293574e+12 -1.49411809e+11 -1.51609809e+13 -1.76495043e+13\n",
      " -1.43763777e+13 -1.37301702e+13 -1.37302459e+13 -1.71095105e+13\n",
      " -1.70211964e+13 -1.70211063e+13 -1.65630655e+13]\n",
      "Gradient Descent(83/99): loss=2.61207574158399e+28, weights = [-3.71659936e-01  6.36325441e+12 -6.46012105e+12 -9.31551339e+11\n",
      "  2.07066943e+13  2.47393055e+13  2.40061207e+13  2.47376472e+13\n",
      " -1.17067402e+13  9.92343229e+12  2.34338969e+13  1.51079554e+12\n",
      "  1.44894826e+13  2.47396217e+13  8.49417318e+12  1.86503393e+11\n",
      "  1.28250224e+11  7.31835503e+12  3.99800723e+11 -3.26477785e+10\n",
      "  1.22714139e+13  2.17163105e+11  2.20357797e+13  2.56527325e+13\n",
      "  2.08953954e+13  1.99561629e+13  1.99562730e+13  2.48678767e+13\n",
      "  2.47395162e+13  2.47393853e+13  2.40736443e+13]\n",
      "Gradient Descent(84/99): loss=5.518080181568692e+28, weights = [-2.30917536e-01 -9.24869394e+12  9.38948508e+12  1.35396649e+12\n",
      " -3.00962158e+13 -3.59574283e+13 -3.48917785e+13 -3.59550182e+13\n",
      "  1.70152016e+13 -1.44232467e+13 -3.40600778e+13 -2.19587095e+12\n",
      " -2.10597881e+13 -3.59578879e+13 -1.23458851e+13 -2.71073996e+11\n",
      " -1.86405728e+11 -1.06368882e+13 -5.81091731e+11  4.74520256e+10\n",
      " -1.78359286e+13 -3.15636460e+11 -3.20279796e+13 -3.72850520e+13\n",
      " -3.03704840e+13 -2.90053534e+13 -2.90055133e+13 -3.61443007e+13\n",
      " -3.59577347e+13 -3.59575443e+13 -3.49899208e+13]\n",
      "Gradient Descent(85/99): loss=1.165709263535993e+29, weights = [-4.36011936e-01  1.34425459e+13 -1.36471793e+13 -1.96792723e+12\n",
      "  4.37434481e+13  5.22624475e+13  5.07135750e+13  5.22589444e+13\n",
      " -2.47308032e+13  2.09635173e+13  4.95047369e+13  3.19159616e+12\n",
      "  3.06094212e+13  5.22631155e+13  1.79441691e+13  3.93993429e+11\n",
      "  2.70932044e+11  1.54602216e+13  8.44589769e+11 -6.89693092e+10\n",
      "  2.59236915e+13  4.58762894e+11  4.65511767e+13  5.41920867e+13\n",
      "  4.41420841e+13  4.21579304e+13  4.21581628e+13  5.25340577e+13\n",
      "  5.22628927e+13  5.22626161e+13  5.08562203e+13]\n",
      "Gradient Descent(86/99): loss=2.4625921378100337e+29, weights = [-1.36927136e-01 -1.95381144e+13  1.98355395e+13  2.86029058e+12\n",
      " -6.35790645e+13 -7.59610334e+13 -7.37098195e+13 -7.59559419e+13\n",
      "  3.59450706e+13 -3.04694961e+13 -7.19528295e+13 -4.63883638e+12\n",
      " -4.44893682e+13 -7.59620043e+13 -2.60810140e+13 -5.72651099e+11\n",
      " -3.93787108e+11 -2.24707121e+13 -1.22757189e+12  1.00243679e+11\n",
      " -3.76788783e+13 -6.66790501e+11 -6.76599673e+13 -7.87656742e+13\n",
      " -6.41584635e+13 -6.12745884e+13 -6.12749262e+13 -7.63558063e+13\n",
      " -7.59616805e+13 -7.59612785e+13 -7.39171477e+13]\n",
      "Gradient Descent(87/99): loss=5.202292052487066e+29, weights = [-5.70847136e-01  2.83977394e+13 -2.88300329e+13 -4.15729915e+12\n",
      "  9.24092093e+13  1.10405824e+14  1.07133790e+14  1.10398424e+14\n",
      " -5.22444861e+13  4.42859935e+13  1.04580087e+14  6.74233264e+12\n",
      "  6.46632247e+13  1.10407236e+14  3.79075392e+13  8.32321703e+11\n",
      "  5.72351222e+11  3.26601335e+13  1.78421858e+12 -1.45699519e+11\n",
      "  5.47644948e+13  9.69148940e+11  9.83406114e+13  1.14482239e+14\n",
      "  9.32513386e+13  8.90597605e+13  8.90602515e+13  1.10979608e+14\n",
      "  1.10406765e+14  1.10406181e+14  1.07435132e+14]\n",
      "Gradient Descent(88/99): loss=1.0989981728536639e+30, weights = [ 5.90920642e-02 -4.12747916e+13  4.19031101e+13  6.04244070e+12\n",
      " -1.34312482e+14 -1.60469724e+14 -1.55713974e+14 -1.60458968e+14\n",
      "  7.59349274e+13 -6.43676291e+13 -1.52002285e+14 -9.79966649e+12\n",
      " -9.39849856e+13 -1.60471775e+14 -5.50968428e+13 -1.20974083e+12\n",
      " -8.31885845e+11 -4.74699830e+13 -2.59327861e+12  2.11767465e+11\n",
      " -7.95976427e+13 -1.40861285e+12 -1.42933499e+14 -1.66394603e+14\n",
      " -1.35536478e+14 -1.29444214e+14 -1.29444928e+14 -1.61303692e+14\n",
      " -1.60471091e+14 -1.60470242e+14 -1.56151960e+14]\n",
      "Gradient Descent(89/99): loss=2.321663166446533e+30, weights = [-8.54162336e-01  5.99909874e+13 -6.09042190e+13 -8.78240614e+12\n",
      "  1.95216937e+14  2.33235270e+14  2.26323009e+14  2.33219637e+14\n",
      " -1.10367881e+14  9.35553512e+13  2.20928243e+14  1.42433588e+13\n",
      "  1.36602799e+14  2.33238251e+14  8.00806951e+13  1.75830196e+12\n",
      "  1.20910733e+12  6.89954095e+13  3.76920968e+12 -3.07794148e+11\n",
      "  1.15691467e+14  2.04735317e+12  2.07747184e+14  2.41846806e+14\n",
      "  1.96995958e+14  1.88141137e+14  1.88142174e+14  2.34447404e+14\n",
      "  2.33237257e+14  2.33236023e+14  2.26959602e+14]\n",
      "Gradient Descent(90/99): loss=4.9045758142058597e+30, weights = [ 4.76525664e-01 -8.71941063e+13  8.85214459e+13  1.27648183e+13\n",
      " -2.83738727e+14 -3.38996603e+14 -3.28949953e+14 -3.38973881e+14\n",
      "  1.60414574e+14 -1.35978346e+14 -3.21108912e+14 -2.07020587e+13\n",
      " -1.98545807e+14 -3.39000936e+14 -1.16393561e+14 -2.55561001e+12\n",
      " -1.75738120e+12 -1.00281614e+14 -5.47837073e+12  4.47364459e+11\n",
      " -1.68152159e+14 -2.97573249e+12 -3.01950856e+14 -3.51513069e+14\n",
      " -2.86324451e+14 -2.73454380e+14 -2.73455888e+14 -3.40758383e+14\n",
      " -3.38999491e+14 -3.38997697e+14 -3.29875211e+14]\n",
      "Gradient Descent(91/99): loss=1.0361048176558112e+31, weights = [-1.42166314e+00  1.26732573e+14 -1.28661799e+14 -1.85530689e+13\n",
      "  4.12401025e+14  4.92715775e+14  4.78113437e+14  4.92682749e+14\n",
      " -2.33155113e+14  1.97638193e+14  4.66716848e+14  3.00894782e+13\n",
      "  2.88577084e+14  4.92722072e+14  1.69172620e+14  3.71446013e+12\n",
      "  2.55427173e+12  1.45754656e+14  7.96255672e+12 -6.50223406e+11\n",
      "  2.44401332e+14  4.32508858e+12  4.38871507e+14  5.10907875e+14\n",
      "  4.16159254e+14  3.97453206e+14  3.97455397e+14  4.95276440e+14\n",
      "  4.92719972e+14  4.92717364e+14  4.79458257e+14]\n",
      "Gradient Descent(92/99): loss=2.1887992638633765e+31, weights = [ 1.38481366e+00 -1.84199892e+14  1.87003933e+14  2.69660216e+13\n",
      " -5.99405683e+14 -7.16139432e+14 -6.94915614e+14 -7.16091430e+14\n",
      "  3.38880098e+14 -2.87257909e+14 -6.78351203e+14 -4.37336553e+13\n",
      " -4.19433352e+14 -7.16148585e+14 -2.45884524e+14 -5.39879481e+12\n",
      " -3.71251499e+12 -2.11847605e+14 -1.15732054e+13  9.45069439e+11\n",
      " -3.55225953e+14 -6.28631483e+12 -6.37879296e+14 -7.42580802e+14\n",
      " -6.04868095e+14 -5.77679725e+14 -5.77682910e+14 -7.19861240e+14\n",
      " -7.16145532e+14 -7.16141742e+14 -6.96870247e+14]\n",
      "Gradient Descent(93/99): loss=4.6238972504038255e+31, weights = [-2.67350314e+00  2.67725965e+14 -2.71801507e+14 -3.91938566e+13\n",
      "  8.71208245e+14  1.04087531e+15  1.01002748e+15  1.04080554e+15\n",
      " -4.92546440e+14  4.17515992e+14  9.85951881e+14  6.35648312e+13\n",
      "  6.09626843e+14  1.04088862e+15  3.57381704e+14  7.84689682e+12\n",
      "  5.39596764e+12  3.07910627e+14  1.68211150e+13 -1.37361442e+12\n",
      "  5.16304379e+14  9.13686583e+12  9.27127849e+14  1.07930661e+15\n",
      "  8.79147606e+14  8.39630577e+14  8.39635205e+14  1.04628479e+15\n",
      "  1.04088418e+15  1.04087867e+15  1.01286845e+15]\n",
      "Gradient Descent(94/99): loss=9.768107169660809e+31, weights = [ 3.19647446e+00 -3.89127221e+14  3.95050832e+14  5.69664452e+13\n",
      " -1.26626061e+15 -1.51286379e+15 -1.46802791e+15 -1.51276238e+15\n",
      "  7.15893311e+14 -6.06840050e+14 -1.43303514e+15 -9.23885217e+13\n",
      " -8.86064224e+14 -1.51288313e+15 -5.19437663e+14 -1.14050991e+13\n",
      " -7.84278767e+12 -4.47533757e+14 -2.44487072e+13  1.99648458e+12\n",
      " -7.50424369e+14 -1.32800090e+13 -1.34753715e+15 -1.56872190e+15\n",
      " -1.27780010e+15 -1.22036394e+15 -1.22037067e+15 -1.52072621e+15\n",
      " -1.51287668e+15 -1.51286867e+15 -1.47215712e+15]\n",
      "Gradient Descent(95/99): loss=2.0635388831281903e+32, weights = [-5.40758314e+00  5.65578293e+14 -5.74187986e+14 -8.27980750e+13\n",
      "  1.84045082e+15  2.19887706e+15  2.13371019e+15  2.19872967e+15\n",
      " -1.04051759e+15  8.82013751e+14  2.08284984e+15  1.34282414e+14\n",
      "  1.28785308e+15  2.19890517e+15  7.54978452e+14  1.65767803e+13\n",
      "  1.13991266e+13  6.50469473e+14  3.55350573e+13 -2.90179736e+12\n",
      "  1.09070687e+15  1.93018745e+13  1.95858249e+15  2.28006422e+15\n",
      "  1.85722294e+15  1.77374215e+15  1.77375193e+15  2.21030472e+15\n",
      "  2.19889579e+15  2.19888415e+15  2.13971181e+15]\n",
      "Gradient Descent(96/99): loss=4.359281330786028e+32, weights = [ 7.29329366e+00 -8.22041709e+14  8.34555496e+14  1.20343146e+14\n",
      " -2.67500956e+15 -3.19596540e+15 -3.10124839e+15 -3.19575118e+15\n",
      "  1.51234385e+15 -1.28196591e+15 -3.02732524e+15 -1.95173234e+14\n",
      " -1.87183448e+15 -3.19600625e+15 -1.09732602e+15 -2.40935782e+13\n",
      " -1.65680997e+13 -9.45427085e+14 -5.16485509e+13  4.21762731e+12\n",
      " -1.58529164e+15 -2.80543757e+13 -2.84670843e+15 -3.31396715e+15\n",
      " -2.69938704e+15 -2.57805161e+15 -2.57806582e+15 -3.21257497e+15\n",
      " -3.19599263e+15 -3.19597571e+15 -3.10997147e+15]\n",
      "Gradient Descent(97/99): loss=9.20909893015039e+32, weights = [-1.09953463e+01  1.19479934e+15 -1.21298755e+15 -1.74913158e+14\n",
      "  3.88800181e+15  4.64518687e+15  4.50752010e+15  4.64487551e+15\n",
      " -2.19812136e+15  1.86327775e+15  4.40007624e+15  2.83675206e+14\n",
      "  2.72062424e+15  4.64524624e+15  1.59491226e+15  3.50188938e+13\n",
      "  2.40809612e+13  1.37413424e+15  7.50687633e+13 -6.13012488e+12\n",
      "  2.30414757e+15  4.07757286e+13  4.13755813e+15  4.81669691e+15\n",
      "  3.92343335e+15  3.74707795e+15  3.74709860e+15  4.66932811e+15\n",
      "  4.64522644e+15  4.64520185e+15  4.52019869e+15]\n",
      "Gradient Descent(98/99): loss=1.9454468906691556e+33, weights = [ 1.54844745e+01 -1.73658519e+15  1.76302091e+15  2.54228129e+14\n",
      " -5.65102956e+15 -6.75156277e+15 -6.55147055e+15 -6.75111023e+15\n",
      "  3.19486702e+15 -2.70818743e+15 -6.39530590e+15 -4.12308701e+14\n",
      " -3.95430063e+15 -6.75164907e+15 -2.31813070e+15 -5.08983313e+13\n",
      " -3.50005557e+13 -1.99724012e+15 -1.09108951e+14  8.90985101e+12\n",
      " -3.34897118e+15 -5.92656224e+13 -6.01374805e+15 -7.00084464e+15\n",
      " -5.70252765e+15 -5.44620329e+15 -5.44623331e+15 -6.78665094e+15\n",
      " -6.75162029e+15 -6.75158456e+15 -6.56989827e+15]\n",
      "Gradient Descent(99/99): loss=4.109808823991495e+33, weights = [-2.34734007e+01  2.52404570e+15 -2.56246878e+15 -3.69508745e+14\n",
      "  8.21350829e+15  9.81308206e+15  9.52225732e+15  9.81242430e+15\n",
      " -4.64359042e+15  3.93622430e+15  9.29527928e+15  5.99271495e+14\n",
      "  5.74739180e+15  9.81320748e+15  3.36929501e+15  7.39783541e+13\n",
      "  5.08716776e+13  2.90289549e+15  1.58584779e+14 -1.29500535e+13\n",
      "  4.86757365e+15  8.61398221e+13  8.74070271e+15  1.01754016e+16\n",
      "  8.28835836e+15  7.91580284e+15  7.91584647e+15  9.86408108e+15\n",
      "  9.81316565e+15  9.81311371e+15  9.54904117e+15]\n"
     ]
    }
   ],
   "source": [
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gamma = 0.2\n",
    "\n",
    "weights, loss = least_squares_GD (y, tx, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for the best value of gamma  with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b38f19e5700d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# store weights and rmse for train and test data for each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mws_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ML_Project1/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma, printing)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ML_Project1/cost.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"compute the loss by mse.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cross_validation import cross_validation\n",
    "from cost import compute_loss_rmse\n",
    "from plots import *\n",
    "\n",
    "w_initial = np.random.rand(num_features)\n",
    "max_iters = 100\n",
    "gammas = np.linspace(0.01, 0.16, 20)\n",
    "k_fold = 4\n",
    "seed = 6\n",
    "\n",
    "# prepare storage of the mean of the weights and rmse for train and test data\n",
    "ws = np.zeros((num_features, len(gammas)))\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "\n",
    "for ind, gamma in enumerate(gammas):\n",
    "    # prepare storage of weights and rmse for train and test data for each fold\n",
    "    ws_tmp = np.zeros((num_features, k_fold))\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross-validation\n",
    "    for i,k in enumerate(range(k_fold)):\n",
    "        tx_train, y_train, tx_test, y_test = cross_validation(y, tx, k, k_fold, seed)\n",
    "        w,_ = least_squares_GD(y_train, tx_train, w_initial, max_iters, gamma, printing=False)\n",
    "        # store weights and rmse for train and test data for each fold\n",
    "        ws_tmp[:, i] = w\n",
    "        rmse_tr.append(compute_loss_rmse(y_train, tx_train, w))\n",
    "        rmse_te.append(compute_loss_rmse(y_test, tx_test, w))\n",
    "    # store the mean of the weights and rmse for train and test data\n",
    "    ws[:, ind] = np.mean(ws_tmp, 1)\n",
    "    rmse_train.append(np.mean(rmse_tr))\n",
    "    rmse_test.append(np.mean(rmse_te))\n",
    "    \n",
    "loss = np.amin(rmse_test)\n",
    "weights = ws[:,np.argmin(rmse_test)]\n",
    "gamma_star = gammas[np.argmin(rmse_test)]\n",
    "\n",
    "plot_cross_validation(gamma,\"Gamma\", gamma_star, rmse_train, rmse_test, loss)\n",
    "print(\" Best value of gamma = {g} \\n Loss = {l} \\n Weights = {we}\".format(\n",
    "    g=gamma_star, l=loss, we = weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "ytest, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tx_test = build_model_data(tX_test,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/submission.csv'\n",
    "y_pred = predict_labels(weights, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
